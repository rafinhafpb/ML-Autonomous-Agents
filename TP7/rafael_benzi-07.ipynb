{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": []
   },
   "source": [
    "# CSC-52081-EP Lab7: Optimization - CEM / ES\n",
    "\n",
    "<img src=\"https://raw.githubusercontent.com/jeremiedecock/polytechnique-csc-52081-ep-2025-students/refs/heads/main/assets/logo.jpg\" style=\"float: left; width: 15%\" />\n",
    "\n",
    "[CSC-52081-EP-2025](https://moodle.polytechnique.fr/course/view.php?id=19336) Lab session #7\n",
    "\n",
    "2019-2025 JÃ©rÃ©mie Decock"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "[![Open in Google Colab](https://colab.research.google.com/assets/colab-badge.svg)](https://colab.research.google.com/github/jeremiedecock/polytechnique-csc-52081-ep-2025-students/blob/main/lab7_optim.ipynb)\n",
    "\n",
    "[![My Binder](https://mybinder.org/badge_logo.svg)](https://mybinder.org/v2/gh/jeremiedecock/polytechnique-csc-52081-ep-2025-students/main?filepath=lab7_optim.ipynb)\n",
    "\n",
    "[![NbViewer](https://raw.githubusercontent.com/jupyter/design/main/logos/Badges/nbviewer_badge.svg)](https://nbviewer.jupyter.org/github/jeremiedecock/polytechnique-csc-52081-ep-2025-students/blob/main/lab7_optim.ipynb)\n",
    "\n",
    "[![Local](https://img.shields.io/badge/Local-Save%20As...-blue)](https://github.com/jeremiedecock/polytechnique-csc-52081-ep-2025-students/raw/main/lab7_optim.ipynb)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Introduction\n",
    "\n",
    "In the previous lab we studied a method that allowed us to apply reinforcement learning in continuous state spaces and/or continuous action spaces.\n",
    "We used REINFORCE, a *Policy gradient* method that directly optimize the parametric policy $\\pi_{\\theta}$.\n",
    "The parameter $\\theta$ was iteratively updated toward a local maximum of the total expected reward $J(\\theta)$ using a gradient ascent method:\n",
    "$$\\theta \\leftarrow \\theta + \\alpha \\nabla_{\\theta}J(\\theta)$$\n",
    "A convenient analytical formulation of $\\nabla_{\\theta}J(\\theta)$ was obtained thanks to the *Policy Gradient theorem*:\n",
    "\n",
    "$$\\nabla_\\theta J(\\theta) = \\nabla_\\theta V^{\\pi_\\theta}(s) = \\mathbb{E}_{\\pi_\\theta} \\left[\\nabla_\\theta \\log \\pi_\\theta (a|s) Q^{\\pi_\\theta}(s,a) \\right].$$\n",
    "However, gradient ascent methods may have a slow convergence and will only found a local optimum.\n",
    "Moreover, this approach requires an analytical formulation of $\\nabla_\\theta \\log \\pi_\\theta (s,a)$ which is not always known (when something else than a neural networks is used for the agent's policy).\n",
    "\n",
    "Direct Policy Search methods using gradient free optimization procedures like Evolution Strategies or Cross Entropy Method (CEM) are interesting alternatives to Policy Gradient algorithms.\n",
    "They can be successfully applied as long as the $\\pi_\\theta$ policy has no more than few hundreds of parameters.\n",
    "Moreover, these method can solve complex problems that cannot be modeled as Markov Decision Processes.\n",
    "\n",
    "As for previous Reinforcement Learning labs, we will use standard problems provided by Gymnasium suite.\n",
    "Especially, we will try to solve the LunarLander-v3 problem (https://gymnasium.farama.org/environments/box2d/lunar_lander/) which offers both continuous states and action spaces.\n",
    "\n",
    "As for previous labs, you can either:\n",
    "- open, edit and execute the notebook in *Google Colab* following this link: https://colab.research.google.com/github/jeremiedecock/polytechnique-csc-52081-ep-2025-students/blob/main/lab7_optim.ipynb ; this is the **recommended** choice as you have nothing to install on your computer\n",
    "- open, edit and execute the notebook in *MyBinder* (if for any reason the Google Colab solution doesn't work): https://mybinder.org/v2/gh/jeremiedecock/polytechnique-csc-52081-ep-2025-students/main?filepath=lab7_optim.ipynb\n",
    "- download, edit and execute the notebook on your computer if Python3 and JypyterLab are already installed: https://github.com/jeremiedecock/polytechnique-csc-52081-ep-2025-students/raw/main/lab7_optim.ipynb\n",
    "\n",
    "If you work with Google Colab or MyBinder, **remember to save or download your work regularly or you may lose it!**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Lab Submission\n",
    "\n",
    "Please submit your completed notebook in [Moodle : \"Lab 7 - Submission\"](https://moodle.polytechnique.fr/course/section.php?id=66535).\n",
    "\n",
    "### Submission Guidelines\n",
    "\n",
    "1. **File Naming:** Rename your notebook as follows: **`firstname_lastname-07.ipynb`** where `firstname` and `lastname` match your email address. *Example: `jesse_read-07.ipynb`*\n",
    "2. **Clear Output Cells:** To reduce file size (**must be under 500 KB**), clear all output cells before submitting. This includes rendered images, videos, plots, and dataframes...\n",
    "   - **JupyterLab:**\n",
    "     - Click **\"Kernel\" â†’ \"Restart Kernel and Clear Outputs of All Cells...\"**\n",
    "     - Then go to **\"File\" â†’ \"Save Notebook As...\"**\n",
    "   - **Google Colab:**\n",
    "     - Click **\"Edit\" â†’ \"Clear all outputs\"**\n",
    "     - Then go to **\"File\" â†’ \"Download\" â†’ \"Download.ipynb\"**\n",
    "   - **VSCode:**\n",
    "     - Click **\"Clear All Outputs\"**\n",
    "     - Then **save your file**\n",
    "3. **Upload Your File:** Only **`.ipynb`** files are accepted.\n",
    "\n",
    "**Note:** Bonus parts (if any) are optional, as their name suggests.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Setup the Python environment"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This notebook relies on several libraries including `cma`, `gymnasium[box2d,classic-control]` (v1.0.0), `ipywidgets`, `matplotlib`, `moviepy`, `numpy`, `pandas`, `pygame`, `seaborn`, and `tqdm`.\n",
    "A complete list of dependencies can be found in the following [requirements-lab7.txt](https://raw.githubusercontent.com/jeremiedecock/polytechnique-csc-52081-ep-2025-students/main/requirements-lab7.txt) file."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### If you use Google Colab\n",
    "\n",
    "If you use Google Colab, execute the next cell to install required libraries."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "xt66Z85AmOI2",
    "outputId": "bd7a6b75-ad3c-4be1-d560-8239fbc0e9d2"
   },
   "outputs": [],
   "source": [
    "import sys\n",
    "import subprocess\n",
    "\n",
    "\n",
    "def is_colab() -> bool:\n",
    "    return \"google.colab\" in sys.modules\n",
    "\n",
    "\n",
    "def run_subprocess_command(cmd: str) -> None:\n",
    "    # run the command\n",
    "    process = subprocess.Popen(cmd.split(), stdout=subprocess.PIPE)\n",
    "    # print the output\n",
    "    for line in process.stdout:\n",
    "        print(line.decode().strip())\n",
    "\n",
    "\n",
    "if is_colab():\n",
    "    run_subprocess_command(\"apt install swig\")\n",
    "    run_subprocess_command(\"pip install -r https://raw.githubusercontent.com/jeremiedecock/polytechnique-csc-52081-ep-2025-students/main/requirements-lab7-google-colab.txt\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### If you have downloaded the notebook on your computer and execute it in your own Python environment"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "To set up the necessary dependencies, run the following commands to establish a [Python virtual environment (venv)](https://docs.python.org/3/library/venv.html) that includes all the essential libraries for this lab.\n",
    "\n",
    "#### On Posix systems (Linux, MacOSX, WSL, ...)\n",
    "\n",
    "```bash\n",
    "python3 -m venv env-lab7\n",
    "source env-lab7/bin/activate\n",
    "python3 -m pip install --upgrade pip\n",
    "python3 -m pip install -r https://raw.githubusercontent.com/jeremiedecock/polytechnique-csc-52081-ep-2025-students/main/requirements-lab7.txt\n",
    "```\n",
    "\n",
    "#### On Windows\n",
    "\n",
    "```bash\n",
    "python3 -m venv env-lab7\n",
    "env-lab7\\Scripts\\activate.bat\n",
    "python3 -m pip install --upgrade pip\n",
    "python3 -m pip install -r https://raw.githubusercontent.com/jeremiedecock/polytechnique-csc-52081-ep-2025-students/main/requirements-lab7.txt\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Run CSC-52081-EP notebooks locally in a dedicated Docker container\n",
    "\n",
    "If you are familiar with Docker, an image is available on Docker Hub for this lab:\n",
    "\n",
    "```bash\n",
    "docker run -it --rm --user root -p 8888:8888 -e NB_UID=$(id -u) -e NB_GID=$(id -g) -v \"${PWD}\":/home/jovyan/work jdhp/csc-52081-ep:latest\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Import required packages"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import gymnasium as gym\n",
    "import math\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from pathlib import Path\n",
    "import itertools\n",
    "import torch\n",
    "from typing import cast, List, Tuple, Deque, Optional, Callable"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "import seaborn as sns\n",
    "from tqdm.notebook import tqdm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from IPython.display import Video\n",
    "from ipywidgets import interact"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import warnings\n",
    "\n",
    "warnings.filterwarnings(\"ignore\", category=UserWarning)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sns.set_context(\"talk\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "FIGS_DIR = Path(\"figs/\") / \"lab7\"       # Where to save figures (.gif or .mp4 files)\n",
    "PLOTS_DIR = Path(\"figs/\") / \"lab7\"      # Where to save plots (.png or .svg files)\n",
    "MODELS_DIR = Path(\"models/\") / \"lab7\"   # Where to save models (.pth files)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if not FIGS_DIR.exists():\n",
    "    FIGS_DIR.mkdir(parents=True)\n",
    "if not PLOTS_DIR.exists():\n",
    "    PLOTS_DIR.mkdir(parents=True)\n",
    "if not MODELS_DIR.exists():\n",
    "    MODELS_DIR.mkdir(parents=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "lines_to_next_cell": 2
   },
   "source": [
    "## Define the video selector widget"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "lines_to_next_cell": 2
   },
   "source": [
    "The `video_selector` function, defined in the next cell, will be used in exercises 2, 3, 4 and 5 to display different episodes of the trained agent."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def video_selector(file_path: List[Path]) -> Video:\n",
    "    return Video(file_path, embed=True, html_attributes=\"controls autoplay loop\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## PyTorch Refresher and Cheat Sheet\n",
    "\n",
    "In this lab, we will be implementing our deep reinforcement learning algorithms using PyTorch.\n",
    "If you need a refresher, you might find this [PyTorch Cheat Sheet](https://pytorch.org/tutorials/beginner/ptcheat.html) helpful. It provides a quick reference for many of the most commonly used PyTorch functions and concepts, and can be a valuable resource as you work through this lab.\n",
    "\n",
    "You can also refer to the [official documentation](https://pytorch.org/docs/stable/index.html)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## PyTorch setup"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "GPUs are useless for this lab. The following cell ask PyTorch to work on CPU."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "device = \"cpu\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Exercise 1: Implement CEM and test it on the CartPole environment"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Before solving the Lunar Lander environment, we will practice on the (simpler) CartPole environment."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Reminder**: a description of the CartPole environment is available at https://gymnasium.farama.org/environments/classic_control/cart_pole/. This environment offers a continuous state space and discrete action space."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Implement the policy to optimize"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "lines_to_next_cell": 2
   },
   "source": [
    "An implementation of the CartPole policy is given in the following cell."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def sigmoid(x: np.ndarray) -> np.ndarray:\n",
    "    \"\"\"\n",
    "    Compute the sigmoid function for the input array.\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    x : np.ndarray\n",
    "        The input array for which to compute the sigmoid function.\n",
    "\n",
    "    Returns\n",
    "    -------\n",
    "    np.ndarray\n",
    "        The output array with the sigmoid function applied element-wise.\n",
    "    \"\"\"\n",
    "    return 1.0 / (1.0 + np.exp(-x))\n",
    "\n",
    "\n",
    "# Logistic Regression ############################################\n",
    "\n",
    "\n",
    "class LogisticRegression:\n",
    "    \"\"\"\n",
    "    Logistic Regression model for binary classification.\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    observations_size : int\n",
    "        The size of the observation space.\n",
    "\n",
    "    Attributes\n",
    "    ----------\n",
    "    observation_size : int\n",
    "        The size of the observation space.\n",
    "    params : np.ndarray\n",
    "        The parameters of the logistic regression model.\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, observations_size: int):\n",
    "        \"\"\"\n",
    "        Initialize the LogisticRegression model with random parameters.\n",
    "\n",
    "        Parameters\n",
    "        ----------\n",
    "        observations_size : int\n",
    "            The size of the observation space.\n",
    "        \"\"\"\n",
    "        self.observation_size = observations_size\n",
    "        self.params = np.random.rand(observations_size)\n",
    "\n",
    "    def __call__(self, observation: np.ndarray) -> int:\n",
    "        \"\"\"\n",
    "        Predict the class label for a given observation.\n",
    "\n",
    "        Parameters\n",
    "        ----------\n",
    "        observation : np.ndarray\n",
    "            The input observation.\n",
    "\n",
    "        Returns\n",
    "        -------\n",
    "        int\n",
    "            The predicted class label (0 or 1).\n",
    "        \"\"\"\n",
    "        prob_push_right = sigmoid(np.dot(observation, np.transpose(self.params)))\n",
    "        return 1 if np.random.rand() < prob_push_right else 0\n",
    "\n",
    "    def get_params(self) -> np.ndarray:\n",
    "        \"\"\"\n",
    "        Get the parameters of the logistic regression model.\n",
    "\n",
    "        Returns\n",
    "        -------\n",
    "        np.ndarray\n",
    "            The parameters of the logistic regression model.\n",
    "        \"\"\"\n",
    "        return self.params.copy()\n",
    "\n",
    "    def set_params(self, params: np.ndarray) -> None:\n",
    "        \"\"\"\n",
    "        Set the parameters of the logistic regression model.\n",
    "\n",
    "        Parameters\n",
    "        ----------\n",
    "        params : np.ndarray\n",
    "            The parameters of the logistic regression model.\n",
    "        \"\"\"\n",
    "        self.params = params.copy()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Test the untrained agent"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def test_agent(\n",
    "    env: gym.Env, policy: torch.nn.Module, num_episode: int = 1\n",
    ") -> List[float]:\n",
    "    \"\"\"\n",
    "    Test a naive agent in the given environment using the provided Q-network.\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    env : gym.Env\n",
    "        The environment in which to test the agent.\n",
    "    policy : torch.nn.Module\n",
    "        The neural network to use for decision making.\n",
    "    num_episode : int, optional\n",
    "        The number of episodes to run, by default 1.\n",
    "\n",
    "    Returns\n",
    "    -------\n",
    "    List[float]\n",
    "        A list of rewards per episode.\n",
    "    \"\"\"\n",
    "    episode_reward_list = []\n",
    "\n",
    "    for episode_id in range(num_episode):\n",
    "        observation, info = env.reset()\n",
    "        done = False\n",
    "        episode_reward = 0.0\n",
    "\n",
    "        while not done:\n",
    "\n",
    "            action = policy(observation)\n",
    "\n",
    "            next_observation, reward, terminated, truncated, info = env.step(action)\n",
    "            done = terminated or truncated\n",
    "\n",
    "            episode_reward += float(reward)\n",
    "\n",
    "            observation = next_observation\n",
    "\n",
    "        episode_reward_list.append(episode_reward)\n",
    "        print(f\"Episode reward: {episode_reward}\")\n",
    "\n",
    "    return episode_reward_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "lines_to_next_cell": 2
   },
   "outputs": [],
   "source": [
    "VIDEO_PREFIX_EX1_UNTRAINED = \"lab7_ex1_untained\"\n",
    "\n",
    "NUM_EPISODES = 3\n",
    "\n",
    "file_path_list = [\n",
    "    FIGS_DIR / f\"{VIDEO_PREFIX_EX1_UNTRAINED}-episode-{episode_index}.mp4\"\n",
    "    for episode_index in range(NUM_EPISODES)\n",
    "]\n",
    "\n",
    "for file_path in file_path_list:\n",
    "    file_path.unlink(missing_ok=True)\n",
    "\n",
    "env = gym.make(\"CartPole-v1\", render_mode=\"rgb_array\")\n",
    "env = gym.wrappers.RecordVideo(\n",
    "    env,\n",
    "    video_folder=str(FIGS_DIR),\n",
    "    name_prefix=VIDEO_PREFIX_EX1_UNTRAINED,\n",
    "    episode_trigger=lambda x: True,\n",
    ")\n",
    "env = gym.wrappers.RecordEpisodeStatistics(env, buffer_length=NUM_EPISODES)\n",
    "\n",
    "nn_policy = LogisticRegression(env.observation_space.shape[0])\n",
    "\n",
    "test_agent(env, nn_policy, num_episode=NUM_EPISODES)\n",
    "\n",
    "# print(f'Episode time taken: {env.time_queue}')\n",
    "# print(f'Episode total rewards: {env.return_queue}')\n",
    "# print(f'Episode lengths: {env.length_queue}')\n",
    "\n",
    "env.close()\n",
    "\n",
    "print(\"\\nSelect the episode to play here ðŸ‘‡\\n\")\n",
    "\n",
    "interact(video_selector, file_path=file_path_list);"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Implement the objective function"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "lines_to_next_cell": 2
   },
   "source": [
    "Optimization algorithms aim to find the minimum of a function. This function is called an \"objective function\".\n",
    "The cell below implements the framework for such a function.\n",
    "Note that in reinforcement learning, by convention the score is a reward to maximize whereas in mathematical optimization the score is a cost to minimize (most optimization libraries like PyCMA used in this lab impose this convention) ; the objective function will therefore return the opposite of the reward as the score of evaluated policies."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "lines_to_next_cell": 2
   },
   "outputs": [],
   "source": [
    "class ObjectiveFunction:\n",
    "    \"\"\"\n",
    "    Objective function for evaluating a policy in a given environment.\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    env : gym.Env\n",
    "        The environment in which to evaluate the policy.\n",
    "    policy : torch.nn.Module\n",
    "        The policy to evaluate.\n",
    "    num_episodes : int, optional\n",
    "        The number of episodes to run for each evaluation, by default 1.\n",
    "    max_time_steps : float, optional\n",
    "        The maximum number of time steps per episode, by default float(\"inf\").\n",
    "    minimization_solver : bool, optional\n",
    "        Whether the solver is a minimization solver, by default True.\n",
    "\n",
    "    Attributes\n",
    "    ----------\n",
    "    env : gym.Env\n",
    "        The environment in which to evaluate the policy.\n",
    "    policy : torch.nn.Module\n",
    "        The policy to evaluate.\n",
    "    num_episodes : int\n",
    "        The number of episodes to run for each evaluation.\n",
    "    max_time_steps : float\n",
    "        The maximum number of time steps per episode.\n",
    "    minimization_solver : bool\n",
    "        Whether the solver is a minimization solver.\n",
    "    num_evals : int\n",
    "        The number of evaluations performed.\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(\n",
    "        self,\n",
    "        env: gym.Env,\n",
    "        policy: torch.nn.Module,\n",
    "        num_episodes: int = 1,\n",
    "        max_time_steps: float = float(\"inf\"),\n",
    "        minimization_solver: bool = True,\n",
    "    ):\n",
    "        self.env = env\n",
    "        self.policy = policy\n",
    "        self.num_episodes = num_episodes\n",
    "        self.max_time_steps = max_time_steps\n",
    "        self.minimization_solver = minimization_solver\n",
    "\n",
    "        self.num_evals = 0\n",
    "\n",
    "    def eval(self, policy_params: np.ndarray, num_episodes: Optional[int] = None, max_time_steps: Optional[float] = None) -> float:\n",
    "        \"\"\"\n",
    "        Evaluate a policy.\n",
    "\n",
    "        Parameters\n",
    "        ----------\n",
    "        policy_params : np.ndarray\n",
    "            The parameters of the policy to evaluate.\n",
    "        num_episodes : int, optional\n",
    "            The number of episodes to run for each evaluation, by default None.\n",
    "        max_time_steps : float, optional\n",
    "            The maximum number of time steps per episode, by default None.\n",
    "\n",
    "        Returns\n",
    "        -------\n",
    "        float\n",
    "            The average total rewards over the evaluation episodes.\n",
    "        \"\"\"\n",
    "        self.policy.set_params(policy_params)\n",
    "\n",
    "        self.num_evals += 1\n",
    "\n",
    "        if num_episodes is None:\n",
    "            num_episodes = self.num_episodes\n",
    "\n",
    "        if max_time_steps is None:\n",
    "            max_time_steps = self.max_time_steps\n",
    "\n",
    "        average_total_rewards = 0\n",
    "\n",
    "        for i_episode in range(num_episodes):\n",
    "            total_rewards = 0.0\n",
    "            observation, info = self.env.reset()\n",
    "\n",
    "            for t in range(max_time_steps):\n",
    "                action = self.policy(observation)\n",
    "                observation, reward, terminated, truncated, info = self.env.step(action)\n",
    "                total_rewards += reward\n",
    "\n",
    "                done = terminated or truncated\n",
    "\n",
    "                if done:\n",
    "                    break\n",
    "\n",
    "            average_total_rewards += float(total_rewards) / num_episodes\n",
    "\n",
    "        if self.minimization_solver:\n",
    "            average_total_rewards *= -1.0\n",
    "\n",
    "        return average_total_rewards  # Optimizers do minimization by default...\n",
    "\n",
    "    def __call__(self, policy_params: np.ndarray, num_episodes: Optional[int] = None, max_time_steps: Optional[float] = None) -> float:\n",
    "        \"\"\"\n",
    "        Evaluate a policy.\n",
    "\n",
    "        Parameters\n",
    "        ----------\n",
    "        policy_params : np.ndarray\n",
    "            The parameters of the policy to evaluate.\n",
    "        num_episodes : int, optional\n",
    "            The number of episodes to run for each evaluation, by default None.\n",
    "        max_time_steps : float, optional\n",
    "            The maximum number of time steps per episode, by default None.\n",
    "\n",
    "        Returns\n",
    "        -------\n",
    "        float\n",
    "            The average total rewards over the evaluation episodes.\n",
    "        \"\"\"\n",
    "        return self.eval(policy_params, num_episodes, max_time_steps)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Implement the CEM optimization algorithm"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Task 1**: Implement the `cem_uncorrelated` function that search the best $\\theta$ parameters with a Cross Entropy Method. Use the objective function defined above.\n",
    "$\\mathbb{P}$ can be defined as an multivariate normal distribution $\\mathcal{N}\\left( \\boldsymbol{\\mu}, \\boldsymbol{\\sigma^2} \\boldsymbol{\\Sigma} \\right)$ where $\\boldsymbol{\\mu}$ and $\\boldsymbol{\\sigma^2} \\boldsymbol{\\Sigma}$ are vectors i.e. we use one mean and one variance parameters per dimension of $\\boldsymbol{\\theta}$."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "lines_to_next_cell": 2
   },
   "source": [
    "**Cross Entropy**\n",
    "\n",
    "**Input**:<br>\n",
    "$\\quad\\quad$ $f$: the objective function<br>\n",
    "$\\quad\\quad$ $\\mathbb{P}$: family of distribution<br>\n",
    "$\\quad\\quad$ $\\boldsymbol{\\theta}$: initial parameters for the proposal distribution $\\mathbb{P}$<br>\n",
    "\n",
    "**Algorithm parameter**:<br>\n",
    "$\\quad\\quad$ $m$: sample size<br>\n",
    "$\\quad\\quad$ $m_{\\text{elite}}$: number of samples to use to fit $\\boldsymbol{\\theta}$<br>\n",
    "\n",
    "**FOR EACH** iteration<br>\n",
    "$\\quad\\quad$ samples $\\leftarrow \\{ \\boldsymbol{x}_1, \\dots, \\boldsymbol{x}_m \\}$ with $\\boldsymbol{x}_i \\sim \\mathbb{P}(\\boldsymbol{\\theta}) ~~ \\forall i \\in 1\\dots m$<br>\n",
    "$\\quad\\quad$ elite $\\leftarrow $ { $m_{\\text{elite}}$ best samples } $\\quad$ (i.e. select best samples according to $f$)<br>\n",
    "$\\quad\\quad$ $\\boldsymbol{\\theta} \\leftarrow $ fit $\\mathbb{P}(\\boldsymbol{\\theta})$ to the elite samples<br>\n",
    "\n",
    "**RETURN** $\\boldsymbol{\\theta}$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def cem_uncorrelated(\n",
    "    objective_function: Callable[[np.ndarray], float],\n",
    "    mean_array: np.ndarray,\n",
    "    var_array: np.ndarray,\n",
    "    max_iterations: int = 500,\n",
    "    sample_size: int = 50,\n",
    "    elite_frac: float = 0.2,\n",
    "    print_every: int = 10,\n",
    "    success_score: float = float(\"inf\"),\n",
    "    num_evals_for_stop: Optional[int] = None,\n",
    "    hist_dict: Optional[dict] = None,\n",
    ") -> np.ndarray:\n",
    "    \"\"\"\n",
    "    Cross-entropy method.\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    objective_function : Callable[[np.ndarray], float]\n",
    "        The function to maximize.\n",
    "    mean_array : np.ndarray\n",
    "        The initial proposal distribution (mean vector).\n",
    "    var_array : np.ndarray\n",
    "        The initial proposal distribution (variance vector).\n",
    "    max_iterations : int, optional\n",
    "        Number of training iterations, by default 500.\n",
    "    sample_size : int, optional\n",
    "        Size of population at each iteration, by default 50.\n",
    "    elite_frac : float, optional\n",
    "        Rate of top performers to use in update with elite_frac âˆˆ ]0;1], by default 0.2.\n",
    "    print_every : int, optional\n",
    "        How often to print average score, by default 10.\n",
    "    success_score : float, optional\n",
    "        The score at which to stop the optimization, by default float(\"inf\").\n",
    "    num_evals_for_stop : Optional[int], optional\n",
    "        Number of evaluations for stopping criteria, by default None.\n",
    "    hist_dict : Optional[dict], optional\n",
    "        Dictionary to log the history, by default None.\n",
    "\n",
    "    Returns\n",
    "    -------\n",
    "    np.ndarray\n",
    "        The optimized mean vector.\n",
    "    \"\"\"\n",
    "    assert 0.0 < elite_frac <= 1.0\n",
    "\n",
    "    n_elite = math.ceil(sample_size * elite_frac)\n",
    "\n",
    "    for iteration_index in range(0, max_iterations):\n",
    "\n",
    "        # SAMPLE A NEW POPULATION OF SOLUTIONS (X VECTORS) ####################\n",
    "        x_array = np.random.multivariate_normal(mean_array, np.diag(var_array), sample_size)\n",
    "\n",
    "        # EVALUATE SAMPLES AND EXTRACT THE BEST ONES (\"ELITE\") ################\n",
    "        score_array = np.array([objective_function(x) for x in x_array])\n",
    "\n",
    "        sorted_indices_array = np.argsort(score_array)  # Sort from the lower score to the higher one\n",
    "        elite_indices_array = sorted_indices_array[:n_elite]  # Recall: we *minimize* the objective function thus we take the samples that are at the beginning of the sorted_indices\n",
    "\n",
    "        elite_x_array = x_array[elite_indices_array]\n",
    "\n",
    "        # FIT THE NORMAL DISTRIBUTION ON THE ELITE POPULATION #################\n",
    "        mean_array = np.mean(elite_x_array, axis=0)\n",
    "        var_array = np.var(elite_x_array, axis=0)\n",
    "        score = np.mean(score_array[elite_indices_array])\n",
    "\n",
    "        # PRINT STATUS ########################################################\n",
    "        if iteration_index % print_every == 0:\n",
    "            print(\"Iteration {}\\tScore {}\".format(iteration_index, score))\n",
    "\n",
    "        if hist_dict is not None:\n",
    "            hist_dict[iteration_index] = [score] + mean_array.tolist() + var_array.tolist()\n",
    "\n",
    "        # STOPPING CRITERIA ####################################################\n",
    "        if num_evals_for_stop is not None:\n",
    "            score = np.mean(score_array[:num_evals_for_stop])\n",
    "\n",
    "        # `num_evals_for_stop = None` may be used to fasten computations but it introduces bias...\n",
    "        if score <= success_score:\n",
    "            break\n",
    "\n",
    "    return mean_array\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Train the agent"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Task 2:** train your implementation using the following cells."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "env = gym.make(\"CartPole-v1\")\n",
    "\n",
    "nn_policy = LogisticRegression(env.observation_space.shape[0])\n",
    "\n",
    "objective_function = ObjectiveFunction(\n",
    "    env=env, policy=nn_policy, num_episodes=10, max_time_steps=1000\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "\n",
    "hist_dict = {}\n",
    "\n",
    "num_params = len(nn_policy.get_params())\n",
    "\n",
    "init_mean_array = np.random.random(num_params)\n",
    "init_var_array = np.ones(num_params) * 100.0\n",
    "\n",
    "optimized_policy_params = cem_uncorrelated(\n",
    "    objective_function=objective_function,\n",
    "    mean_array=init_mean_array,\n",
    "    var_array=init_var_array,\n",
    "    max_iterations=30,\n",
    "    sample_size=50,\n",
    "    elite_frac=0.1,\n",
    "    print_every=1,\n",
    "    success_score=-500,\n",
    "    num_evals_for_stop=None,\n",
    "    hist_dict=hist_dict,\n",
    ")\n",
    "\n",
    "env.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "lines_to_next_cell": 0
   },
   "outputs": [],
   "source": [
    "df = pd.DataFrame.from_dict(\n",
    "    hist_dict,\n",
    "    orient=\"index\",\n",
    "    columns=[\"score\", \"mu1\", \"mu2\", \"mu3\", \"mu4\", \"var1\", \"var2\", \"var3\", \"var4\"],\n",
    ")\n",
    "ax = df.score.plot(title=\"Average reward\", figsize=(20, 5))\n",
    "plt.xlabel(\"Training Steps\")\n",
    "plt.ylabel(\"Reward\")\n",
    "plt.savefig(PLOTS_DIR / \"lab7_ex1_cem_cartpole_avg_reward_wrt_iterations.png\");"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "lines_to_next_cell": 0
   },
   "outputs": [],
   "source": [
    "ax = df[[\"mu1\", \"mu2\", \"mu3\", \"mu4\"]].plot(\n",
    "    title=\"Theta w.r.t training steps\", figsize=(20, 5)\n",
    ");\n",
    "plt.xlabel(\"Training Steps\")\n",
    "plt.savefig(PLOTS_DIR / \"lab7_ex1_cem_cartpole_params_wrt_iterations.png\");"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ax = df[[\"var1\", \"var2\", \"var3\", \"var4\"]].plot(\n",
    "    logy=True, title=\"Variance w.r.t training steps\", figsize=(20, 5)\n",
    ")\n",
    "plt.xlabel(\"Training Steps\")\n",
    "plt.savefig(PLOTS_DIR / \"lab7_ex1_cem_cartpole_var_wrt_iterations.png\");"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "print(\"Optimized weights: \", optimized_policy_params)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Test the trained agent"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Task 3:** test the optimized policy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "lines_to_next_cell": 2
   },
   "outputs": [],
   "source": [
    "VIDEO_PREFIX_EX1_CEM_CARTPOLE_TRAINED = \"lab7_ex1_cem_cartpole_tained\"\n",
    "\n",
    "NUM_EPISODES = 3\n",
    "\n",
    "file_path_list = [\n",
    "    FIGS_DIR / f\"{VIDEO_PREFIX_EX1_CEM_CARTPOLE_TRAINED}-episode-{episode_index}.mp4\"\n",
    "    for episode_index in range(NUM_EPISODES)\n",
    "]\n",
    "\n",
    "for file_path in file_path_list:\n",
    "    file_path.unlink(missing_ok=True)\n",
    "\n",
    "env = gym.make(\"CartPole-v1\", render_mode=\"rgb_array\")\n",
    "env = gym.wrappers.RecordVideo(\n",
    "    env,\n",
    "    video_folder=str(FIGS_DIR),\n",
    "    name_prefix=VIDEO_PREFIX_EX1_CEM_CARTPOLE_TRAINED,\n",
    "    episode_trigger=lambda x: True,\n",
    ")\n",
    "env = gym.wrappers.RecordEpisodeStatistics(env, buffer_length=NUM_EPISODES)\n",
    "\n",
    "objective_function = ObjectiveFunction(env=env, policy=nn_policy)\n",
    "\n",
    "objective_function.eval(optimized_policy_params, num_episodes=NUM_EPISODES, max_time_steps=200)\n",
    "\n",
    "# print(f'Episode time taken: {env.time_queue}')\n",
    "# print(f'Episode total rewards: {env.return_queue}')\n",
    "# print(f'Episode lengths: {env.length_queue}')\n",
    "\n",
    "env.close()\n",
    "\n",
    "print(\"\\nSelect the episode to play here ðŸ‘‡\\n\")\n",
    "\n",
    "interact(video_selector, file_path=file_path_list);"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Exercise 2: implement SAES and solve the CartPole problem with it"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Implement the (1+1)-SA-ES optimization algorithm"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Task 1**: Implement the `saes_1_1` function that search the best $\\theta$ parameters with a (1+1)-SA-ES algorithm. Use the objective function defined above."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "lines_to_next_cell": 2
   },
   "source": [
    "**(1+1)-SA-ES**\n",
    "\n",
    "**Input**:<br>\n",
    "$\\quad\\quad$ $f$: the objective function<br>\n",
    "$\\quad\\quad$ $\\boldsymbol{x}$: initial solution<br>\n",
    "\n",
    "**Algorithm parameter**:<br>\n",
    "$\\quad\\quad$ $\\tau$: self-adaptation learning rate<br>\n",
    "\n",
    "**FOR EACH** generation<br>\n",
    "$\\quad\\quad$ 1. mutation of $\\sigma$ (current individual strategy) : $\\sigma' \\leftarrow \\sigma ~ e^{\\tau \\mathcal{N}(0,1)}$<br>\n",
    "$\\quad\\quad$ 2. mutation of $\\boldsymbol{x}$ (current solution) : $\\boldsymbol{x}' \\leftarrow \\boldsymbol{x} + \\sigma' ~ \\mathcal{N}(0,1)$<br>\n",
    "$\\quad\\quad$ 3. eval $f(\\boldsymbol{x}')$<br>\n",
    "$\\quad\\quad$ 4. survivor selection $\\boldsymbol{x} \\leftarrow \\boldsymbol{x}'$ and $\\sigma \\leftarrow \\sigma'$ if $f(\\boldsymbol{x}') \\leq f(\\boldsymbol{x})$<br>\n",
    "\n",
    "**RETURN** $\\boldsymbol{x}$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def saes_1_1(\n",
    "    objective_function: Callable[[np.ndarray], float],\n",
    "    x_array: np.ndarray,\n",
    "    sigma_array: np.ndarray,\n",
    "    max_iterations: int = 500,\n",
    "    tau: Optional[float] = None,\n",
    "    print_every: int = 10,\n",
    "    success_score: float = float(\"inf\"),\n",
    "    num_evals_for_stop: Optional[int] = None,\n",
    "    hist_dict: Optional[dict] = None,\n",
    ") -> np.ndarray:\n",
    "    \"\"\"\n",
    "    (1+1)-Self-Adaptive Evolution Strategy (SA-ES) optimization algorithm.\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    objective_function : Callable[[np.ndarray], float]\n",
    "        The function to minimize.\n",
    "    x_array : np.ndarray\n",
    "        The initial solution vector.\n",
    "    sigma_array : np.ndarray\n",
    "        The initial strategy parameter vector (step sizes).\n",
    "    max_iterations : int, optional\n",
    "        The maximum number of iterations, by default 500.\n",
    "    tau : Optional[float], optional\n",
    "        The self-adaptation learning rate, by default None.\n",
    "    print_every : int, optional\n",
    "        How often to print the current score, by default 10.\n",
    "    success_score : float, optional\n",
    "        The score at which to stop the optimization, by default float(\"inf\").\n",
    "    num_evals_for_stop : Optional[int], optional\n",
    "        Number of evaluations for stopping criteria, by default None.\n",
    "    hist_dict : Optional[dict], optional\n",
    "        Dictionary to log the history, by default None.\n",
    "\n",
    "    Returns\n",
    "    -------\n",
    "    np.ndarray\n",
    "        The optimized solution vector.\n",
    "    \"\"\"\n",
    "    # Number of dimension of the solution space\n",
    "    d = x_array.shape[0]\n",
    "\n",
    "    if tau is None:\n",
    "        # Self-adaptation learning rate\n",
    "        tau = 1.0 / (2.0 * d)\n",
    "\n",
    "    score = objective_function(x_array)\n",
    "\n",
    "    for iteration_index in range(0, max_iterations):\n",
    "        # 1. Mutation of sigma (current \"individual strategy\")\n",
    "        new_sigma_array = sigma_array * np.exp(tau * np.random.normal(size=d))\n",
    "\n",
    "        # 2. Mutation of x (current solution)\n",
    "        new_x_array = x_array + new_sigma_array * np.random.normal(size=d)\n",
    "\n",
    "        # 3. Eval f(x')\n",
    "        new_score = objective_function(new_x_array)\n",
    "\n",
    "        # 4. survivor selection (we follow the ES convention and do minimization)\n",
    "        # You may try `new_score < score` for less exploration\n",
    "        if new_score < score:  \n",
    "            score = new_score\n",
    "            x_array = new_x_array\n",
    "            sigma_array = new_sigma_array\n",
    "\n",
    "        # PRINT STATUS ########################################################\n",
    "\n",
    "        if iteration_index % print_every == 0:\n",
    "            print(\"Iteration {}\\tScore {}\".format(iteration_index, score))\n",
    "\n",
    "        if hist_dict is not None:\n",
    "            hist_dict[iteration_index] = [score] + x_array.tolist() + sigma_array.tolist()\n",
    "\n",
    "        # STOPPING CRITERIA ####################################################\n",
    "\n",
    "        if num_evals_for_stop is not None:\n",
    "            score = np.mean([objective_function(x_array) for _ in range(num_evals_for_stop)])\n",
    "\n",
    "        # `num_evals_for_stop = None` may be used to fasten computations but it introduces bias...\n",
    "        if score <= success_score:\n",
    "            break\n",
    "\n",
    "    return x_array"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Train the agent"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Task 2:** train your implementation using the following cells."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "env = gym.make(\"CartPole-v1\")\n",
    "\n",
    "nn_policy = LogisticRegression(env.observation_space.shape[0])\n",
    "\n",
    "objective_function = ObjectiveFunction(\n",
    "    env=env, policy=nn_policy, num_episodes=10, max_time_steps=1000\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "\n",
    "hist_dict = {}\n",
    "\n",
    "num_params = len(nn_policy.get_params())\n",
    "\n",
    "initial_solution_array = np.random.random(num_params)\n",
    "initial_sigma_array = np.ones(num_params) * 1.0\n",
    "\n",
    "optimized_policy_params = saes_1_1(\n",
    "    objective_function=objective_function,\n",
    "    x_array=initial_solution_array,\n",
    "    sigma_array=initial_sigma_array,\n",
    "    tau=0.001,\n",
    "    max_iterations=1000,\n",
    "    print_every=100,\n",
    "    success_score=-500,\n",
    "    num_evals_for_stop=None,\n",
    "    hist_dict=hist_dict,\n",
    ")\n",
    "\n",
    "env.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "lines_to_next_cell": 0
   },
   "outputs": [],
   "source": [
    "df = pd.DataFrame.from_dict(\n",
    "    hist_dict,\n",
    "    orient=\"index\",\n",
    "    columns=[\n",
    "        \"score\",\n",
    "        \"mu1\",\n",
    "        \"mu2\",\n",
    "        \"mu3\",\n",
    "        \"mu4\",\n",
    "        \"sigma1\",\n",
    "        \"sigma2\",\n",
    "        \"sigma3\",\n",
    "        \"sigma4\",\n",
    "    ],\n",
    ")\n",
    "ax = df.score.plot(title=\"Average reward\", figsize=(30, 5))\n",
    "plt.xlabel(\"Training Steps\")\n",
    "plt.ylabel(\"Reward\")\n",
    "plt.savefig(PLOTS_DIR / \"lab7_ex2_saes_cartpole_avg_reward_wrt_iterations.png\");"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "lines_to_next_cell": 0
   },
   "outputs": [],
   "source": [
    "ax = df[[\"mu1\", \"mu2\", \"mu3\", \"mu4\"]].plot(\n",
    "    title=\"Theta w.r.t training steps\", figsize=(30, 5)\n",
    ")\n",
    "plt.xlabel(\"Training Steps\")\n",
    "plt.savefig(PLOTS_DIR / \"lab7_ex2_saes_cartpole_params_wrt_iterations.png\");"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ax = df[[\"sigma1\", \"sigma2\", \"sigma3\", \"sigma4\"]].plot(\n",
    "    logy=True, title=\"Sigma w.r.t training steps\", figsize=(30, 5)\n",
    ")\n",
    "plt.xlabel(\"Training Steps\")\n",
    "plt.savefig(PLOTS_DIR / \"lab7_ex2_saes_cartpole_var_wrt_iterations.png\");"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Optimized weights: \", optimized_policy_params)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Test the trained agent"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Task 3:** test the optimized policy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "lines_to_next_cell": 2
   },
   "outputs": [],
   "source": [
    "VIDEO_PREFIX_EX2_SAES_CARTPOLE_TRAINED = \"lab7_ex2_saes_cartpole_tained\"\n",
    "\n",
    "NUM_EPISODES = 3\n",
    "\n",
    "file_path_list = [\n",
    "    FIGS_DIR / f\"{VIDEO_PREFIX_EX2_SAES_CARTPOLE_TRAINED}-episode-{episode_index}.mp4\"\n",
    "    for episode_index in range(NUM_EPISODES)\n",
    "]\n",
    "\n",
    "for file_path in file_path_list:\n",
    "    file_path.unlink(missing_ok=True)\n",
    "\n",
    "env = gym.make(\"CartPole-v1\", render_mode=\"rgb_array\")\n",
    "env = gym.wrappers.RecordVideo(\n",
    "    env,\n",
    "    video_folder=str(FIGS_DIR),\n",
    "    name_prefix=VIDEO_PREFIX_EX2_SAES_CARTPOLE_TRAINED,\n",
    "    episode_trigger=lambda x: True,\n",
    ")\n",
    "env = gym.wrappers.RecordEpisodeStatistics(env, buffer_length=NUM_EPISODES)\n",
    "\n",
    "objective_function = ObjectiveFunction(env=env, policy=nn_policy)\n",
    "\n",
    "objective_function.eval(optimized_policy_params, num_episodes=NUM_EPISODES, max_time_steps=200)\n",
    "\n",
    "# print(f'Episode time taken: {env.time_queue}')\n",
    "# print(f'Episode total rewards: {env.return_queue}')\n",
    "# print(f'Episode lengths: {env.length_queue}')\n",
    "\n",
    "env.close()\n",
    "\n",
    "print(\"\\nSelect the episode to play here ðŸ‘‡\\n\")\n",
    "\n",
    "interact(video_selector, file_path=file_path_list);"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Task 4:** try different values of $\\tau$. What happen ?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Exercise 3: Implement a parametric policy $\\pi_\\theta$ for environments having a continuous action space"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "lines_to_next_cell": 2
   },
   "source": [
    "To solve problems having a continuous space, especially to solve the LunarLander problem in the next exercise, we need to define and implement an appropriate parametric policy.\n",
    "For this purpose, we recommend the following neural network:\n",
    "- one hidden layer of 16 units having a ReLu activation function\n",
    "- a tanh activation function on the output layer (be careful on the number of output units)\n",
    "\n",
    "To solve environments with continuous action space like LunarLander with Direct Policy Search methods, a simple procedure that compute the feed forward signal is needed (we don't do back propagation here).\n",
    "A procedure to set/get weights of the network from/to a single vector $\\theta$ will also be required."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class NeuralNetworkPolicy(torch.nn.Module):\n",
    "    \"\"\"\n",
    "    A neural network used as a policy.\n",
    "\n",
    "    Attributes\n",
    "    ----------\n",
    "    hidden_layer : torch.nn.Linear\n",
    "        The hidden layer of the neural network.\n",
    "    output_layer : torch.nn.Linear\n",
    "        The output layer of the neural network.\n",
    "\n",
    "    Methods\n",
    "    -------\n",
    "    forward(observation: torch.Tensor) -> np.ndarray\n",
    "        Define the forward pass of the NeuralNetworkPolicy.\n",
    "    get_params() -> np.ndarray\n",
    "        Get the parameters of the neural network.\n",
    "    set_params(params: np.ndarray) -> None\n",
    "        Set the parameters of the neural network.\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(\n",
    "        self, observations_size: int, actions_size: int, hidden_size: int = 16\n",
    "    ):\n",
    "        \"\"\"\n",
    "        Initialize a new instance of NeuralNetworkPolicy.\n",
    "\n",
    "        Parameters\n",
    "        ----------\n",
    "        observations_size : int\n",
    "            The size of the observation space (i.e. the number of features i.e. the number of elements in the observation vector).\n",
    "        actions_size : int\n",
    "            The size of the action space.\n",
    "        hidden_size : int, optional\n",
    "            The number of units in the hidden layer, by default 16.\n",
    "        \"\"\"\n",
    "        super(NeuralNetworkPolicy, self).__init__()\n",
    "\n",
    "        self.hidden_layer = torch.nn.Linear(observations_size, hidden_size)\n",
    "        self.output_layer = torch.nn.Linear(hidden_size, actions_size)\n",
    "\n",
    "    def forward(self, observation: torch.Tensor) -> np.ndarray:\n",
    "        \"\"\"\n",
    "        Calculate the probability of action a_1 for the given observation.\n",
    "\n",
    "        Parameters\n",
    "        ----------\n",
    "        observation : torch.Tensor\n",
    "            The input tensor (observation).\n",
    "            The shape of the tensor should be (N, dim),\n",
    "            where N is the number of observations vectors in the batch\n",
    "            and dim is the dimension of observation vectors.\n",
    "\n",
    "        Returns\n",
    "        -------\n",
    "        np.ndarray\n",
    "            The output tensor (the probability of each action for the given observation).\n",
    "        \"\"\"\n",
    "        with torch.no_grad():\n",
    "            observation_tensor = torch.tensor(observation, dtype=torch.float32, device=device)\n",
    "            hidden_tensor = torch.nn.functional.relu(self.hidden_layer(observation_tensor))\n",
    "            output_tensor = torch.tanh(self.output_layer(hidden_tensor))\n",
    "            return output_tensor.squeeze().detach().cpu().numpy()\n",
    "\n",
    "    def get_params(self) -> np.ndarray:\n",
    "        \"\"\"\n",
    "        Get the parameters of the neural network.\n",
    "\n",
    "        Returns\n",
    "        -------\n",
    "        np.ndarray\n",
    "            The parameters of the neural network.\n",
    "        \"\"\"\n",
    "        params_tensor = torch.nn.utils.parameters_to_vector(self.parameters())\n",
    "        return params_tensor.detach().cpu().numpy()\n",
    "\n",
    "    def set_params(self, params: np.ndarray) -> None:\n",
    "        \"\"\"\n",
    "        Set the parameters of the neural network.\n",
    "\n",
    "        Parameters\n",
    "        ----------\n",
    "        params : np.ndarray\n",
    "            The parameters of the neural network.\n",
    "        \"\"\"\n",
    "        params_tensor = torch.tensor(params, dtype=torch.float32, device=device)\n",
    "        torch.nn.utils.vector_to_parameters(params_tensor, self.parameters())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Exercise 4: solve the LunarLander problem (continuous version) with CEM and SAES"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Hands on the LunarLander problem"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Task 1:** read https://gymnasium.farama.org/environments/box2d/lunar_lander/ to discover the LunarLanderContinuous environment.\n",
    "\n",
    "**Notice:** A reminder of Gymnasium main concepts is available at https://gymnasium.farama.org/introduction/basic_usage/."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Print some information about the environment:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%capture\n",
    "%pip install swig\n",
    "%pip install gymnasium[box2d]\n",
    "%pip install Box2D"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "env = gym.make(\"LunarLander-v3\", continuous=True)\n",
    "print(\"Observation space dimension is:\", env.observation_space.shape[0])\n",
    "print(\"Observation upper bounds:\", env.observation_space.high)\n",
    "print(\"Observation lower bounds:\", env.observation_space.low)\n",
    "print(\"Actions upper bounds:\", env.action_space.high)\n",
    "print(\"Actions lower bounds:\", env.action_space.low)\n",
    "env.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Task 2:** Run the following cells and check different basic policies (for instance constant actions or randomly drawn actions) to discover the Lunar Lander environment."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "VIDEO_PREFIX_EX4_UNTRAINED = \"lab7_ex4_lunarlander_untrained\"\n",
    "\n",
    "(FIGS_DIR / f\"{VIDEO_PREFIX_EX4_UNTRAINED}-episode-0.mp4\").unlink(missing_ok=True)\n",
    "\n",
    "env = gym.make(\"LunarLander-v3\", continuous=True, render_mode=\"rgb_array\")\n",
    "env = gym.wrappers.RecordVideo(env, video_folder=str(FIGS_DIR), name_prefix=VIDEO_PREFIX_EX4_UNTRAINED)\n",
    "\n",
    "observation, info = env.reset()\n",
    "done = False\n",
    "\n",
    "while not done:\n",
    "    # action = np.array([1., 1.])\n",
    "    action = np.array([-1.0, -1.0])\n",
    "    # action = env.action_space.sample()   # Random policy\n",
    "\n",
    "    observation, reward, terminated, truncated, info = env.step(action)\n",
    "    done = terminated or truncated\n",
    "\n",
    "env.close()\n",
    "\n",
    "Video(\n",
    "    FIGS_DIR / f\"{VIDEO_PREFIX_EX4_UNTRAINED}-episode-0.mp4\",\n",
    "    embed=True,\n",
    "    html_attributes=\"controls autoplay loop\",\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Note**: If you encounter a dependency issue with *swig*, follow the procedure outlined at https://gymnasium.farama.org/environments/box2d/"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Question 1:** We want to use CEM and SAES to compute the optimal policy for the Lunar Lander environment.\n",
    "What is the size of the search space (number of dimensions) for optimizers knowing that the policy is the one defined in exercise 3 (a neural network of one hidden layer of 16 neurons) and knowing that the observation space of the Lunar Lander environment is $\\mathcal{S} = \\mathbb{R}^8$ and its action space is $\\mathcal{A} \\subset \\mathbb{R}^2$ ?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The size of the seach space (number of dimensions) for the optimizers is the total number of parameters in the neural network policy that needs to be optimized. The NN architecture is composed by:\n",
    "\n",
    "**1.** Input layer of 8 neurons (corresponding to the 8 dimentions of the observation state).\n",
    "\n",
    "**2.** Hidden layer of 16 neurons.\n",
    "\n",
    "**3.** Output layer of 2 neurons (corresponding to the 2 dimentions of the action space).\n",
    "\n",
    "With that, the number of parameters is:\n",
    "\n",
    "**Weights between Input layer and Hidden layer:** 8 * 16 = 128\n",
    "\n",
    "**Biases for Hidden layer:** 16\n",
    "\n",
    "**Weights between Hidden layer and Output layer:** 16 * 2 = 32\n",
    "\n",
    "**Biases for Output layer:** 2\n",
    "\n",
    "Therefore, in total, we have 128 + 16 + 32 + 2 = **178** parameters to optimize."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Train the CEM agent on the LunarLander environment"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Task 3:** Train the agent using the CEM algorithm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "env = gym.make(\"LunarLander-v3\", continuous=True)\n",
    "\n",
    "nn_policy = NeuralNetworkPolicy(\n",
    "    observations_size=env.observation_space.shape[0],\n",
    "    actions_size=env.action_space.shape[0],\n",
    "    hidden_size=16,\n",
    ")\n",
    "\n",
    "objective_function = ObjectiveFunction(\n",
    "    env=env, policy=nn_policy, num_episodes=3, max_time_steps=500\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "\n",
    "hist_dict = {}\n",
    "\n",
    "num_params = len(nn_policy.get_params())\n",
    "\n",
    "init_mean_array = np.random.random(num_params)\n",
    "init_var_array = np.ones(num_params) * 1000.0\n",
    "\n",
    "optimized_policy_params = cem_uncorrelated(\n",
    "    objective_function=objective_function,\n",
    "    mean_array=init_mean_array,\n",
    "    var_array=init_var_array,\n",
    "    max_iterations=100,\n",
    "    sample_size=50,\n",
    "    elite_frac=0.2,\n",
    "    print_every=10,\n",
    "    success_score=-200,\n",
    "    hist_dict=hist_dict,\n",
    ")\n",
    "\n",
    "objective_function.env.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "lines_to_next_cell": 0
   },
   "outputs": [],
   "source": [
    "df = pd.DataFrame.from_dict(hist_dict, orient=\"index\")\n",
    "ax = df.iloc[:, 0].plot(title=\"Average reward\", figsize=(20, 5))\n",
    "plt.xlabel(\"Training Steps\")\n",
    "plt.ylabel(\"Reward\")\n",
    "plt.savefig(PLOTS_DIR / \"lab7_ex4_cem_lunarlander_avg_reward_wrt_iterations.png\");"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "lines_to_next_cell": 0
   },
   "outputs": [],
   "source": [
    "ax = df.iloc[:, 1:66].plot(\n",
    "    title=\"Theta w.r.t training steps\", legend=None, figsize=(20, 10)\n",
    ")\n",
    "# ax.get_legend().remove()\n",
    "plt.xlabel(\"Training Steps\")\n",
    "plt.savefig(PLOTS_DIR / \"lab7_ex4_cem_lunarlander_params_wrt_iterations.png\");"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ax = df.iloc[:, 67:].plot(\n",
    "    logy=True, title=\"Variance w.r.t training steps\", legend=None, figsize=(20, 10)\n",
    ")\n",
    "# ax.get_legend().remove()\n",
    "plt.xlabel(\"Training Steps\")\n",
    "plt.savefig(PLOTS_DIR / \"lab7_ex4_cem_lunarlander_var_wrt_iterations.png\");"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Optimized weights: \", optimized_policy_params)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Test the trained agent"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Task 4:** check the optimized policy."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "VIDEO_PREFIX_EX4_CEM_LUNARLANDER_TRAINED = \"lab7_ex4_cem_lunarlander_tained\"\n",
    "\n",
    "NUM_EPISODES = 3\n",
    "\n",
    "file_path_list = [\n",
    "    FIGS_DIR / f\"{VIDEO_PREFIX_EX4_CEM_LUNARLANDER_TRAINED}-episode-{episode_index}.mp4\"\n",
    "    for episode_index in range(NUM_EPISODES)\n",
    "]\n",
    "\n",
    "for file_path in file_path_list:\n",
    "    file_path.unlink(missing_ok=True)\n",
    "\n",
    "env = gym.make(\"LunarLander-v3\", continuous=True, render_mode=\"rgb_array\")\n",
    "env = gym.wrappers.RecordVideo(\n",
    "    env,\n",
    "    video_folder=str(FIGS_DIR),\n",
    "    name_prefix=VIDEO_PREFIX_EX4_CEM_LUNARLANDER_TRAINED,\n",
    "    episode_trigger=lambda x: True,\n",
    ")\n",
    "env = gym.wrappers.RecordEpisodeStatistics(env, buffer_length=NUM_EPISODES)\n",
    "\n",
    "objective_function = ObjectiveFunction(env=env, policy=nn_policy)\n",
    "\n",
    "objective_function.eval(optimized_policy_params, num_episodes=NUM_EPISODES, max_time_steps=500)\n",
    "\n",
    "# print(f'Episode time taken: {env.time_queue}')\n",
    "# print(f'Episode total rewards: {env.return_queue}')\n",
    "# print(f'Episode lengths: {env.length_queue}')\n",
    "\n",
    "env.close()\n",
    "\n",
    "print(\"\\nSelect the episode to play here ðŸ‘‡\\n\")\n",
    "\n",
    "interact(video_selector, file_path=file_path_list);"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Train the (1+1)-SA-ES agent on the LunarLander environment"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Task 5:** Train the agent using the (1+1)-SA-ES algorithm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "env = gym.make(\"LunarLander-v3\", continuous=True)\n",
    "\n",
    "nn_policy = NeuralNetworkPolicy(\n",
    "    observations_size=env.observation_space.shape[0],\n",
    "    actions_size=env.action_space.shape[0],\n",
    "    hidden_size=16,\n",
    ")\n",
    "\n",
    "objective_function = ObjectiveFunction(\n",
    "    env=env,\n",
    "    policy=nn_policy,\n",
    "    num_episodes=5,  # <- resampling\n",
    "    max_time_steps=500,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "\n",
    "hist_dict = {}\n",
    "\n",
    "num_params = len(nn_policy.get_params())\n",
    "\n",
    "initial_solution_array = np.random.random(num_params)\n",
    "initial_sigma_array = np.ones(num_params) * 1.0\n",
    "\n",
    "optimized_policy_params = saes_1_1(\n",
    "    objective_function=objective_function,\n",
    "    x_array=initial_solution_array,\n",
    "    sigma_array=initial_sigma_array,\n",
    "    max_iterations=1000,\n",
    "    print_every=100,\n",
    "    success_score=-200,\n",
    "    num_evals_for_stop=None,\n",
    "    hist_dict=hist_dict,\n",
    ")\n",
    "\n",
    "objective_function.env.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "lines_to_next_cell": 0
   },
   "outputs": [],
   "source": [
    "df = pd.DataFrame.from_dict(hist_dict, orient=\"index\")\n",
    "ax = df.iloc[:, 0].plot(title=\"Average reward\", figsize=(20, 5))\n",
    "plt.xlabel(\"Training Steps\")\n",
    "plt.ylabel(\"Reward\")\n",
    "plt.savefig(PLOTS_DIR / \"lab7_ex4_saes_lunarlander_avg_reward_wrt_iterations.png\");"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "lines_to_next_cell": 0
   },
   "outputs": [],
   "source": [
    "ax = df.iloc[:, 1:66].plot(\n",
    "    title=\"Theta w.r.t training steps\", legend=None, figsize=(20, 10)\n",
    ")\n",
    "# ax.get_legend().remove()\n",
    "plt.xlabel(\"Training Steps\")\n",
    "plt.savefig(PLOTS_DIR / \"lab7_ex4_saes_lunarlander_params_wrt_iterations.png\");"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "lines_to_next_cell": 0
   },
   "outputs": [],
   "source": [
    "ax = df.iloc[:, 67:].plot(\n",
    "    logy=True, title=\"Variance w.r.t training steps\", legend=None, figsize=(20, 10)\n",
    ")\n",
    "# ax.get_legend().remove()\n",
    "plt.xlabel(\"Training Steps\")\n",
    "plt.savefig(PLOTS_DIR / \"lab7_ex4_saes_lunarlander_var_wrt_iterations.png\");"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Optimized weights: \", optimized_policy_params)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Test the trained agent"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Task 6:** check the optimized policy."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "lines_to_next_cell": 2
   },
   "outputs": [],
   "source": [
    "VIDEO_PREFIX_EX4_SAES_LUNARLANDER_TRAINED = \"lab7_ex4_saes_lunarlander_tained\"\n",
    "\n",
    "NUM_EPISODES = 3\n",
    "\n",
    "file_path_list = [\n",
    "    FIGS_DIR / f\"{VIDEO_PREFIX_EX4_SAES_LUNARLANDER_TRAINED}-episode-{episode_index}.mp4\"\n",
    "    for episode_index in range(NUM_EPISODES)\n",
    "]\n",
    "\n",
    "for file_path in file_path_list:\n",
    "    file_path.unlink(missing_ok=True)\n",
    "\n",
    "env = gym.make(\"LunarLander-v3\", continuous=True, render_mode=\"rgb_array\")\n",
    "env = gym.wrappers.RecordVideo(\n",
    "    env,\n",
    "    video_folder=str(FIGS_DIR),\n",
    "    name_prefix=VIDEO_PREFIX_EX4_SAES_LUNARLANDER_TRAINED,\n",
    "    episode_trigger=lambda x: True,\n",
    ")\n",
    "env = gym.wrappers.RecordEpisodeStatistics(env, buffer_length=NUM_EPISODES)\n",
    "\n",
    "objective_function = ObjectiveFunction(env=env, policy=nn_policy)\n",
    "\n",
    "objective_function.eval(optimized_policy_params, num_episodes=NUM_EPISODES, max_time_steps=500)\n",
    "\n",
    "# print(f'Episode time taken: {env.time_queue}')\n",
    "# print(f'Episode total rewards: {env.return_queue}')\n",
    "# print(f'Episode lengths: {env.length_queue}')\n",
    "\n",
    "env.close()\n",
    "\n",
    "print(\"\\nSelect the episode to play here ðŸ‘‡\\n\")\n",
    "\n",
    "interact(video_selector, file_path=file_path_list);"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Bonus exercise 1: implement CEM with an alternative *Proposal distribution*"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "lines_to_next_cell": 2
   },
   "source": [
    "Implement CEM with the following alternative *Proposal distribution*: a multivariate normal distribution parametrized by a mean vector and **a covariance matrix** (to use correlations in the search space dimensions).\n",
    "\n",
    "Test it on the CartPole and the LunarLander environments."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def cem_correlated(\n",
    "    objective_function: Callable[[np.ndarray], float],\n",
    "    mean_array: np.ndarray,\n",
    "    var_array: np.ndarray,\n",
    "    max_iterations: int = 500,\n",
    "    sample_size: int = 50,\n",
    "    elite_frac: float = 0.2,\n",
    "    print_every: int = 10,\n",
    "    success_score: float = float(\"inf\"),\n",
    "    num_evals_for_stop: Optional[int] = None,\n",
    "    hist_dict: Optional[dict] = None,\n",
    ") -> np.ndarray:\n",
    "    \"\"\"Cross-entropy method.\n",
    "\n",
    "    Params\n",
    "    ======\n",
    "        objective_function (function): the function to maximize\n",
    "        mean_array (array of floats): the initial proposal distribution (mean vector)\n",
    "        var_array (array of floats): the initial proposal distribution (variance vector)\n",
    "        max_iterations (int): number of training iterations\n",
    "        sample_size (int): size of population at each iteration\n",
    "        elite_frac (float): rate of top performers to use in update with elite_frac âˆˆ ]0;1]\n",
    "        print_every (int): how often to print average score\n",
    "        hist_dict (dict): logs\n",
    "    \"\"\"\n",
    "    assert 0.0 < elite_frac <= 1.0\n",
    "\n",
    "    n_elite = math.ceil(sample_size * elite_frac)\n",
    "\n",
    "    cov_array = np.diag(var_array)\n",
    "\n",
    "    for iteration_index in range(0, max_iterations):\n",
    "        # SAMPLE A NEW POPULATION OF SOLUTIONS (X VECTORS) ####################\n",
    "        x_array = np.random.multivariate_normal(mean_array, cov_array, sample_size)\n",
    "\n",
    "        # EVALUATE SAMPLES AND EXTRACT THE BEST ONES (\"ELITE\") ################\n",
    "        score_array = np.array([objective_function(x) for x in x_array])\n",
    "\n",
    "        sorted_indices_array = np.argsort(score_array)  # Sort from the lower score to the higher one\n",
    "        elite_indices_array = sorted_indices_array[:n_elite]  # Recall: we *minimize* the objective function thus we take the samples that are at the beginning of the sorted_indices\n",
    "\n",
    "        elite_x_array = x_array[elite_indices_array]\n",
    "\n",
    "        # FIT THE NORMAL DISTRIBUTION ON THE ELITE POPULATION #################\n",
    "        mean_array = np.mean(elite_x_array, axis=0)\n",
    "        cov_array = np.cov(elite_x_array, rowvar=False)\n",
    "        score = np.mean(score_array[elite_indices_array])\n",
    "\n",
    "        # PRINT STATUS ########################################################\n",
    "        if iteration_index % print_every == 0:\n",
    "            print(\"Iteration {}\\tScore {}\".format(iteration_index, score))\n",
    "\n",
    "        if hist_dict is not None:\n",
    "            hist_dict[iteration_index] = [score] + mean_array.tolist() + cov_array.flatten().tolist()\n",
    "\n",
    "        # STOPPING CRITERIA ####################################################\n",
    "        if num_evals_for_stop is not None:\n",
    "            score = np.mean(score_array[:num_evals_for_stop])\n",
    "\n",
    "        # `num_evals_for_stop = None` may be used to fasten computations but it introduces bias...\n",
    "        if score <= success_score:\n",
    "            break\n",
    "\n",
    "    return mean_array\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Test on CartPole"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "env = gym.make(\"CartPole-v1\")\n",
    "\n",
    "nn_policy = LogisticRegression(env.observation_space.shape[0])\n",
    "\n",
    "objective_function = ObjectiveFunction(\n",
    "    env=env, policy=nn_policy, num_episodes=10, max_time_steps=1000\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "\n",
    "hist_dict = {}\n",
    "\n",
    "num_params = len(nn_policy.get_params())\n",
    "\n",
    "init_mean_array = np.random.random(num_params)\n",
    "init_var_array = np.ones(num_params) * 100.0\n",
    "\n",
    "optimized_policy_params = cem_correlated(\n",
    "    objective_function=objective_function,\n",
    "    mean_array=init_mean_array,\n",
    "    var_array=init_var_array,\n",
    "    max_iterations=30,\n",
    "    sample_size=50,\n",
    "    elite_frac=0.1,\n",
    "    print_every=1,\n",
    "    success_score=-500,\n",
    "    num_evals_for_stop=None,\n",
    "    hist_dict=hist_dict,\n",
    ")\n",
    "\n",
    "objective_function.env.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "lines_to_next_cell": 0
   },
   "outputs": [],
   "source": [
    "df = pd.DataFrame.from_dict(\n",
    "    hist_dict,\n",
    "    orient=\"index\",\n",
    "    columns=[\"score\"]\n",
    "    + [f\"mu{i}\" for i in range(1, 5)]\n",
    "    + [f\"cov{i}{j}\" for i in range(1, 5) for j in range(1, 5)],\n",
    ")\n",
    "ax = df.score.plot(title=\"Average reward\", figsize=(20, 5))\n",
    "plt.xlabel(\"Training Steps\")\n",
    "plt.ylabel(\"Reward\")\n",
    "plt.savefig(PLOTS_DIR / \"lab7_ex5_cem_cartpole_avg_reward_wrt_iterations.png\");"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "lines_to_next_cell": 0
   },
   "outputs": [],
   "source": [
    "ax = df[[\"mu1\", \"mu2\", \"mu3\", \"mu4\"]].plot(\n",
    "    title=\"Theta w.r.t training steps\", figsize=(20, 5)\n",
    ")\n",
    "plt.xlabel(\"Training Steps\")\n",
    "plt.savefig(PLOTS_DIR / \"lab7_ex5_cem_cartpole_params_wrt_iterations.png\");"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "lines_to_next_cell": 0
   },
   "outputs": [],
   "source": [
    "ax = df[[f\"cov{i}{j}\" for i in range(1, 5) for j in range(1, 5)]].plot(\n",
    "    logy=True, title=\"Variance w.r.t training steps\", figsize=(20, 5)\n",
    ")\n",
    "plt.xlabel(\"Training Steps\")\n",
    "plt.savefig(PLOTS_DIR / \"lab7_ex5_cem_cartpole_var_wrt_iterations.png\");"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Optimized weights: \", optimized_policy_params)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "VIDEO_PREFIX_EX5_CEM_CARTPOLE_TRAINED = \"lab7_ex5_cem_cartpole_tained\"\n",
    "\n",
    "NUM_EPISODES = 3\n",
    "\n",
    "file_path_list = [\n",
    "    FIGS_DIR / f\"{VIDEO_PREFIX_EX5_CEM_CARTPOLE_TRAINED}-episode-{episode_index}.mp4\"\n",
    "    for episode_index in range(NUM_EPISODES)\n",
    "]\n",
    "\n",
    "for file_path in file_path_list:\n",
    "    file_path.unlink(missing_ok=True)\n",
    "\n",
    "env = gym.make(\"CartPole-v1\", render_mode=\"rgb_array\")\n",
    "env = gym.wrappers.RecordVideo(\n",
    "    env,\n",
    "    video_folder=str(FIGS_DIR),\n",
    "    name_prefix=VIDEO_PREFIX_EX5_CEM_CARTPOLE_TRAINED,\n",
    "    episode_trigger=lambda x: True,\n",
    ")\n",
    "env = gym.wrappers.RecordEpisodeStatistics(env, buffer_length=NUM_EPISODES)\n",
    "\n",
    "objective_function = ObjectiveFunction(env=env, policy=nn_policy)\n",
    "\n",
    "objective_function.eval(optimized_policy_params, num_episodes=NUM_EPISODES, max_time_steps=500)\n",
    "\n",
    "env.close()\n",
    "\n",
    "# print(f'Episode time taken: {env.time_queue}')\n",
    "# print(f'Episode total rewards: {env.return_queue}')\n",
    "# print(f'Episode lengths: {env.length_queue}')\n",
    "\n",
    "print(\"\\nSelect the episode to play here ðŸ‘‡\\n\")\n",
    "\n",
    "interact(video_selector, file_path=file_path_list);"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Bonus exercise 2: test the CMAES algorithm"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "PyCMA: Python implementation of CMA-ES (from Nikolaus Hansen - CMAP).\n",
    "\n",
    "Source code:\n",
    "\n",
    "- http://cma.gforge.inria.fr/cmaes_sourcecode_page.html#python\n",
    "- https://github.com/CMA-ES/pycma\n",
    "- https://pypi.org/project/cma/\n",
    "\n",
    "Official documentation:\n",
    "\n",
    "- http://cma.gforge.inria.fr/apidocs-pycma/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import cma"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "env = gym.make(\"LunarLander-v3\", continuous=True)\n",
    "\n",
    "nn_policy = NeuralNetworkPolicy(\n",
    "    observations_size=env.observation_space.shape[0],\n",
    "    actions_size=env.action_space.shape[0],\n",
    "    hidden_size=16,\n",
    ")\n",
    "\n",
    "objective_function = ObjectiveFunction(\n",
    "    env=env, policy=nn_policy, num_episodes=1, max_time_steps=500\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "\n",
    "num_params = len(nn_policy.get_params())\n",
    "\n",
    "x_optimal, es = cma.fmin2(\n",
    "    objective_function,\n",
    "    x0=np.random.random(num_params),\n",
    "    sigma0=10.0,\n",
    "    options={\"maxfevals\": 1500},\n",
    ")\n",
    "optimized_policy_params = x_optimal"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "optimized_policy_params"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sns.set_context(\"talk\")\n",
    "plt.rcParams[\"figure.figsize\"] = 20, 10\n",
    "\n",
    "cma.plot();  # shortcut for es.logger.plot()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Test the final policy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "VIDEO_PREFIX_EX6_CMAES_LUNARLANDER_TRAINED = \"lab7_ex6_cmaes_lunarlander_tained\"\n",
    "\n",
    "NUM_EPISODES = 3\n",
    "\n",
    "file_path_list = [\n",
    "    FIGS_DIR / f\"{VIDEO_PREFIX_EX6_CMAES_LUNARLANDER_TRAINED}-episode-{episode_index}.mp4\"\n",
    "    for episode_index in range(NUM_EPISODES)\n",
    "]\n",
    "\n",
    "for file_path in file_path_list:\n",
    "    file_path.unlink(missing_ok=True)\n",
    "\n",
    "env = gym.make(\"LunarLander-v3\", continuous=True, render_mode=\"rgb_array\")\n",
    "env = gym.wrappers.RecordVideo(\n",
    "    env,\n",
    "    video_folder=str(FIGS_DIR),\n",
    "    name_prefix=VIDEO_PREFIX_EX6_CMAES_LUNARLANDER_TRAINED,\n",
    "    episode_trigger=lambda x: True,\n",
    ")\n",
    "env = gym.wrappers.RecordEpisodeStatistics(env, buffer_length=NUM_EPISODES)\n",
    "\n",
    "objective_function = ObjectiveFunction(env=env, policy=nn_policy)\n",
    "\n",
    "objective_function.eval(optimized_policy_params, num_episodes=NUM_EPISODES, max_time_steps=500)\n",
    "\n",
    "env.close()\n",
    "\n",
    "# print(f'Episode time taken: {env.time_queue}')\n",
    "# print(f'Episode total rewards: {env.return_queue}')\n",
    "# print(f'Episode lengths: {env.length_queue}')\n",
    "\n",
    "print(\"\\nSelect the episode to play here ðŸ‘‡\\n\")\n",
    "\n",
    "interact(video_selector, file_path=file_path_list);"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
