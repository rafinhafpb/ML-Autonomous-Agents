{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "c1df705c",
   "metadata": {},
   "source": [
    "# CSC-52081-EP Lab5: Reinforcement Learning - TD Learning, QLearning and SARSA\n",
    "\n",
    "<img src=\"https://raw.githubusercontent.com/jeremiedecock/polytechnique-csc-52081-ep-2025-students/refs/heads/main/assets/logo.jpg\" style=\"float: left; width: 15%\" />\n",
    "\n",
    "[CSC-52081-EP-2025](https://moodle.polytechnique.fr/course/view.php?id=19336) Lab session #5\n",
    "\n",
    "2019-2025 JÃ©rÃ©mie Decock"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4ab28542",
   "metadata": {},
   "source": [
    "[![Open in Google Colab](https://colab.research.google.com/assets/colab-badge.svg)](https://colab.research.google.com/github/jeremiedecock/polytechnique-csc-52081-ep-2025-students/blob/main/lab5_rl2_tdlearning_qlearning_sarsa.ipynb)\n",
    "\n",
    "[![My Binder](https://mybinder.org/badge_logo.svg)](https://mybinder.org/v2/gh/jeremiedecock/polytechnique-csc-52081-ep-2025-students/main?filepath=lab5_rl2_tdlearning_qlearning_sarsa.ipynb)\n",
    "\n",
    "[![NbViewer](https://raw.githubusercontent.com/jupyter/design/main/logos/Badges/nbviewer_badge.svg)](https://nbviewer.jupyter.org/github/jeremiedecock/polytechnique-csc-52081-ep-2025-students/blob/main/lab5_rl2_tdlearning_qlearning_sarsa.ipynb)\n",
    "\n",
    "[![Local](https://img.shields.io/badge/Local-Save%20As...-blue)](https://github.com/jeremiedecock/polytechnique-csc-52081-ep-2025-students/raw/main/lab5_rl2_tdlearning_qlearning_sarsa.ipynb)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1c4a5ad5",
   "metadata": {},
   "source": [
    "## Introduction\n",
    "\n",
    "The purpose of this lab is to introduce some classic concepts used\n",
    "in reinforcement learning: *Temporal Difference Learning* (*TD Learning*), *QLearning* and *SARSA*.\n",
    "\n",
    "As for previous lab, you can either:\n",
    "- open, edit and execute the notebook in *Google Colab* following this link: https://colab.research.google.com/github/jeremiedecock/polytechnique-csc-52081-ep-2025-students/blob/main/lab5_rl2_tdlearning_qlearning_sarsa.ipynb ; this is the **recommended** choice as you have nothing to install on your computer\n",
    "- open, edit and execute the notebook in *MyBinder* (if for any reason the Google Colab solution doesn't work): https://mybinder.org/v2/gh/jeremiedecock/polytechnique-csc-52081-ep-2025-students/main?filepath=lab5_rl2_tdlearning_qlearning_sarsa.ipynb\n",
    "- download, edit and execute the notebook on your computer if Python3 and JypyterLab are already installed: https://github.com/jeremiedecock/polytechnique-csc-52081-ep-2025-students/raw/main/lab5_rl2_tdlearning_qlearning_sarsa.ipynb\n",
    "\n",
    "If you work with Google Colab or MyBinder, **remember to save or download your work regularly or you may lose it!**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8807b571-eeaa-47af-ad48-1ff4eeed988f",
   "metadata": {},
   "source": [
    "## Lab Submission\n",
    "\n",
    "Please submit your completed notebook in [Moodle : \"Lab 5 - Submission\"](https://moodle.polytechnique.fr/course/section.php?id=66533).\n",
    "\n",
    "### Submission Guidelines\n",
    "\n",
    "1. **File Naming:** Rename your notebook as follows: **`firstname_lastname-05.ipynb`** where `firstname` and `lastname` match your email address. *Example: `jesse_read-05.ipynb`*\n",
    "2. **Clear Output Cells:** To reduce file size (**must be under 500 KB**), clear all output cells before submitting. This includes rendered images, videos, plots, and dataframes...\n",
    "   - **JupyterLab:**\n",
    "     - Click **\"Kernel\" â†’ \"Restart Kernel and Clear Outputs of All Cells...\"**\n",
    "     - Then go to **\"File\" â†’ \"Save Notebook As...\"**\n",
    "   - **Google Colab:**\n",
    "     - Click **\"Edit\" â†’ \"Clear all outputs\"**\n",
    "     - Then go to **\"File\" â†’ \"Download\" â†’ \"Download.ipynb\"**\n",
    "   - **VSCode:**\n",
    "     - Click **\"Clear All Outputs\"**\n",
    "     - Then **save your file**\n",
    "3. **Upload Your File:** Only **`.ipynb`** files are accepted.\n",
    "\n",
    "**Note:** Bonus parts (if any) are optional, as their name suggests.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1a1ed8af-7123-45d1-a950-085ec3de47bd",
   "metadata": {},
   "source": [
    "## Setup the Python environment"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "91305cc2-fa83-41fa-9a91-77f7fa41c994",
   "metadata": {},
   "source": [
    "This notebook relies on several libraries including `gymnasium[other]` (v1.0.0), `ipywidgets`, `matplotlib`, `numpy`, `pandas`, `pygame`, `seaborn`, and `tqdm`.\n",
    "A complete list of dependencies can be found in the following [requirements-lab5.txt](https://raw.githubusercontent.com/jeremiedecock/polytechnique-csc-52081-ep-2025-students/main/requirements-lab5.txt) file."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2247e545-5d2d-49e4-91a9-db22b10a1e4a",
   "metadata": {},
   "source": [
    "### If you use Google Colab\n",
    "\n",
    "If you use Google Colab, execute the next cell to install required libraries."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bca0f294-a82d-4a81-8d5f-8ed8d46e00f0",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "xt66Z85AmOI2",
    "outputId": "bd7a6b75-ad3c-4be1-d560-8239fbc0e9d2"
   },
   "outputs": [],
   "source": [
    "import sys, subprocess\n",
    "\n",
    "def is_colab():\n",
    "    return \"google.colab\" in sys.modules\n",
    "\n",
    "def run_subprocess_command(cmd):\n",
    "    # run the command\n",
    "    process = subprocess.Popen(cmd.split(), stdout=subprocess.PIPE)\n",
    "    # print the output\n",
    "    for line in process.stdout:\n",
    "        print(line.decode().strip())\n",
    "\n",
    "if is_colab():\n",
    "    # run_subprocess_command(\"apt install xvfb x11-utils\")\n",
    "    run_subprocess_command(\"pip install -r https://raw.githubusercontent.com/jeremiedecock/polytechnique-csc-52081-ep-2025-students/main/requirements-lab5-google-colab.txt\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4a1993b8-5910-46a1-b56e-dbf69fad72ab",
   "metadata": {},
   "source": [
    "### If you have downloaded the notebook on your computer and execute it in your own Python environment"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "66a4d7b2-3ec0-4da2-8149-6604629953a1",
   "metadata": {},
   "source": [
    "To set up the necessary dependencies, run the following command to establish a [Python virtual environment (venv)](https://docs.python.org/3/library/venv.html) that includes all the essential libraries for this lab.\n",
    "\n",
    "#### On Posix systems (Linux, MacOSX, WSL, ...)\n",
    "\n",
    "```bash\n",
    "python3 -m venv env-lab5\n",
    "source env-lab5/bin/activate\n",
    "python3 -m pip install --upgrade pip\n",
    "python3 -m pip install -r https://raw.githubusercontent.com/jeremiedecock/polytechnique-csc-52081-ep-2025-students/main/requirements-lab5.txt\n",
    "```\n",
    "\n",
    "#### On Windows\n",
    "\n",
    "```bash\n",
    "python3 -m venv env-lab5\n",
    "env-lab5\\Scripts\\activate.bat\n",
    "python3 -m pip install --upgrade pip\n",
    "python3 -m pip install -r https://raw.githubusercontent.com/jeremiedecock/polytechnique-csc-52081-ep-2025-students/main/requirements-lab5.txt\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e9b78154-83bc-46d8-a4dc-f332f57d20f8",
   "metadata": {},
   "source": [
    "### Import required packages"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b0e519bd",
   "metadata": {},
   "outputs": [],
   "source": [
    "import gymnasium as gym\n",
    "import math\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from pathlib import Path\n",
    "\n",
    "from typing import cast, List, Optional, Sequence, Tuple, Union"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3fbc95c6-84bb-4236-bc5b-7d4ecb2be53c",
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "\n",
    "import matplotlib\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "import seaborn as sns\n",
    "from tqdm.notebook import tqdm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8ec9c23a-14f9-4d87-ab05-ec5cd2c83035",
   "metadata": {},
   "outputs": [],
   "source": [
    "from IPython.display import Video"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "59f3f924",
   "metadata": {},
   "outputs": [],
   "source": [
    "sns.set_context(\"talk\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "334a1eab",
   "metadata": {},
   "source": [
    "## Setup the FrozenLake toy problem with Gymnasium"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "edd010fb",
   "metadata": {},
   "source": [
    "For the purpose of focusing on the algorithms, we will use standard environments provided\n",
    "by the Gymnasium framework. Especially, as in Lab 4, we will try to solve the FrozenLake-v1\n",
    "problem (https://gymnasium.farama.org/environments/toy_text/frozen_lake/).\n",
    "As a reminder, this environment is described [here](https://gymnasium.farama.org/environments/toy_text/frozen_lake/).\n",
    "\n",
    "The action indices are outlined below:\n",
    "\n",
    "| Action Index | Action     |\n",
    "|--------------|------------|\n",
    "| 0            | Move Left  |\n",
    "| 1            | Move Down  |\n",
    "| 2            | Move Right |\n",
    "| 3            | Move Up    |"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "495c5593",
   "metadata": {},
   "source": [
    "The following dictionary may be used to understand actions:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6cf072d9",
   "metadata": {
    "lines_to_next_cell": 2
   },
   "outputs": [],
   "source": [
    "action_labels = {0: \"Move Left\", 1: \"Move Down\", 2: \"Move Right\", 3: \"Move Up\"}"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c3173772",
   "metadata": {},
   "source": [
    "**Notice**: this environment is *fully observable*, thus here the terms (environment) *state* and (agent) *observation* are equivalent.\n",
    "This is not always the case for example in poker, the agent doesn't know the opponent's cards."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5d1c5fbc",
   "metadata": {},
   "source": [
    "### Display functions"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "55762df7",
   "metadata": {
    "lines_to_next_cell": 2
   },
   "source": [
    "The next cells contain three functions that can be used to display Q-tables, states and (greedy) policies in the FrozenLake environment."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "435e02df",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Display the Q-table as a set of heatmaps, one for each action\n",
    "def display_qtable(\n",
    "    q_array: np.ndarray,\n",
    "    title: str = \"\",\n",
    "    figsize: Tuple[int, int] = (4, 4),\n",
    "    annot: bool = True,\n",
    "    fmt: str = \"0.1f\",\n",
    "    linewidths: float = 0.5,\n",
    "    square: bool = True,\n",
    "    cbar: bool = False,\n",
    "    cmap: str = \"Reds\",\n",
    "    ticklabels: bool = False,\n",
    ") -> None:\n",
    "    \"\"\"\n",
    "    Display a Q-table as a set of heatmaps, one for each action.\n",
    "\n",
    "    For the frozen lake environment, there are 16 states and 4 actions thus this function will display 4 heatmaps, one for each action.\n",
    "    Each heatmap will display the Q-values for each state when performing the action indexed by the heatmap.\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    q_array : np.ndarray\n",
    "        The Q-table to display. A 2D numpy array of 16x4 elements.\n",
    "        Each row corresponds to a state, and each column corresponds to an action.\n",
    "        In the frozen lake environment, there are 16 states and 4 actions thus the Q-table has a shape of (16, 4).\n",
    "        For instance, q_array[0, 3] is the Q-value (estimation of the expected reward) for performing action 3 (\"move up\") in state 0 (the top left square).\n",
    "    title : str, optional\n",
    "        The title of the plot. Default is an empty string.\n",
    "    figsize : tuple, optional\n",
    "        The size of the figure (in inches), by default (4, 4)\n",
    "    annot : bool, optional\n",
    "        If True, write the data value in each cell, by default True\n",
    "    fmt : str, optional\n",
    "        The string formatting code to use when adding annotations, by default \"0.1f\" that will display a single decimal\n",
    "    linewidths : float, optional\n",
    "        The width of the lines that will divide each cell, by default .5\n",
    "    square : bool, optional\n",
    "        Whether to set the Axes aspect to \"equal\" so each cell is square-shaped, by default True\n",
    "    cbar : bool, optional\n",
    "        Whether to draw a colorbar, by default False\n",
    "    cmap : str, optional\n",
    "        The mapping from data values to color space, by default \"Reds\".\n",
    "    heatmap : bool, optional\n",
    "        If True, display the data as a heatmap. Default is True.\n",
    "    ticklabels : bool, optional\n",
    "        If True, display the tick labels. Default is False.\n",
    "\n",
    "    Returns\n",
    "    -------\n",
    "    None\n",
    "    \"\"\"\n",
    "    # Get the number of actions from the shape of the Q-table\n",
    "    num_actions = q_array.shape[1]\n",
    "\n",
    "    # Adjust the figure size (in inches) based on the number of actions\n",
    "    global_figsize = list(figsize)\n",
    "    global_figsize[0] *= num_actions\n",
    "\n",
    "    # Create a subplot for each action\n",
    "    fig, ax_list = plt.subplots(ncols=num_actions, figsize=global_figsize)\n",
    "\n",
    "    # For each action, display the Q-values for all states as a heatmap\n",
    "    for action_index in range(num_actions):\n",
    "        ax = ax_list[action_index]\n",
    "\n",
    "        # Retrieve the Q-values for each state when performing the action indexed by \"action_index\".\n",
    "        # This forms a 1D array, state_vec, where state_vec[i] = Q(i, action_index).\n",
    "        state_vec = q_array[:, action_index]\n",
    "\n",
    "        # Display the Q-values for each state when performing the action indexed by \"action_index\"\n",
    "        # i.e. display Q(., action_index)\n",
    "        display_state(\n",
    "            state_vec,\n",
    "            title=r\"$Q(\\cdot,a_{})$\".format(action_index),\n",
    "            # title=r\"$Q(\\cdot,a_{})$ {}\".format(action_index, action_labels[action_index]),\n",
    "            figsize=figsize,\n",
    "            annot=annot,\n",
    "            fmt=fmt,\n",
    "            linewidths=linewidths,\n",
    "            square=square,\n",
    "            cbar=cbar,\n",
    "            cmap=cmap,\n",
    "            ticklabels=ticklabels,\n",
    "            ax=ax,\n",
    "        )\n",
    "\n",
    "    # Set the title for the entire figure\n",
    "    plt.suptitle(title)\n",
    "    # Display the figure\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f9a2915a",
   "metadata": {},
   "outputs": [],
   "source": [
    "def display_state(\n",
    "    state_seq: Union[List[int], List[float], np.ndarray],\n",
    "    title: str = \"\",\n",
    "    figsize: Tuple[int, int] = (5, 5),\n",
    "    annot: bool = True,\n",
    "    fmt: str = \"0.1f\",\n",
    "    linewidths: float = 0.5,\n",
    "    square: bool = True,\n",
    "    cbar: bool = False,\n",
    "    cmap: str = \"Reds\",\n",
    "    ticklabels: bool = False,\n",
    "    ax: Optional[matplotlib.axes.Axes] = None,\n",
    ") -> Union[matplotlib.axes.Axes, None]:\n",
    "    \"\"\"\n",
    "    Display the expected values of all states as a heatmap.\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    state_seq : list of int, list of float or 1D numpy array of 16 elements\n",
    "        The sequence of expected values to display. This can be a list, a 1D array, etc.\n",
    "        Each element is the estimation of the expected value of the corresponding state.\n",
    "        For example, state_seq[0] is the estimation of the expected value of the first state.\n",
    "        There are 16 elements in this sequence for the frozenlake environment, i.e., one per state of the environment.\n",
    "    title : str, optional\n",
    "        The title of the plot, by default None\n",
    "    figsize : tuple, optional\n",
    "        The size of the figure (in inches), by default (5, 5)\n",
    "    annot : bool, optional\n",
    "        If True, write the data value in each cell, by default True\n",
    "    fmt : str, optional\n",
    "        The string formatting code to use when adding annotations, by default \"0.1f\"\n",
    "    linewidths : float, optional\n",
    "        The width of the lines that will divide each cell, by default .5\n",
    "    square : bool, optional\n",
    "        Whether to set the Axes aspect to \"equal\" so each cell is square-shaped, by default True\n",
    "    cbar : bool, optional\n",
    "        Whether to draw a colorbar, by default False\n",
    "    cmap : str, optional\n",
    "        The mapping from data values to color space, by default \"Reds\".\n",
    "    ticklabels : bool, optional\n",
    "        If True, display the tick labels. Default is False.\n",
    "    ax : matplotlib.axes.Axes, optional\n",
    "        The axes object to draw the heatmap on, by default None\n",
    "\n",
    "    Returns\n",
    "    -------\n",
    "    matplotlib.axes.Axes, optional\n",
    "        The axes object with the heatmap if one was provided, otherwise None.\n",
    "    \"\"\"\n",
    "    # Calculate the size of the state array\n",
    "    size = int(math.sqrt(len(state_seq)))\n",
    "\n",
    "    # Convert the state sequence to a numpy array (if it isn't already one)\n",
    "    state_array = np.array(state_seq)\n",
    "\n",
    "    # Reshape the state array into a square matrix\n",
    "    # (we assume here that the original frozen lake environment is used,\n",
    "    # thus the state space can be visualized as a square grid)\n",
    "    state_array = state_array.reshape(size, size)\n",
    "\n",
    "    # If no axes object is provided, create a new figure and axes\n",
    "    if ax is None:\n",
    "        fig, ax = plt.subplots(figsize=figsize)\n",
    "\n",
    "    # Create a heatmap of the state array on the axes\n",
    "    ax = sns.heatmap(\n",
    "        state_array,\n",
    "        annot=annot,\n",
    "        fmt=fmt,\n",
    "        linewidths=linewidths,\n",
    "        square=square,\n",
    "        cbar=cbar,\n",
    "        cmap=cmap,\n",
    "        xticklabels=ticklabels,\n",
    "        yticklabels=ticklabels,\n",
    "        ax=ax,\n",
    "    )\n",
    "\n",
    "    # If a title is provided, set the title of the plot\n",
    "    if title != \"\":\n",
    "        if ax is None:\n",
    "            plt.title(title)\n",
    "        else:\n",
    "            ax.set_title(title)\n",
    "\n",
    "    # If no axes object was provided, display the plot\n",
    "    # Otherwise, return the axes object with the heatmap\n",
    "    if ax is None:\n",
    "        plt.show()\n",
    "        return None\n",
    "    else:\n",
    "        return ax"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3c532444-f0e5-4335-a22d-aa7a69bd069e",
   "metadata": {},
   "outputs": [],
   "source": [
    "def display_policy(policy: List[int], environment: gym.Env) -> None:\n",
    "    \"\"\"\n",
    "    Display the policy as a heatmap.\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    policy : list of int\n",
    "        The policy to be displayed. Each integer represents an action to be taken in a state.\n",
    "\n",
    "    Returns\n",
    "    -------\n",
    "    None\n",
    "    \"\"\"\n",
    "    # Create a list of actions with their corresponding labels\n",
    "    actions_src = [\n",
    "        f\"{action}={action_labels[action].replace('Move ', '')}\" for action in range(environment.action_space.n)\n",
    "    ]\n",
    "\n",
    "    # Create a title for the heatmap using the actions and their labels\n",
    "    title = f\"Policy ({', '.join(actions_src)})\"\n",
    "\n",
    "    # Use the display_state function to create a heatmap of the policy\n",
    "    # The fmt parameter is set to \"d\" to display integers, cbar is set to False to not display a colorbar,\n",
    "    # and cmap is set to \"Reds\" to use the Reds color map\n",
    "    display_state(policy, title=title, fmt=\"d\", cbar=False, cmap=\"Reds\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "daeef735",
   "metadata": {},
   "source": [
    "## Exercise 1: TD Learning"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "23fece41-6205-4eb7-a4b9-cc03ff272b92",
   "metadata": {},
   "source": [
    "**Notice**: Here we assume that the reward only depends on the state: $r(\\boldsymbol{s}) \\equiv \\mathcal{r}(\\boldsymbol{s}, \\boldsymbol{a}, \\boldsymbol{s}')$.\n",
    "\n",
    "**Notice**:\n",
    "-  $\\mathcal{S}$ is the set of all nonterminal states\n",
    "-  $\\mathcal{S^F}$ is the set of all terminal states\n",
    "-  $\\mathcal{S^+}$ is the set of all states, includint the terminal states"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "180195e1",
   "metadata": {},
   "source": [
    "In Lab4, we explored Dynamic Programming methods that can be used to solve Markov Decision Process (MDP) when the environment is perfectly known to the agent, i.e., in cases where the agent knows the transition and reward functions in advance. However, this is a strong assumption, as in most practical problems, these functions are not known beforehand. In this lab, we will learn how to create agents that can solve Markov Decision Problems without prior knowledge of the environment. These agents learn the dynamics of their environment by exploring it and use this knowledge to find an optimal policy.\n",
    "\n",
    "We will start with the *TD Learning* (*Temporal Difference Learning*) algorithm, which can be used to **evaluate** any **given policy** (i.e. compute the *value function* of the environment following the given policy).\n",
    "Variants of TD Learning can also be adapted for optimal control, such as in applications like [TD-Gammon](https://en.wikipedia.org/wiki/TD-Gammon).\n",
    "Exercises 2 and 3 also reuse the main concepts of TD Learning to calculate an optimal policy.\n",
    "\n",
    "The algorithm is outlined below.\n",
    "\n",
    "---\n",
    "\n",
    "### The TD Learning algorithm\n",
    "\n",
    "<b>Input</b>:<br>\n",
    "\t$\\quad\\quad$ the policy $\\pi$ to be evaluated<br>\n",
    "<b>Algorithm parameter</b>:<br>\n",
    "\t$\\quad\\quad$ discount factor $\\gamma$<br>\n",
    "\t$\\quad\\quad$ step size $\\alpha \\in (0,1]$<br><br>\n",
    "\n",
    "Initialize $V(\\boldsymbol{s}) ~~~ \\forall \\boldsymbol{s} \\in \\mathcal{S^+}$, arbitrarily except that V(terminal)=0<br>\n",
    "\n",
    "<b>FOR EACH</b> episode<br>\n",
    "\t$\\quad$ $\\boldsymbol{s} \\leftarrow \\text{env.reset}() \\quad\\quad\\quad\\quad\\quad\\quad$ (initialize $s$)<br>\n",
    "\t$\\quad$ <b>DO</b> <br>\n",
    "\t\t$\\quad\\quad$ $\\boldsymbol{a} \\leftarrow \\pi(\\boldsymbol{s})$<br>\n",
    "\t\t$\\quad\\quad$ $\\boldsymbol{s}', r \\leftarrow \\text{env.step}(\\boldsymbol{a}) \\quad\\quad\\quad\\quad$ (take action $a$, observe $r$ and $s'$)<br>\n",
    "\t\t$\\quad\\quad$ $V(\\boldsymbol{s}) \\leftarrow V(\\boldsymbol{s}) + \\alpha \\left[ \\underbrace{\\overbrace{r + \\gamma ~ V(\\boldsymbol{s}')}^{\\text{Target for } V(\\boldsymbol{s})} ~ - ~ V(\\boldsymbol{s})}_{\\text{TD error}} \\right]$<br>\n",
    "\t\t$\\quad\\quad$ $\\boldsymbol{s} \\leftarrow \\boldsymbol{s}'$<br>\n",
    "\t$\\quad$ <b>UNTIL</b> $\\boldsymbol{s}$ is final"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e2666142",
   "metadata": {},
   "source": [
    "**Notice**: in the following cell, `policy` is a list of actions (one per state c.f. two cells bellow)."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c264f90f-b643-4e7d-9153-01b8e49b10e5",
   "metadata": {},
   "source": [
    "### Task 1: Implement the TD Learning algorithm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b05aaf1b",
   "metadata": {},
   "outputs": [],
   "source": [
    "DISPLAY_EVERY_N_EPISODES = 100\n",
    "\n",
    "def td_learning(\n",
    "    policy: Sequence[float],\n",
    "    environment: gym.Env,\n",
    "    alpha: float = 0.1,\n",
    "    alpha_factor: float = 0.995,\n",
    "    gamma: float = 0.95,\n",
    "    num_episodes: int = 1000,\n",
    "    display: bool = False,\n",
    ") -> Tuple[np.ndarray, List[np.ndarray], List[float]]:\n",
    "    \"\"\"\n",
    "    Perform Temporal Difference learning on a given policy and environment.\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    policy : Sequence[float]\n",
    "        The policy to be learned, represented as a sequence mapping states (the index of the sequence) to actions (the value of the sequence for this index).\n",
    "        For example, policy[0] is the action to take in state 0.\n",
    "    environment : gym.Env\n",
    "        The environment in which the agent operates.\n",
    "    alpha : float, optional\n",
    "        The learning rate, between 0 and 1. By default 0.1\n",
    "    alpha_factor : float, optional\n",
    "        The factor by which the learning rate alpha decreases each episode, by default 0.995\n",
    "    gamma : float, optional\n",
    "        The discount factor, between 0 and 1. By default 0.95\n",
    "    num_episodes : int, optional\n",
    "        The number of episodes to run, by default 1000\n",
    "    display : bool, optional\n",
    "        Whether to display the value function (every DISPLAY_EVERY_N_EPISODES episodes), by default False\n",
    "\n",
    "    Returns\n",
    "    -------\n",
    "    np.ndarray\n",
    "        The learned value function. This is a 1D ndarray with a shape of (16,) as there are 16 states in the frozenlake environment.\n",
    "    \"\"\"\n",
    "    # Initialize the history of the value function and the learning rate\n",
    "    v_array_history = []\n",
    "    alpha_history = []\n",
    "\n",
    "    observation_space = cast(gym.spaces.Discrete, environment.observation_space)\n",
    "\n",
    "    # Get the number of states in the environment\n",
    "    num_states = observation_space.n\n",
    "\n",
    "    # Initialize the value function to zeros\n",
    "    v_array = np.zeros(num_states)\n",
    "\n",
    "    # Loop over the episodes\n",
    "    for episode_index in tqdm(range(1, num_episodes)):\n",
    "        # Display the value function every DISPLAY_EVERY_N_EPISODES episodes if display is True\n",
    "        if display and episode_index % DISPLAY_EVERY_N_EPISODES == 0:\n",
    "            display_state(\n",
    "                v_array,\n",
    "                title=f\"Value function (ep. {episode_index})\",\n",
    "                cbar=True,\n",
    "                cmap=\"Reds\",\n",
    "            )\n",
    "\n",
    "        # Save the current value function and learning rate\n",
    "        v_array_history.append(v_array.copy())\n",
    "        alpha_history.append(alpha)\n",
    "\n",
    "        # Decrease the learning rate if alpha_factor is not None\n",
    "        if alpha_factor is not None:\n",
    "            alpha = alpha * alpha_factor\n",
    "\n",
    "        state, _ = environment.reset()\n",
    "\n",
    "        while True:\n",
    "            action = policy[state]\n",
    "            next_state, reward, is_final, _, _ = environment.step(action)\n",
    "            v_array[state] = v_array[state] + alpha * (reward + gamma*v_array[next_state] - v_array[state])\n",
    "            state = next_state\n",
    "\n",
    "            if is_final:\n",
    "                break\n",
    "\n",
    "    # Return the learned value function\n",
    "    return v_array, v_array_history, alpha_history"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5fddba4b",
   "metadata": {},
   "source": [
    "**Note**: In the following cell, the `display` argument can be set to `True` to see the evolution of the value function `v_array` over iterations."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9498322c",
   "metadata": {
    "lines_to_next_cell": 2
   },
   "outputs": [],
   "source": [
    "# Define the policy to evaluate\n",
    "policy = [0, 3, 3, 3,\n",
    "          0, 0, 0, 0,\n",
    "          3, 1, 0, 0,\n",
    "          0, 2, 1, 0]\n",
    "\n",
    "# Create the environment and set the maximum number of steps per episode\n",
    "environment = gym.make(\"FrozenLake-v1\", max_episode_steps=1000)\n",
    "\n",
    "# Apply Temporal Difference (TD) Learning to calculate the value function for the policy defined earlier, within the context of the FrozenLake environment.\n",
    "v_array_ex1, v_array_history_ex1, alpha_history_ex1 = td_learning(policy, environment, display=True)\n",
    "\n",
    "environment.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d41a47b3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Display the learned value function\n",
    "display_state(v_array_ex1, title=\"Value function\", cbar=True, cmap=\"Reds\");"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7caa3bc2",
   "metadata": {},
   "source": [
    "### Display the evolution of the value function over iterations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "139261c4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Print the evolution of the value function during the learning process\n",
    "df_v_hist_ex1 = pd.DataFrame(v_array_history_ex1)\n",
    "df_v_hist_ex1"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1cdee110",
   "metadata": {},
   "source": [
    "Evolution of `v_array_ex1` (the estimated expected value of each state) over iterations (one curve per state):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "74da2d6a",
   "metadata": {
    "lines_to_next_cell": 0
   },
   "outputs": [],
   "source": [
    "# Show the evolution of the estimated expected value for each state during the learning process\n",
    "df_v_hist_ex1.plot(figsize=(14, 8))\n",
    "plt.title(r\"$V^{\\pi}(\\cdot)$ w.r.t iteration\")\n",
    "plt.ylabel(r\"$V^{\\pi}(\\cdot)$\")\n",
    "plt.xlabel(\"iteration\")\n",
    "plt.legend(loc=\"upper right\");"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "182f7b5e",
   "metadata": {
    "lines_to_next_cell": 2
   },
   "outputs": [],
   "source": [
    "# Show the evolution of the alpha parameter during the learning process\n",
    "plt.loglog(alpha_history_ex1)\n",
    "plt.title(\"Alpha w.r.t iteration\")\n",
    "plt.ylabel(\"Alpha\")\n",
    "plt.xlabel(\"iteration\");"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fde6614c-abb9-4e79-9f9f-26237b1a1a7d",
   "metadata": {},
   "source": [
    "### Question 1"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "66e6114c-070b-4eed-a583-2cea4a6f5f85",
   "metadata": {},
   "source": [
    "Copy and paste the output of the following cell into the first question of the [Lab 5 - Evaluation](https://moodle.polytechnique.fr/course/section.php?id=66533) in Moodle:  \n",
    "*\"What is the expected value of the 9th state in the FrozenLake environment for the given policy Ï€ using the default parameters?\"*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fbf8f7a0-dc39-43e8-8e6d-88768ffb3e5f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# %%time\n",
    "\n",
    "NUM_RUNS = 10\n",
    "\n",
    "# Define the policy to evaluate\n",
    "policy = [0, 3, 3, 3,\n",
    "          0, 0, 0, 0,\n",
    "          3, 1, 0, 0,\n",
    "          0, 2, 1, 0]\n",
    "\n",
    "# Create the environment and set the maximum number of steps per episode\n",
    "environment = gym.make(\"FrozenLake-v1\", max_episode_steps=1000)\n",
    "\n",
    "run_list = []\n",
    "for _ in tqdm(range(1, NUM_RUNS)):\n",
    "    v_array, _, _ = td_learning(policy, environment, display=False)\n",
    "    run_list.append(v_array[8].item())\n",
    "\n",
    "environment.close()\n",
    "\n",
    "print(f\"\\n\\nâš ï¸ Copy and paste this value into Moodle ðŸ‘‰ {np.mean(run_list).item():.2f}\\n(it has the required number of decimals)\\n\\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b4245482",
   "metadata": {},
   "source": [
    "### Task 2: The learning rate $\\alpha$ in TD-Learning"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4deadf82-0647-4a58-a87b-1a4f208672fa",
   "metadata": {},
   "source": [
    "In the previous exercise, set `alpha_factor` to 1 then check the algorithm with different `alpha` values between 0.1 and 1."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "94d7a04c-6419-49e1-977a-c7d0c30953ad",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the policy to evaluate\n",
    "policy = [0, 3, 3, 3,\n",
    "          0, 0, 0, 0,\n",
    "          3, 1, 0, 0,\n",
    "          0, 2, 1, 0]\n",
    "\n",
    "# Create the environment and set the maximum number of steps per episode\n",
    "environment = gym.make(\"FrozenLake-v1\", max_episode_steps=1000)\n",
    "\n",
    "# Apply Temporal Difference (TD) Learning to calculate the value function for the policy defined earlier, within the context of the FrozenLake environment.\n",
    "_, v_array_history, _ = td_learning(policy, environment, display=False, alpha_factor=1, alpha=0.1)  # ðŸ‘ˆ Test different `alpha` values here\n",
    "\n",
    "df = pd.DataFrame(v_array_history)\n",
    "\n",
    "# Show the evolution of the estimated expected value for each state during the learning process\n",
    "df.plot(figsize=(14, 8))\n",
    "plt.title(r\"$V^{\\pi}(\\cdot)$ w.r.t iteration\")\n",
    "plt.ylabel(r\"$V^{\\pi}(\\cdot)$\")\n",
    "plt.xlabel(\"iteration\")\n",
    "plt.legend(loc=\"upper right\");\n",
    "\n",
    "environment.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5306051f-e912-4162-91c0-750a2d7a5c36",
   "metadata": {},
   "source": [
    "### Bonus question"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1ce31afc-c2d2-4114-8aff-7ac6061a1460",
   "metadata": {},
   "source": [
    "What do you observe when you change the `alpha_factor` ?\n",
    "What is the role of this parameter ?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "adb26884",
   "metadata": {},
   "source": [
    "The `alpha` parameter controls the learning rate of the algorithm:\n",
    "\n",
    "The TD Error, $\\delta = r + \\gamma V(s') - V(s)$, measures the difference between the current estimate $V(s)$ and the better estimate using $r$ and $V(s')$.\n",
    "\n",
    "If $\\delta > 0, V(s)$ was underestimated, therefore $V(s) = V(s) + \\alpha * \\delta$ will increase the estimated $V(s)$.\n",
    "\n",
    "If $\\delta < 0, V(s)$ was overestimated, therefore $V(s) = V(s) + \\alpha * \\delta$ will decrease the estimated $V(s)$.\n",
    "\n",
    "`alpha` controls how much we update $V(s)$. For small $\\alpha$ values, we have a slow learning, but it's more stable. For a large $\\alpha$, we have faster learning, but can be unstable and difficult to converge.\n",
    "\n",
    "We clearly see that by the graph because, as we increase `alpha`, the fluctuations over iteration becomes more and more important (to the point that just seems like noise)."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "999b07d2",
   "metadata": {},
   "source": [
    "## Exercise 2: SARSA"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a04f74d6-3374-4951-8587-178cb13bbe26",
   "metadata": {},
   "source": [
    "### The Greedy and Epsilon Greedy policies"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2aa503e3",
   "metadata": {
    "lines_to_next_cell": 2
   },
   "source": [
    "In exercise 1, **TD-Learning** has been used to **estimate the value function** for a **given policy**.  \n",
    "In the following exercises, we will now explore how to **find the optimal** (or a nearly optimal) policy.  \n",
    "To do that, we will use two algorithms (*SARSA* and *Q-Learning*) that estimate a *Q-table* instead of a *V-table*.  \n",
    "\n",
    "This *Q-table* provides the expected **cumulative reward** when the agent takes a given action $\\boldsymbol{a}$ from any state $\\boldsymbol{s}$ and then follows a specified *behavior policy* to choose subsequent actions until a terminal state is reached. (This behavior policy may also serve as an *exploration policy* guiding action selection.)  \n",
    "\n",
    "While exploring the environment, the agent **updates** its *Q-table* using a **rule derived from the Bellman equation** (i.e., an *update rule*).  \n",
    "\n",
    "The purpose of this third exercise is to implement the *greedy* and the $\\epsilon$-*greedy* policies that agents will used to explore the environment and update their QTable:\n",
    "\n",
    "$\\displaystyle \\pi^{Q^{\\pi}}(\\boldsymbol{s}) := \\text{greedy}(\\boldsymbol{s}, Q^{\\pi}) = \\arg\\max_{\\boldsymbol{a} \\in \\mathcal{A}} Q^{\\pi}(\\boldsymbol{s}, \\boldsymbol{a})$\n",
    "\n",
    "\n",
    "$\\pi^{Q^{\\pi}}_{\\epsilon}(\\boldsymbol{s}) := \\epsilon\\text{-greedy}(\\boldsymbol{s}, Q^{\\pi}) = $\n",
    "randomly choose between $\\underbrace{\\text{greedy}(\\boldsymbol{s}, Q^{\\pi})}_{\\text{with probability } 1 - \\epsilon}$\n",
    "and $~~ \\underbrace{\\text{a random action}}_{\\text{with probability } \\epsilon}$    i.e. $\\epsilon \\in (0,1]$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b2364ec0-7510-4105-96e0-6a921b20cf1d",
   "metadata": {},
   "source": [
    "### Task 1: Implement the Greedy and Epsilon Greedy policies"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "04e97141",
   "metadata": {},
   "outputs": [],
   "source": [
    "def greedy_policy(state: int, q_array: np.ndarray) -> int:\n",
    "    \"\"\"\n",
    "    Determine the action that maximizes the Q-value for a given state.\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    state : int\n",
    "        The current state.\n",
    "    q_array : np.ndarray\n",
    "        The Q-table.\n",
    "\n",
    "    Returns\n",
    "    -------\n",
    "    int\n",
    "        The action that maximizes the Q-value for the given state.\n",
    "    \"\"\"\n",
    "    action = np.argmax(q_array[state])\n",
    "\n",
    "    return action\n",
    "\n",
    "\n",
    "def epsilon_greedy_policy(state: int, q_array: np.ndarray, epsilon: float) -> int:\n",
    "    \"\"\"\n",
    "    Determine the action to take based on an epsilon-greedy policy.\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    state : int\n",
    "        The current state.\n",
    "    q_array : np.ndarray\n",
    "        The Q-table.\n",
    "    epsilon : float\n",
    "        The probability of choosing a random action.\n",
    "\n",
    "    Returns\n",
    "    -------\n",
    "    int\n",
    "        The action to take.\n",
    "    \"\"\"\n",
    "    assert epsilon > 0 and epsilon <= 1, \"'epsilon' parameter must be in the range (0, 1]\"\n",
    "\n",
    "    if np.random.rand() <= epsilon:\n",
    "        action = np.random.randint(0, 4)\n",
    "    else:\n",
    "        action = np.argmax(q_array[state])\n",
    "\n",
    "    return action"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0c71a5a2",
   "metadata": {},
   "source": [
    "### The SARSA algorithm\n",
    "\n",
    "To find the optimal policy (or a nearly optimal policy) for the FrozenLake-v1 problem, we will first use the SARSA algorithm.\n",
    "It is based on the online update of the *QTable* for the current policy, defined as:\n",
    "$$\n",
    "Q(s, a) = \\mathbb{E}^{\\pi} \\left[ \\sum_{t=0}^{H} \\gamma^t r_{t}(s_t, a_t) | s_t=s_0, a=a_0 \\right] ,\n",
    "$$\n",
    "where $\\gamma \\in (0, 1]$ is the discount factor, and $H$ the horizon of the episode.\n",
    "\n",
    "The SARSA algorithm updates a tabular estimate of the Q-function using the following update rule:\n",
    "$$\n",
    "Q(s, a) \\leftarrow Q(s, a) +\n",
    "\\alpha\n",
    "\\left[\n",
    "\\underbrace{\\overbrace{r + \\gamma Q(s', a')}^{\\text{Target}} - Q(s, a)}_{\\text{TD error}}\n",
    "\\right] ,\n",
    "$$\n",
    "where $\\alpha \\in (0, 1]$ is the learning rate, and $r_t$ is the reward received by the agent at time step $t$.\n",
    "Most of the time, the SARSA algorithm is implemented with an $\\epsilon$-greedy exploration strategy.\n",
    "This strategy consists in selecting the best action learned so far with probability $(1 - \\epsilon)$ and to select a random\n",
    "action with probability $\\epsilon$.\n",
    "\n",
    "---\n",
    "\n",
    "***The SARSA algorithm***\n",
    "\n",
    "<b>Input</b>:<br>\n",
    "\t$\\quad\\quad$ none<br>\n",
    "<b>Algorithm parameter</b>:<br>\n",
    "\t$\\quad\\quad$ discount factor $\\gamma$<br>\n",
    "\t$\\quad\\quad$ step size $\\alpha \\in (0,1]$<br>\n",
    "\t$\\quad\\quad$ small $\\epsilon > 0$<br><br>\n",
    "\n",
    "Initialize arbitrarily $Q(\\boldsymbol{s}, \\boldsymbol{a}) ~~~ \\forall \\boldsymbol{s} \\in \\mathcal{S}, \\boldsymbol{a} \\in \\mathcal{A}(\\boldsymbol{s})$,<br>\n",
    "except that $Q(\\boldsymbol{s}_F, \\cdot) = 0 ~~~ \\forall \\boldsymbol{s}_F \\in \\mathcal{S}^F$ (initialize finale states)<br><br>\n",
    "\n",
    "<b>FOR EACH</b> episode<br>\n",
    "\t$\\quad$ $\\boldsymbol{s} \\leftarrow \\text{env.reset}() \\quad\\quad\\quad\\quad\\quad\\quad$ (initialize $s$)<br>\n",
    "\t$\\quad$ $\\boldsymbol{a} \\leftarrow \\epsilon\\text{-greedy}(\\boldsymbol{s}, Q)$<br>\n",
    "\t$\\quad$ <b>DO</b> <br>\n",
    "\t\t$\\quad\\quad$ $r, \\boldsymbol{s}' \\leftarrow \\text{env.step}(\\boldsymbol{a}) \\quad\\quad\\quad\\quad$ (take action $a$, observe $r$ and $s'$)<br>\n",
    "\t\t$\\quad\\quad$ $\\boldsymbol{a}' \\leftarrow \\epsilon\\text{-greedy}(\\boldsymbol{s}', Q)$<br>\n",
    "\t\t$\\quad\\quad$ $Q(\\boldsymbol{s},\\boldsymbol{a}) \\leftarrow Q(\\boldsymbol{s},\\boldsymbol{a}) + \\alpha \\left[ \\underbrace{\\overbrace{r + \\gamma Q(s', a')}^{\\text{Target}} ~ - ~ Q(\\boldsymbol{s},\\boldsymbol{a})}_{\\text{TD error}} \\right]$<br>\n",
    "\t\t$\\quad\\quad$ $\\boldsymbol{s} \\leftarrow \\boldsymbol{s}'$<br>\n",
    "\t\t$\\quad\\quad$ $\\boldsymbol{a} \\leftarrow \\boldsymbol{a}'$<br>\n",
    "\t$\\quad$ <b>UNTIL</b> $\\boldsymbol{s}$ is final"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "30e40900-fa16-471b-b35d-e5cf967851c1",
   "metadata": {},
   "source": [
    "### Task 2: Implement the SARSA algorithm\n",
    "\n",
    "**Tasks**: Implement the SARSA algorithm with $\\epsilon$-greedy exploration (start with $\\epsilon = 0.5$)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5528fc01",
   "metadata": {},
   "outputs": [],
   "source": [
    "DISPLAY_EVERY_N_EPISODES = 100\n",
    "\n",
    "def sarsa(\n",
    "    environment: gym.Env,\n",
    "    alpha: float = 0.1,\n",
    "    alpha_factor: float = 0.9995,\n",
    "    gamma: float = 0.99,\n",
    "    epsilon: float = 0.5,\n",
    "    num_episodes: int = 10000,\n",
    "    display: bool = False,\n",
    ") -> Tuple[np.ndarray, List[np.ndarray], List[float]]:\n",
    "    \"\"\"\n",
    "    Perform SARSA learning on a given environment.\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    environment : gym.Env\n",
    "        The environment to learn in.\n",
    "    alpha : float, optional\n",
    "        The learning rate, between 0 and 1. By default 0.1\n",
    "    alpha_factor : float, optional\n",
    "        The factor to decrease alpha by each episode, by default 0.9995\n",
    "    gamma : float, optional\n",
    "        The discount factor, between 0 and 1. By default 0.99\n",
    "    epsilon : float, optional\n",
    "        The probability of choosing a random action, by default 0.5\n",
    "    num_episodes : int, optional\n",
    "        The number of episodes to run, by default 10000\n",
    "    display : bool, optional\n",
    "        Whether to display the Q-table (every DISPLAY_EVERY_N_EPISODES episodes), by default False\n",
    "\n",
    "    Returns\n",
    "    -------\n",
    "    np.ndarray\n",
    "        The learned Q-table.\n",
    "        Each row corresponds to a state, and each column corresponds to an action.\n",
    "        In the frozen lake environment, there are 16 states and 4 actions thus the Q-table has a shape of (16, 4).\n",
    "        For instance, q_array[0, 3] is the Q-value (estimation of the expected reward) for performing action 3 (\"move up\") in state 0 (the top left square).\n",
    "    \"\"\"\n",
    "    # Initialize the history of the Q-table and learning rate\n",
    "    q_array_history = []\n",
    "    alpha_history = []\n",
    "\n",
    "    observation_space = cast(gym.spaces.Discrete, environment.observation_space)\n",
    "    action_space = cast(gym.spaces.Discrete, environment.action_space)\n",
    "\n",
    "    # Get the number of states in the environment\n",
    "    num_states = observation_space.n\n",
    "\n",
    "    # Get the number of actions in the environment\n",
    "    num_actions = action_space.n\n",
    "\n",
    "    # Initialize the Q-table to zeros\n",
    "    q_array = np.zeros([num_states, num_actions])\n",
    "\n",
    "    # Loop over the episodes\n",
    "    for episode_index in tqdm(range(1, num_episodes)):\n",
    "        # Display the Q-table every DISPLAY_EVERY_N_EPISODES episodes if display is True\n",
    "        if display and episode_index % DISPLAY_EVERY_N_EPISODES == 0:\n",
    "            display_qtable(q_array, title=\"Q table\")\n",
    "\n",
    "        # Save the current Q-table and learning rate\n",
    "        q_array_history.append(q_array.copy())\n",
    "        alpha_history.append(alpha)\n",
    "\n",
    "        # Decrease the learning rate if alpha_factor is not None\n",
    "        if alpha_factor is not None:\n",
    "            alpha = alpha * alpha_factor\n",
    "\n",
    "        state, _ = environment.reset()\n",
    "        action = epsilon_greedy_policy(state, q_array, epsilon)\n",
    "\n",
    "        while True:\n",
    "            next_state, reward, is_final, _, _ = environment.step(action)\n",
    "            next_action = epsilon_greedy_policy(next_state, q_array, epsilon)\n",
    "            q_array[state, action] = q_array[state, action] + alpha * (reward + gamma * q_array[next_state, next_action] - q_array[state, action])\n",
    "            state, action = next_state, next_action\n",
    "\n",
    "            if is_final:\n",
    "                break\n",
    "\n",
    "    # Return the learned Q-table\n",
    "    return q_array, q_array_history, alpha_history"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eceecb2c",
   "metadata": {},
   "source": [
    "**Note**: In the following cell, the `display` argument can be set to `True` to see the evolution of the action-value function `q_array` over iterations."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7b910413",
   "metadata": {
    "lines_to_next_cell": 2
   },
   "outputs": [],
   "source": [
    "# Create the environment and set the maximum number of steps per episode\n",
    "environment = gym.make(\"FrozenLake-v1\", max_episode_steps=1000)\n",
    "\n",
    "# Apply SARSA to calculate the Q-table for the FrozenLake environment\n",
    "q_array_ex2, q_array_history_ex2, alpha_history_ex2 = sarsa(environment, display=True)\n",
    "\n",
    "environment.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "33158453",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Display the learned Q-table\n",
    "display_qtable(q_array_ex2, title=\"Q Table\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2e6108c1-b521-4df4-bad4-dfa78b6ff1ad",
   "metadata": {},
   "source": [
    "### Display the greedy policy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fcc94810-82fe-4d14-be4b-044d6672a7c9",
   "metadata": {},
   "outputs": [],
   "source": [
    "policy_ex2 = [greedy_policy(state, q_array_ex2) for state in range(environment.observation_space.n)]\n",
    "policy_ex2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "18b9ef21-46a9-48a5-aa35-f331de5bab69",
   "metadata": {},
   "outputs": [],
   "source": [
    "display_policy(policy_ex2, environment)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "529a6db2",
   "metadata": {},
   "source": [
    "### Display the evolution of the value function over iterations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "587239de",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Make a Pandas dataframe containing the evolution of the Q-table during the learning process\n",
    "q_array_history_array_ex2 = np.array(q_array_history_ex2)\n",
    "df_q_hist_list_ex2 = []\n",
    "\n",
    "for action_index in range(q_array_history_array_ex2.shape[2]):\n",
    "    df_q_hist_list_ex2.append(pd.DataFrame(q_array_history_array_ex2[:, :, action_index]))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ec46f0df",
   "metadata": {},
   "source": [
    "Evolution of `q_array_ex2` over iterations (one curve per state):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5da39d0a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Show the evolution of the estimated expected value for each action and state during the learning process\n",
    "for action_index, df_q_hist in enumerate(df_q_hist_list_ex2):\n",
    "    df_q_hist.plot(figsize=(14,8))\n",
    "    plt.title(r'$Q(\\cdot,a_{})$ w.r.t iteration with $a_{}$ := \"{}\"'.format(action_index, action_index, action_labels[action_index]))\n",
    "    plt.ylabel(r\"$Q(\\cdot,a_{})$\".format(action_index))\n",
    "    plt.xlabel(\"iteration\")\n",
    "    plt.legend(loc=\"upper right\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ae72d8be",
   "metadata": {
    "lines_to_next_cell": 0
   },
   "outputs": [],
   "source": [
    "# Show the evolution of the alpha parameter during the learning process\n",
    "plt.loglog(alpha_history_ex2)\n",
    "plt.title(\"Alpha w.r.t iteration\")\n",
    "plt.ylabel(\"Alpha\")\n",
    "plt.xlabel(\"iteration\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b0c74025",
   "metadata": {},
   "source": [
    "### Evaluate Policy with Gymnasium"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ad4403a6",
   "metadata": {},
   "source": [
    "As a measure of performance, count the number of successful trials on 1000 episodes.\n",
    "\n",
    "**Note**: Gymnasium considers the task is solved if you reach 76\\% of success over the episodes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "be14ab8d",
   "metadata": {},
   "outputs": [],
   "source": [
    "environment = gym.make(\"FrozenLake-v1\", max_episode_steps=1000)\n",
    "\n",
    "reward_list = []\n",
    "NUM_EPISODES = 1000\n",
    "\n",
    "for episode_index in range(NUM_EPISODES):\n",
    "    state, info = environment.reset()\n",
    "    episode_over = False\n",
    "\n",
    "    while not episode_over:\n",
    "        # action = epsilon_greedy_policy(state, q_array_ex2, epsilon)\n",
    "        action = greedy_policy(state, q_array_ex2)\n",
    "        state, reward, terminated, truncated, info = environment.step(action)\n",
    "        episode_over = terminated or truncated\n",
    "\n",
    "    reward_list.append(reward)\n",
    "\n",
    "reward_df = pd.DataFrame(reward_list)\n",
    "\n",
    "print('Average reward (which is equivalent to a \"success rate\" in the FrozenLake environment as the total rewards in this environment are either 0 or 1):', np.average(reward_df))\n",
    "\n",
    "environment.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7134c7d9-ec9e-41ad-a94f-4612f3c8528f",
   "metadata": {},
   "source": [
    "### Test the optimized policy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cdb0a5f4-adc2-4ac0-8c78-dd2870b4c520",
   "metadata": {},
   "outputs": [],
   "source": [
    "NUM_EPISODES = 3\n",
    "\n",
    "FIGS_DIR = Path(\"figs/\")       # Where to save figures (.gif files)\n",
    "\n",
    "VIDEO_DIRNAME_EX2 = \"lab5_ex2_sarsa\"\n",
    "\n",
    "(FIGS_DIR / VIDEO_DIRNAME_EX2 / \"rl-video-episode-0.mp4\").unlink(missing_ok=True)\n",
    "(FIGS_DIR / VIDEO_DIRNAME_EX2 / \"rl-video-episode-1.mp4\").unlink(missing_ok=True)\n",
    "(FIGS_DIR / VIDEO_DIRNAME_EX2 / \"rl-video-episode-2.mp4\").unlink(missing_ok=True)\n",
    "\n",
    "environment = gym.make(\"FrozenLake-v1\", max_episode_steps=1000, render_mode=\"rgb_array\")\n",
    "environment = gym.wrappers.RecordVideo(environment, video_folder=FIGS_DIR / VIDEO_DIRNAME_EX2, episode_trigger=lambda x: True)\n",
    "environment = gym.wrappers.RecordEpisodeStatistics(environment, buffer_length=NUM_EPISODES)\n",
    "\n",
    "for episode_index in range(NUM_EPISODES):\n",
    "    state, info = environment.reset()\n",
    "\n",
    "    episode_over = False\n",
    "    while not episode_over:\n",
    "        # action = epsilon_greedy_policy(state, q_array_ex2, epsilon)\n",
    "        action = greedy_policy(state, q_array_ex2)\n",
    "        state, reward, terminated, truncated, info = environment.step(action)\n",
    "        episode_over = terminated or truncated\n",
    "\n",
    "    reward_list.append(reward)\n",
    "\n",
    "print(f'Episode time taken: {environment.time_queue}')\n",
    "print(f'Episode total rewards: {environment.return_queue}')\n",
    "print(f'Episode lengths: {environment.length_queue}')\n",
    "\n",
    "environment.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "800aa03d-a8b7-4a84-b643-fbabdbdd4c5d",
   "metadata": {},
   "outputs": [],
   "source": [
    "Video(FIGS_DIR / VIDEO_DIRNAME_EX2 / \"rl-video-episode-0.mp4\", embed=True, html_attributes=\"controls autoplay loop\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "01d2ada7-3417-4639-a442-8af0d922b39e",
   "metadata": {},
   "outputs": [],
   "source": [
    "Video(FIGS_DIR / VIDEO_DIRNAME_EX2 / \"rl-video-episode-1.mp4\", embed=True, html_attributes=\"controls autoplay loop\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "13d24fbf-32e5-4018-a7ee-8e0db29b7a3e",
   "metadata": {},
   "outputs": [],
   "source": [
    "Video(FIGS_DIR / VIDEO_DIRNAME_EX2 / \"rl-video-episode-2.mp4\", embed=True, html_attributes=\"controls autoplay loop\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "13d0c6f1-aa1e-44e8-9413-ae5c4328eacc",
   "metadata": {},
   "source": [
    "### Question 2"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4df4103e-f1c0-426b-be46-53d90b810b2a",
   "metadata": {},
   "source": [
    "Copy and paste the output of the following cell into the second question of the [Lab 5 - Evaluation](https://moodle.polytechnique.fr/course/section.php?id=66533) in Moodle:  \n",
    "*\"What is the Q-value Q(s0,a0) of the initial state and action 0 (\"Move Left\") in the FrozenLake environment using the default parameters?\"*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3d99d718-a187-4d45-a986-c7cdb1407fc9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# %%time\n",
    "\n",
    "NUM_RUNS = 10\n",
    "\n",
    "# Create the environment and set the maximum number of steps per episode\n",
    "environment = gym.make(\"FrozenLake-v1\", max_episode_steps=1000)\n",
    "\n",
    "run_list = []\n",
    "for _ in tqdm(range(1, NUM_RUNS)):\n",
    "    q_array, _, _ = sarsa(environment, display=False)\n",
    "    run_list.append(q_array[0][0].item())\n",
    "\n",
    "environment.close()\n",
    "\n",
    "print(f\"\\n\\nâš ï¸ Copy and paste this value into Moodle ðŸ‘‰ {np.mean(run_list).item():.2f}\\n(it has the required number of decimals)\\n\\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b9cab2ba",
   "metadata": {},
   "source": [
    "## Exercise 3: QLearning"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0e911d29",
   "metadata": {},
   "source": [
    "Another reinforcement learning algorithm is *Q-Learning*.\n",
    "The fundamental difference from *SARSA* is that it is an *off-policy* algorithm.\n",
    "This means it learns from trajectories generated by a policy different from the one it is optimizing.\n",
    "In other words, the *behavior policy* (*epsilon-greedy*) is different from the *target policy* (*greedy*).\n",
    "\n",
    "To do so, it uses the following update rule:\n",
    "$$\n",
    "Q(s, a) \\leftarrow Q(s, a) +\n",
    "\\alpha\n",
    "\\left[\n",
    "\\underbrace{\\overbrace{r + \\gamma \\max_{a^* \\in \\mathcal{A}} Q(s', a^*)}^{\\text{Target}} - Q(s, a)}_{\\text{TD error}}\n",
    "\\right] ,\n",
    "$$\n",
    "\n",
    "**Task**: in this exercise, you will replace the SARSA update rule by the Q-learning one and analyze the\n",
    "differences in performances.\n",
    "\n",
    "---\n",
    "\n",
    "### The QLearning algorithm\n",
    "\n",
    "<b>Input</b>:<br>\n",
    "\t$\\quad\\quad$ none<br>\n",
    "<b>Algorithm parameter</b>:<br>\n",
    "\t$\\quad\\quad$ discount factor $\\gamma$<br>\n",
    "\t$\\quad\\quad$ step size $\\alpha \\in (0,1]$<br>\n",
    "\t$\\quad\\quad$ small $\\epsilon > 0$<br><br>\n",
    "\n",
    "Initialize arbitrarily $Q(\\boldsymbol{s}, \\boldsymbol{a}) ~~~ \\forall \\boldsymbol{s} \\in \\mathcal{S}, \\boldsymbol{a} \\in \\mathcal{A}(\\boldsymbol{s})$,<br>\n",
    "except that $Q(\\boldsymbol{s}_F, \\cdot) = 0 ~~~ \\forall \\boldsymbol{s}_F \\in \\mathcal{S}^F$ (initialize finale states)<br><br>\n",
    "\n",
    "<b>FOR EACH</b> episode<br>\n",
    "\t$\\quad$ $\\boldsymbol{s} \\leftarrow \\text{env.reset}() \\quad\\quad\\quad\\quad\\quad\\quad$ (initialize $s$)<br>\n",
    "\t$\\quad$ <b>DO</b> <br>\n",
    "\t\t$\\quad\\quad$ $\\boldsymbol{a} \\leftarrow \\epsilon\\text{-greedy}(\\boldsymbol{s}, Q)$<br>\n",
    "\t\t$\\quad\\quad$ $r, \\boldsymbol{s}' \\leftarrow \\text{env.step}(\\boldsymbol{a}) \\quad\\quad\\quad\\quad$ (take action $a$, observe $r$ and $s'$)<br>\n",
    "\t\t$\\quad\\quad$ $Q(\\boldsymbol{s},\\boldsymbol{a}) \\leftarrow Q(\\boldsymbol{s},\\boldsymbol{a}) + \\alpha \\left[ \\underbrace{\\overbrace{r + \\gamma ~ \\max_{\\boldsymbol{a^*} \\in \\mathcal{A}} Q(\\boldsymbol{s}', \\boldsymbol{a^*})}^{\\text{Target}} ~ - ~ Q(\\boldsymbol{s},\\boldsymbol{a})}_{\\text{TD error}} \\right]$<br>\n",
    "\t\t$\\quad\\quad$ $\\boldsymbol{s} \\leftarrow \\boldsymbol{s}'$<br>\n",
    "\t$\\quad$ <b>UNTIL</b> $\\boldsymbol{s}$ is final"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "13400eb8-378b-4415-a306-53775bb65f62",
   "metadata": {},
   "source": [
    "### Task 1: Implement the QLearning algorithm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0d494d10",
   "metadata": {},
   "outputs": [],
   "source": [
    "DISPLAY_EVERY_N_EPISODES = 200\n",
    "\n",
    "def q_learning(\n",
    "    environment: gym.Env,\n",
    "    alpha: float = 0.1,\n",
    "    alpha_factor: float = 0.9995,\n",
    "    gamma: float = 0.99,\n",
    "    epsilon: float = 0.5,\n",
    "    num_episodes: int = 10000,\n",
    "    display: bool = False,\n",
    ") -> Tuple[np.ndarray, List[np.ndarray], List[float]]:\n",
    "    \"\"\"\n",
    "    Perform Q-learning on a given environment.\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    environment : gym.Env\n",
    "        The environment to learn in.\n",
    "    alpha : float, optional\n",
    "        The learning rate, between 0 and 1. By default 0.1\n",
    "    alpha_factor : float, optional\n",
    "        The factor to decrease alpha by each episode, by default 0.9995\n",
    "    gamma : float, optional\n",
    "        The discount factor, between 0 and 1. By default 0.99\n",
    "    epsilon : float, optional\n",
    "        The probability of choosing a random action, by default 0.5\n",
    "    num_episodes : int, optional\n",
    "        The number of episodes to run, by default 10000\n",
    "    display : bool, optional\n",
    "        Whether to display the Q-table (every DISPLAY_EVERY_N_EPISODES episodes), by default False\n",
    "\n",
    "    Returns\n",
    "    -------\n",
    "    np.ndarray\n",
    "        The learned Q-table.\n",
    "        Each row corresponds to a state, and each column corresponds to an action.\n",
    "        In the frozen lake environment, there are 16 states and 4 actions thus the Q-table has a shape of (16, 4).\n",
    "        For instance, q_array[0, 3] is the Q-value (estimation of the expected reward) for performing action 3 (\"move up\") in state 0 (the top left square).\n",
    "    \"\"\"\n",
    "    # Initialize the history of the Q-table and learning rate\n",
    "    q_array_history = []\n",
    "    alpha_history = []\n",
    "\n",
    "    observation_space = cast(gym.spaces.Discrete, environment.observation_space)\n",
    "    action_space = cast(gym.spaces.Discrete, environment.action_space)\n",
    "\n",
    "    # Get the number of states in the environment\n",
    "    num_states = observation_space.n\n",
    "\n",
    "    # Get the number of actions in the environment\n",
    "    num_actions = action_space.n\n",
    "\n",
    "    # Initialize the Q-table to zeros\n",
    "    q_array = np.zeros([num_states, num_actions])\n",
    "\n",
    "    # Loop over the episodes\n",
    "    for episode_index in tqdm(range(1, num_episodes)):\n",
    "        # Display the Q-table every DISPLAY_EVERY_N_EPISODES episodes if display is True\n",
    "        if display and episode_index % DISPLAY_EVERY_N_EPISODES == 0:\n",
    "            display_qtable(q_array, title=\"Q table\")\n",
    "\n",
    "        # Save the current Q-table and learning rate\n",
    "        q_array_history.append(q_array.copy())\n",
    "        alpha_history.append(alpha)\n",
    "\n",
    "        # Decrease the learning rate if alpha_factor is not None\n",
    "        if alpha_factor is not None:\n",
    "            alpha = alpha * alpha_factor\n",
    "\n",
    "        state, _ = environment.reset()\n",
    "\n",
    "        while True:\n",
    "            action = epsilon_greedy_policy(state, q_array, epsilon)\n",
    "            next_state, reward, is_final, _, _ = environment.step(action)\n",
    "            q_array[state, action] = q_array[state, action] + alpha * (reward + gamma * np.max(q_array[next_state]) - q_array[state, action])\n",
    "            state = next_state\n",
    "            \n",
    "            if is_final:\n",
    "                break\n",
    "\n",
    "    # Return the learned Q-table\n",
    "    return q_array, q_array_history, alpha_history"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "47785fde",
   "metadata": {},
   "source": [
    "**Note**: In the following cell, the `display` argument can be set to `True` to see the evolution of the action-value function `q_array` over iterations."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4f865981",
   "metadata": {
    "lines_to_next_cell": 2
   },
   "outputs": [],
   "source": [
    "# Create the environment and set the maximum number of steps per episode\n",
    "environment = gym.make(\"FrozenLake-v1\", max_episode_steps=1000)\n",
    "\n",
    "# Apply Q-learning to calculate the Q-table for the FrozenLake environment\n",
    "q_array_ex3, q_array_history_ex3, alpha_history_ex3 = q_learning(environment, display=True)\n",
    "\n",
    "environment.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2c82b69a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Display the learned Q-table\n",
    "display_qtable(q_array_ex3, title=\"Q Table\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "959eb0ee",
   "metadata": {},
   "source": [
    "### Display the greedy policy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9a2ef2e7-5173-4c94-95b9-84049228c5e9",
   "metadata": {},
   "outputs": [],
   "source": [
    "policy_ex3 = [greedy_policy(state, q_array_ex3) for state in range(environment.observation_space.n)]\n",
    "policy_ex3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "deaa3ffe-352b-4594-baa8-3113b6c6ecd7",
   "metadata": {},
   "outputs": [],
   "source": [
    "display_policy(policy_ex3, environment)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "86b48d5b-cca9-4a46-9ebb-f3430e66d786",
   "metadata": {},
   "source": [
    "### Display the evolution of the value function over iterations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bcca3096",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Make a Pandas dataframe containing the evolution of the Q-table during the learning process\n",
    "q_array_history_array_ex3 = np.array(q_array_history_ex3)\n",
    "df_q_hist_list_ex3 = []\n",
    "\n",
    "for action_index in range(q_array_history_array_ex3.shape[2]):\n",
    "    df_q_hist_list_ex3.append(pd.DataFrame(q_array_history_array_ex3[:, :, action_index]))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "928f2a34",
   "metadata": {},
   "source": [
    "Evolution of `q_array_ex3` over iterations (one curve per state):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "07469c5d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Show the evolution of the estimated expected value for each action and state during the learning process\n",
    "for action_index, df_q_hist in enumerate(df_q_hist_list_ex3):\n",
    "    df_q_hist.plot(figsize=(14,8))\n",
    "    plt.title(r'$Q(\\cdot,a_{})$ w.r.t iteration with $a_{}$ := \"{}\"'.format(action_index, action_index, action_labels[action_index]))\n",
    "    plt.ylabel(r\"$Q(\\cdot,a_{})$\".format(action_index))\n",
    "    plt.xlabel(\"iteration\")\n",
    "    plt.legend(loc=\"upper right\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "df2fa97a",
   "metadata": {
    "lines_to_next_cell": 0
   },
   "outputs": [],
   "source": [
    "# Show the evolution of the alpha parameter during the learning process\n",
    "plt.loglog(alpha_history_ex3)\n",
    "plt.title(\"Alpha w.r.t iteration\")\n",
    "plt.ylabel(\"Alpha\")\n",
    "plt.xlabel(\"iteration\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2ca26609",
   "metadata": {},
   "source": [
    "### Evaluate Policy with Gymnasium"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "81fba3e6",
   "metadata": {},
   "source": [
    "As a measure of performance, count the number of successful trials on 1000 episodes.\n",
    "\n",
    "**Note**: Gymnasium considers the task is solved if you reach 76\\% of success over the episodes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f74a1c14",
   "metadata": {},
   "outputs": [],
   "source": [
    "environment = gym.make(\"FrozenLake-v1\", max_episode_steps=1000)\n",
    "\n",
    "reward_list = []\n",
    "NUM_EPISODES = 1000\n",
    "\n",
    "for episode_index in range(NUM_EPISODES):\n",
    "    state, info = environment.reset()\n",
    "    episode_over = False\n",
    "\n",
    "    while not episode_over:\n",
    "        # action = epsilon_greedy_policy(state, q_array_ex3, epsilon)\n",
    "        action = greedy_policy(state, q_array_ex3)\n",
    "        state, reward, terminated, truncated, info = environment.step(action)\n",
    "        episode_over = terminated or truncated\n",
    "\n",
    "    reward_list.append(reward)\n",
    "\n",
    "reward_df = pd.DataFrame(reward_list)\n",
    "\n",
    "print(\n",
    "    'Average reward (which is equivalent to a \"success rate\" in the FrozenLake environment as the total rewards in this environment are either 0 or 1):',\n",
    "    np.average(reward_df),\n",
    ")\n",
    "\n",
    "environment.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b88f9aaf-7487-4419-9dc1-3c977ee932b4",
   "metadata": {},
   "source": [
    "### Test the optimized policy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "97d14712-b7e0-4b71-9080-4c97ac2f0c63",
   "metadata": {},
   "outputs": [],
   "source": [
    "NUM_EPISODES = 3\n",
    "\n",
    "FIGS_DIR = Path(\"figs/\")       # Where to save figures (.gif files)\n",
    "\n",
    "VIDEO_DIRNAME_EX3 = \"lab5_ex3_qlearning\"\n",
    "\n",
    "(FIGS_DIR / VIDEO_DIRNAME_EX3 / \"rl-video-episode-0.mp4\").unlink(missing_ok=True)\n",
    "(FIGS_DIR / VIDEO_DIRNAME_EX3 / \"rl-video-episode-1.mp4\").unlink(missing_ok=True)\n",
    "(FIGS_DIR / VIDEO_DIRNAME_EX3 / \"rl-video-episode-2.mp4\").unlink(missing_ok=True)\n",
    "\n",
    "environment = gym.make(\"FrozenLake-v1\", max_episode_steps=1000, render_mode=\"rgb_array\")\n",
    "environment = gym.wrappers.RecordVideo(environment, video_folder=FIGS_DIR / VIDEO_DIRNAME_EX3, episode_trigger=lambda x: True)\n",
    "environment = gym.wrappers.RecordEpisodeStatistics(environment, buffer_length=NUM_EPISODES)\n",
    "\n",
    "for episode_index in range(NUM_EPISODES):\n",
    "    state, info = environment.reset()\n",
    "\n",
    "    episode_over = False\n",
    "    while not episode_over:\n",
    "        # action = epsilon_greedy_policy(state, q_array_ex3, epsilon)\n",
    "        action = greedy_policy(state, q_array_ex3)\n",
    "        state, reward, terminated, truncated, info = environment.step(action)\n",
    "        episode_over = terminated or truncated\n",
    "\n",
    "    reward_list.append(reward)\n",
    "\n",
    "print(f'Episode time taken: {environment.time_queue}')\n",
    "print(f'Episode total rewards: {environment.return_queue}')\n",
    "print(f'Episode lengths: {environment.length_queue}')\n",
    "\n",
    "environment.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "15726a10-7c4a-4384-9460-3ffc3ca97b9c",
   "metadata": {},
   "outputs": [],
   "source": [
    "Video(FIGS_DIR / VIDEO_DIRNAME_EX3 / \"rl-video-episode-0.mp4\", embed=True, html_attributes=\"controls autoplay loop\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9e883496-fd77-4847-821e-2b88fde6b69a",
   "metadata": {},
   "outputs": [],
   "source": [
    "Video(FIGS_DIR / VIDEO_DIRNAME_EX3 / \"rl-video-episode-1.mp4\", embed=True, html_attributes=\"controls autoplay loop\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cb88b6d4-0494-4265-9895-a98723a9d09d",
   "metadata": {},
   "outputs": [],
   "source": [
    "Video(FIGS_DIR / VIDEO_DIRNAME_EX3 / \"rl-video-episode-2.mp4\", embed=True, html_attributes=\"controls autoplay loop\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cff60184-eff8-4f3d-83a4-2802bc4f8fd8",
   "metadata": {},
   "source": [
    "### Question 3"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ac9c4333-9637-4eaa-bd2e-24ed389e9343",
   "metadata": {},
   "source": [
    "Copy and paste the output of the following cell into the third question of the [Lab 5 - Evaluation](https://moodle.polytechnique.fr/course/section.php?id=66533) in Moodle:  \n",
    "*\"What is the Q-value Q(s0,a0) of the initial state and action 0 (\"Move Left\") in the FrozenLake environment using the default parameters?\"*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8e4c5de1-7900-4c06-8c1e-2ee7998b3e16",
   "metadata": {},
   "outputs": [],
   "source": [
    "# %%time\n",
    "\n",
    "NUM_RUNS = 10\n",
    "\n",
    "# Create the environment and set the maximum number of steps per episode\n",
    "environment = gym.make(\"FrozenLake-v1\", max_episode_steps=1000)\n",
    "\n",
    "run_list = []\n",
    "for _ in tqdm(range(1, NUM_RUNS)):\n",
    "    q_array, _, _ = q_learning(environment, display=False)\n",
    "    run_list.append(q_array[0][0].item())\n",
    "\n",
    "environment.close()\n",
    "\n",
    "print(f\"\\n\\nâš ï¸ Copy and paste this value into Moodle ðŸ‘‰ {np.mean(run_list).item():.2f}\\n(it has the required number of decimals)\\n\\n\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
