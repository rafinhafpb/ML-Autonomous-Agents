{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# CSC-52081-EP Lab6: Deep Reinforcement Learning\n",
    "\n",
    "<img src=\"https://raw.githubusercontent.com/jeremiedecock/polytechnique-csc-52081-ep-2025-students/refs/heads/main/assets/logo.jpg\" style=\"float: left; width: 15%\" />\n",
    "\n",
    "[CSC-52081-EP-2025](https://moodle.polytechnique.fr/course/view.php?id=19336) Lab session #6\n",
    "\n",
    "2019-2025 Jérémie Decock"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "[![Open in Google Colab](https://colab.research.google.com/assets/colab-badge.svg)](https://colab.research.google.com/github/jeremiedecock/polytechnique-csc-52081-ep-2025-students/blob/main/lab6_rl3_deep_reinforcement_learning.ipynb)\n",
    "\n",
    "[![My Binder](https://mybinder.org/badge_logo.svg)](https://mybinder.org/v2/gh/jeremiedecock/polytechnique-csc-52081-ep-2025-students/main?filepath=lab6_rl3_deep_reinforcement_learning.ipynb)\n",
    "\n",
    "[![NbViewer](https://raw.githubusercontent.com/jupyter/design/main/logos/Badges/nbviewer_badge.svg)](https://nbviewer.jupyter.org/github/jeremiedecock/polytechnique-csc-52081-ep-2025-students/blob/main/lab6_rl3_deep_reinforcement_learning.ipynb)\n",
    "\n",
    "[![Local](https://img.shields.io/badge/Local-Save%20As...-blue)](https://github.com/jeremiedecock/polytechnique-csc-52081-ep-2025-students/raw/main/lab6_rl3_deep_reinforcement_learning.ipynb)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Introduction\n",
    "\n",
    "In the previous lab, we dealt with reinforcement learning in discrete state and action spaces.\n",
    "To do so, we used methods based on action-value function and especially $Q$-function estimation.\n",
    "The $Q$-function was stored in a table and updated with on- or off- policy algorithms (namely SARSA and $Q$-Learning).\n",
    "\n",
    "Yet, these methods do not scale to large state spaces and especially not to the case of continuous state spaces.\n",
    "To address these issues one can either extend the value-based methods making use of the value-function approximation or directly search in policy spaces.\n",
    "In this lab, we will explore both solutions.\n",
    "\n",
    "The first part of this lab presents the problem to solve: the CartPole environment.\n",
    "\n",
    "In the second part, we will apply value-function approximation methods (namely DQN) to solve the CartPole problem.\n",
    "\n",
    "In the third part of this lab (\"Bonus 1\"), we will search in a family of parameterized policies $\\pi_\\theta(s, a)$ using a policy gradient method (this part is **optional** and **will not be graded**).\n",
    "\n",
    "The fourth part introduce a useful tool to optimize hyper-parameters (this is not an exercise but a useful information to go further).\n",
    "\n",
    "As for previous labs, you can either:\n",
    "- open, edit and execute the notebook in *Google Colab* following this link: https://colab.research.google.com/github/jeremiedecock/polytechnique-csc-52081-ep-2025-students/blob/main/lab6_rl3_deep_reinforcement_learning.ipynb ; this is the **recommended** choice as you have nothing to install on your computer\n",
    "- open, edit and execute the notebook in *MyBinder* (if for any reason the Google Colab solution doesn't work): https://mybinder.org/v2/gh/jeremiedecock/polytechnique-csc-52081-ep-2025-students/main?filepath=lab6_rl3_deep_reinforcement_learning.ipynb\n",
    "- download, edit and execute the notebook on your computer if Python3 and JypyterLab are already installed: https://github.com/jeremiedecock/polytechnique-csc-52081-ep-2025-students/raw/main/lab6_rl3_deep_reinforcement_learning.ipynb\n",
    "\n",
    "If you work with Google Colab or MyBinder, **remember to save or download your work regularly or you may lose it!**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Lab Submission\n",
    "\n",
    "Please submit your completed notebook in [Moodle : \"Lab 6 - Submission\"](https://moodle.polytechnique.fr/course/section.php?id=66534).\n",
    "\n",
    "### Submission Guidelines\n",
    "\n",
    "1. **File Naming:** Rename your notebook as follows: **`firstname_lastname-06.ipynb`** where `firstname` and `lastname` match your email address. *Example: `jesse_read-06.ipynb`*\n",
    "2. **Clear Output Cells:** To reduce file size (**must be under 500 KB**), clear all output cells before submitting. This includes rendered images, videos, plots, and dataframes...\n",
    "   - **JupyterLab:**\n",
    "     - Click **\"Kernel\" → \"Restart Kernel and Clear Outputs of All Cells...\"**\n",
    "     - Then go to **\"File\" → \"Save Notebook As...\"**\n",
    "   - **Google Colab:**\n",
    "     - Click **\"Edit\" → \"Clear all outputs\"**\n",
    "     - Then go to **\"File\" → \"Download\" → \"Download.ipynb\"**\n",
    "   - **VSCode:**\n",
    "     - Click **\"Clear All Outputs\"**\n",
    "     - Then **save your file**\n",
    "3. **Upload Your File:** Only **`.ipynb`** files are accepted.\n",
    "\n",
    "**Note:** Bonus parts (if any) are optional, as their name suggests.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Setup the Python environment"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This notebook relies on several libraries including `gymnasium[classic-control]` (v1.0.0), `ipywidgets`, `matplotlib`, `moviepy`, `numpy`, `pandas`, `pygame`, `seaborn`, `torch`, and `tqdm`.\n",
    "A complete list of dependencies can be found in the following [requirements-lab6.txt](https://raw.githubusercontent.com/jeremiedecock/polytechnique-csc-52081-ep-2025-students/main/requirements-lab6.txt) file."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### If you use Google Colab\n",
    "\n",
    "If you use Google Colab, execute the next cell to install required libraries."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "xt66Z85AmOI2",
    "outputId": "bd7a6b75-ad3c-4be1-d560-8239fbc0e9d2"
   },
   "outputs": [],
   "source": [
    "import sys\n",
    "import subprocess\n",
    "\n",
    "\n",
    "def is_colab():\n",
    "    return \"google.colab\" in sys.modules\n",
    "\n",
    "\n",
    "def run_subprocess_command(cmd):\n",
    "    # run the command\n",
    "    process = subprocess.Popen(cmd.split(), stdout=subprocess.PIPE)\n",
    "    # print the output\n",
    "    for line in process.stdout:\n",
    "        print(line.decode().strip())\n",
    "\n",
    "\n",
    "if is_colab():\n",
    "    # run_subprocess_command(\"apt install xvfb x11-utils\")\n",
    "    run_subprocess_command(\n",
    "        \"pip install -r https://raw.githubusercontent.com/jeremiedecock/polytechnique-csc-52081-ep-2025-students/main/requirements-lab6-google-colab.txt\"\n",
    "    )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### If you have downloaded the notebook on your computer and execute it in your own Python environment"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To set up the necessary dependencies, run the following commands to establish a [Python virtual environment (venv)](https://docs.python.org/3/library/venv.html) that includes all the essential libraries for this lab.\n",
    "\n",
    "#### On Posix systems (Linux, MacOSX, WSL, ...)\n",
    "\n",
    "```bash\n",
    "python3 -m venv env-lab6\n",
    "source env-lab6/bin/activate\n",
    "python3 -m pip install --upgrade pip\n",
    "python3 -m pip install -r https://raw.githubusercontent.com/jeremiedecock/polytechnique-csc-52081-ep-2025-students/main/requirements-lab6.txt\n",
    "```\n",
    "\n",
    "#### On Windows\n",
    "\n",
    "```bash\n",
    "python3 -m venv env-lab6\n",
    "env-lab6\\Scripts\\activate.bat\n",
    "python3 -m pip install --upgrade pip\n",
    "python3 -m pip install -r https://raw.githubusercontent.com/jeremiedecock/polytechnique-csc-52081-ep-2025-students/main/requirements-lab6.txt\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Run CSC-52081-EP notebooks locally in a dedicated Docker container\n",
    "\n",
    "If you are familiar with Docker, an image is available on Docker Hub for this lab:\n",
    "\n",
    "```bash\n",
    "docker run -it --rm --user root -p 8888:8888 -e NB_UID=$(id -u) -e NB_GID=$(id -g) -v \"${PWD}\":/home/jovyan/work jdhp/csc-52081-ep:latest\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Import required packages"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import collections\n",
    "import gymnasium as gym\n",
    "import itertools\n",
    "import numpy as np\n",
    "\n",
    "# from numpy.typing import NDArray\n",
    "import pandas as pd\n",
    "from pathlib import Path\n",
    "import random\n",
    "import torch\n",
    "from typing import Callable, cast, List, Tuple, Union"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "import seaborn as sns\n",
    "from tqdm.notebook import tqdm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from IPython.display import Video\n",
    "from ipywidgets import interact"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import warnings\n",
    "\n",
    "warnings.filterwarnings(\"ignore\", category=UserWarning)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sns.set_context(\"talk\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "FIGS_DIR = Path(\"figs/\") / \"lab6\"       # Where to save figures (.gif or .mp4 files)\n",
    "PLOTS_DIR = Path(\"figs/\") / \"lab6\"      # Where to save plots (.png or .svg files)\n",
    "MODELS_DIR = Path(\"models/\") / \"lab6\"   # Where to save models (.pth files)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if not FIGS_DIR.exists():\n",
    "    FIGS_DIR.mkdir(parents=True)\n",
    "if not PLOTS_DIR.exists():\n",
    "    PLOTS_DIR.mkdir(parents=True)\n",
    "if not MODELS_DIR.exists():\n",
    "    MODELS_DIR.mkdir(parents=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Define some parameters"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Number of trainings\n",
    "\n",
    "To achieve more representative outcomes at the conclusion of each exercise, we average the results across multiple training sessions. The `DEFAULT_NUMBER_OF_TRAININGS` variable specifies the number of training sessions conducted before the results are displayed.\n",
    "\n",
    "We recommend setting a lower value (such as 1 or 2) during the development and testing phases of your implementations. Once you have completed your work and are confident in its functionality, you can increase the number of training sessions to minimize the variance in results. Be aware that a higher number of training sessions will extend the execution time, so adjust this setting in accordance with your computer's capabilities.\n",
    "\n",
    "Additionally, you have the option to assign a specific value to the `NUMBER_OF_TRAININGS` variable for each exercise directly within the cells where the training loop is defined."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "lines_to_next_cell": 2
   },
   "outputs": [],
   "source": [
    "DEFAULT_NUMBER_OF_TRAININGS = 3"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "lines_to_next_cell": 2
   },
   "source": [
    "## Define the video selector widget"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The `video_selector` function, defined in the next cell, will be used in exercises 2, 3, 4 and 5 to display different episodes of the trained agent."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def video_selector(file_path: List[Path]):\n",
    "    return Video(file_path, embed=True, html_attributes=\"controls autoplay loop\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## PyTorch Refresher and Cheat Sheet\n",
    "\n",
    "In this lab, we will be implementing our deep reinforcement learning algorithms using PyTorch.\n",
    "If you need a refresher, you might find this [PyTorch Cheat Sheet](https://pytorch.org/tutorials/beginner/ptcheat.html) helpful. It provides a quick reference for many of the most commonly used PyTorch functions and concepts, and can be a valuable resource as you work through this lab.\n",
    "\n",
    "You can also refer to the [official documentation](https://pytorch.org/docs/stable/index.html)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## PyTorch setup"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "PyTorch can run on both CPUs and GPUs. The following cell will determine the device PyTorch will use. If a GPU is available, PyTorch will use it; otherwise, it will use the CPU."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")  # Set the device to CUDA if available, otherwise use CPU"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For utilizing a GPU on Google Colab, you also have to activate it following the steps outlined [here](https://colab.research.google.com/notebooks/gpu.ipynb)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Available GPUs:\")\n",
    "if torch.cuda.is_available():\n",
    "    for i in range(torch.cuda.device_count()):\n",
    "        print(f\"- Device {i}: {torch.cuda.get_device_name(i)}\")\n",
    "else:\n",
    "    print(\"- No GPU available.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If you have a recent GPU and want to use it, you may need to install a specific version of PyTorch compatible with your Cuda version. For this, you will have to edit the [requirements-lab6.txt](https://raw.githubusercontent.com/jeremiedecock/polytechnique-csc-52081-ep-2025-students/main/requirements-lab6.txt) file and replace the current version of PyTorch with the one compatible with your Cuda version. Check the [official PyTorch website](https://pytorch.org/get-started/locally/) for more information."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Note that the GPU is not very useful in this lab because CartPole is a simple and quick problem to solve, and CUDA spends more time transferring data between the CPU and GPU than processing it directly on the CPU.\n",
    "\n",
    "You can uncomment the next cell to explicitly instruct PyTorch to train neural networks using the CPU."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# device = \"cpu\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(f\"PyTorch will train and test neural networks on {device}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part 1: Hands on Cart Pole environment"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As for previous labs, we will use standard environments provided by the Gymnasium, but this time, we will try to solve the CartPole-v1 environment (c.f. https://gymnasium.farama.org/environments/classic_control/cart_pole/) which offers a continuous state space and discrete action space.\n",
    "The Cart Pole task consists in maintaining a pole in a vertical position by moving a cart on which the pole is attached with a joint.\n",
    "No friction is considered.\n",
    "The task is supposed to be solved if the pole stays up-right (within 15 degrees) for 500 steps in average over 100 episodes while keeping the cart position within reasonable bounds.\n",
    "The state is given by $\\{x,\\frac{\\partial x}{\\partial t},\\omega,\\frac{\\partial \\omega}{\\partial t}\\}$ where $x$ is the position of the cart and $\\omega$ is the angle between the pole and vertical position.\n",
    "There are only two possible actions: $a \\in \\{0, 1\\}$ where $a = 0$ means \"push the cart to the LEFT\" and $a = 1$ means \"push the cart to the RIGHT\"."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Exercise 1: Hands on Cart Pole\n",
    "\n",
    "**Task 1.1:** refer to the following link [CartPole Environment](https://gymnasium.farama.org/environments/classic_control/cart_pole/) to familiarize yourself with the CartPole environment.\n",
    "\n",
    "**Note:** for a refresher on the key concepts of Gymnasium, you can visit this [Basic Usage Guide](https://gymnasium.farama.org/introduction/basic_usage/)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Print some information about the environment:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "env = gym.make(\"CartPole-v1\", render_mode=\"rgb_array\")\n",
    "\n",
    "cartpole_observation_space = cast(gym.spaces.Box, env.observation_space)\n",
    "cartpole_action_space = cast(gym.spaces.Discrete, env.action_space)\n",
    "\n",
    "cartpole_observation_dim = cartpole_observation_space.shape[0]  # Number of *dimensions* in the observation space (i.e. number of *elements* in the observation vector)\n",
    "cartpole_num_actions = cartpole_action_space.n.item()           # Number of possible actions\n",
    "\n",
    "print(f\"State space size is: {cartpole_observation_space}\")\n",
    "print(f\"Action space size is: {cartpole_action_space}\")\n",
    "print(\"Actions are: {\" + \", \".join([str(a) for a in range(cartpole_action_space.n)]) + \"}\")\n",
    "\n",
    "env.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Task 1.2:** Run the following cells and check different basic\n",
    "policies (for instance constant actions or randomly drawn actions) to discover the CartPole environment.\n",
    "Although this environment has easy dynamics that can be computed analytically, we will solve this problem with deep Reinforcement Learning methods."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": []
   },
   "source": [
    "#### Test the CartPole environment with a constant policy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "VIDEO_PREFIX_EX1_LEFT = \"lab6_ex1_cartpole_left\"\n",
    "\n",
    "(FIGS_DIR / f\"{VIDEO_PREFIX_EX1_LEFT}-episode-0.mp4\").unlink(missing_ok=True)\n",
    "\n",
    "env = gym.make(\"CartPole-v1\", render_mode=\"rgb_array\")\n",
    "env = gym.wrappers.RecordVideo(env, video_folder=str(FIGS_DIR), name_prefix=VIDEO_PREFIX_EX1_LEFT)\n",
    "\n",
    "observation, info = env.reset()\n",
    "\n",
    "for t in range(50):\n",
    "    action = 0\n",
    "    observation, reward, terminated, truncated, info = env.step(action)\n",
    "\n",
    "env.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Video(\n",
    "    FIGS_DIR / f\"{VIDEO_PREFIX_EX1_LEFT}-episode-0.mp4\",\n",
    "    embed=True,\n",
    "    html_attributes=\"controls autoplay loop\",\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "VIDEO_PREFIX_EX1_RIGHT = \"lab6_ex1_cartpole_right\"\n",
    "\n",
    "(FIGS_DIR / f\"{VIDEO_PREFIX_EX1_RIGHT}-episode-0.mp4\").unlink(missing_ok=True)\n",
    "\n",
    "env = gym.make(\"CartPole-v1\", render_mode=\"rgb_array\")\n",
    "env = gym.wrappers.RecordVideo(env, video_folder=str(FIGS_DIR), name_prefix=VIDEO_PREFIX_EX1_RIGHT)\n",
    "\n",
    "observation, info = env.reset()\n",
    "\n",
    "for t in range(50):\n",
    "    action = 1\n",
    "    observation, reward, terminated, truncated, info = env.step(action)\n",
    "\n",
    "env.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Video(\n",
    "    FIGS_DIR / f\"{VIDEO_PREFIX_EX1_RIGHT}-episode-0.mp4\",\n",
    "    embed=True,\n",
    "    html_attributes=\"controls autoplay loop\",\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Test the CartPole environment with a random policy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "VIDEO_PREFIX_EX1_RANDOM = \"lab6_ex1_cartpole_random\"\n",
    "\n",
    "(FIGS_DIR / f\"{VIDEO_PREFIX_EX1_RANDOM}-episode-0.mp4\").unlink(missing_ok=True)\n",
    "\n",
    "env = gym.make(\"CartPole-v1\", render_mode=\"rgb_array\")\n",
    "env = gym.wrappers.RecordVideo(env, video_folder=str(FIGS_DIR), name_prefix=VIDEO_PREFIX_EX1_RANDOM)\n",
    "\n",
    "observation, info = env.reset()\n",
    "\n",
    "for t in range(100):\n",
    "    action = int(cartpole_action_space.sample())  # Convert signedinteger[_64Bit] to int\n",
    "    observation, reward, terminated, truncated, info = env.step(action)\n",
    "\n",
    "env.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "lines_to_next_cell": 2
   },
   "outputs": [],
   "source": [
    "Video(\n",
    "    FIGS_DIR / f\"{VIDEO_PREFIX_EX1_RANDOM}-episode-0.mp4\",\n",
    "    embed=True,\n",
    "    html_attributes=\"controls autoplay loop\",\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part 2: Deep value-based reinforcement learning with Deep Q-Networks (DQN)\n",
    "\n",
    "In this part, we will begin our exploration of deep reinforcement learning with Deep Q-Networks (DQN), a famous value-based method.\n",
    "\n",
    "Deep reinforcement learning methods like DQN (Deep Q-Networks) are significant advancements over tabular methods such as Q-Learning because they can handle complex, high-dimensional environments that were previously intractable. While Q-Learning is limited to environments where the state and action spaces are sufficiently small to maintain a table of values, DQN uses neural networks to approximate the Q-value function, allowing it to generalize across similar states and scale to problems with vast state spaces. This enables DQN to learn optimal policies for tasks like video games, robotic control, and other applications where the number of possible states is extraordinarily large.\n",
    "\n",
    "While DQN was designed to tackle large environments like Atari games, the primary focus of this lab is to delve into the underlying algorithms, understand them thoroughly, and evaluate them comprehensively. It's important to note that working with not-so-deep networks captures the essence of deep reinforcement learning, excluding the computational expense. The transition from tabular Q-learning to DQN involves significant implications, primarily due to the ability of DQN to handle high-dimensional state spaces. Moving from DQN to very-deep-DQN is primarily a matter of scale and computational resources. The core principles remain the same, and understanding these principles is the key to mastering reinforcement learning, regardless of the complexity of the network used.\n",
    "For these reasons, in this lab, we will focus on studying the CartPole environment. The CartPole problem is a classic in reinforcement learning, and it provides a simpler and more manageable context for understanding the principles of DQN. The convergence in the CartPole environment is much faster than in Atari games - typically within a minute, as opposed to approximately 10 hours on a well-equipped personal computer for Atari games. This allows us to experiment and iterate more quickly, facilitating a deeper understanding of the algorithms at play."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Exercise 2: Implement a naive value-based reinforcement learning algorithm with approximation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Our first step will be to write a naive implementation of a version of Q-Learning, where the Q-function is approximated by a neural network. This approach combines traditional Q-Learning with the power of function approximation provided by neural networks, allowing us to handle environments with large state spaces."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### The algorithm"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<b>Input</b>:<br>\n",
    "\t$\\quad\\quad$ none<br>\n",
    "<b>Algorithm parameter</b>:<br>\n",
    "\t$\\quad\\quad$ discount factor $\\gamma$<br>\n",
    "\t$\\quad\\quad$ step size $\\alpha \\in (0,1]$<br>\n",
    "\t$\\quad\\quad$ small $\\epsilon > 0$<br><br>\n",
    "\n",
    "<b>FOR EACH</b> episode<br>\n",
    "\t$\\quad$ $\\mathbf{s} \\leftarrow \\text{env.reset}()$<br>\n",
    "\t$\\quad$ <b>DO</b> <br>\n",
    "\t\t$\\quad\\quad$ $\\mathbf{a} \\leftarrow \\epsilon\\text{-greedy}(\\mathbf{s}, \\hat{Q}_{\\mathbf{\\omega}}$ $)$<br>\n",
    "\t\t$\\quad\\quad$ $r, \\mathbf{s'} \\leftarrow \\text{env.step}(\\mathbf{a})$<br>\n",
    "\t\t$\\quad\\quad$ $y \\leftarrow\n",
    "\t\t\\begin{cases}\n",
    "\t\t\tr & \\text{for terminal } \\mathbf{s'}\\\\\n",
    "\t\t\tr + \\gamma \\max_{\\mathbf{a}^\\star \\in \\mathcal{A}} \\hat{Q}_{\\mathbf{\\omega}}(\\mathbf{s'})_{\\mathbf{a}^\\star} & \\text{for non-terminal } \\mathbf{s'}\n",
    "\t\t\\end{cases}$<br>\n",
    "\t\t$\\quad\\quad$ $\\mathbf{\\omega} \\leftarrow \\mathbf{\\omega} + \\alpha \\left[ y - \\hat{Q}_{\\mathbf{\\omega}}(\\mathbf{s})_{\\mathbf{a}} \\right] ~ \\nabla_{\\mathbf{\\omega}} \\hat{Q}_{\\mathbf{\\omega}}(\\mathbf{s})_{\\mathbf{a}}$ <br>\n",
    "\t\t$\\quad\\quad$ $\\mathbf{s} \\leftarrow \\mathbf{s'}$ <br>\n",
    "\t$\\quad$ <b>UNTIL</b> $\\mathbf{s}$ is final<br><br>\n",
    "<b>RETURN</b> $\\mathbf{\\omega}$ <br>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "lines_to_next_cell": 2
   },
   "source": [
    "#### Implement the Q-network\n",
    "\n",
    "The Q-Network is used to approximate the action-value function, which estimates the expected future reward for taking a particular action in a given state. The network is trained by minimizing the difference between its predicted Q-values and the actual returns received.\n",
    "\n",
    "In this exercise, the Q-Network is implemented as a simple feedforward neural network with two hidden layers. The input to the network consists of the observed state, and the output provides a Q-value for each possible action.  \n",
    "Each hidden layer applies the ReLU activation function, while the output layer uses a linear activation function.\n",
    "\n",
    "**Important note:** The \"naive\" agent you will implement in this exercise is **not** a DQN agent and **will not be able to solve the CartPole environment**. However, it serves as a stepping stone toward implementing the DQN agent in Exercises 3 and 4.\n",
    "\n",
    "**Task 2.1:** implement the constructor and the `forward` method of the Q-network we will use in our RL agents"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "lines_to_next_cell": 2
   },
   "outputs": [],
   "source": [
    "class QNetwork(torch.nn.Module):\n",
    "    \"\"\"\n",
    "    A Q-Network implemented with PyTorch.\n",
    "\n",
    "    Attributes\n",
    "    ----------\n",
    "    layer1 : torch.nn.Linear\n",
    "        First fully connected layer.\n",
    "    layer2 : torch.nn.Linear\n",
    "        Second fully connected layer.\n",
    "    layer3 : torch.nn.Linear\n",
    "        Third fully connected layer.\n",
    "\n",
    "    Methods\n",
    "    -------\n",
    "    forward(x: torch.Tensor) -> torch.Tensor\n",
    "        Define the forward pass of the QNetwork.\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, n_observations: int, n_actions: int, nn_l1: int, nn_l2: int):\n",
    "        \"\"\"\n",
    "        Initialize a new instance of QNetwork.\n",
    "\n",
    "        Parameters\n",
    "        ----------\n",
    "        n_observations : int\n",
    "            The size of the observation space.\n",
    "        n_actions : int\n",
    "            The size of the action space.\n",
    "        nn_l1 : int\n",
    "            The number of neurons on the first layer.\n",
    "        nn_l2 : int\n",
    "            The number of neurons on the second layer.\n",
    "        \"\"\"\n",
    "        super(QNetwork, self).__init__()\n",
    "\n",
    "        # TODO...\n",
    "\n",
    "    def forward(self, x: torch.Tensor) -> torch.Tensor:\n",
    "        \"\"\"\n",
    "        Define the forward pass of the QNetwork.\n",
    "\n",
    "        Parameters\n",
    "        ----------\n",
    "        x : torch.Tensor\n",
    "            The input tensor (state).\n",
    "\n",
    "        Returns\n",
    "        -------\n",
    "        torch.Tensor\n",
    "            The output tensor (Q-values).\n",
    "        \"\"\"\n",
    "\n",
    "        # TODO...\n",
    "\n",
    "        return output_tensor"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "lines_to_next_cell": 2
   },
   "source": [
    "#### Implement an inference function"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Parctical tips\n",
    "\n",
    "To complete task 2.2, you need to be familiar with the following PyTorch concepts:\n",
    "\n",
    "- [torch.squeeze](https://pytorch.org/docs/stable/generated/torch.squeeze.html) (or [torch.Tensor.squeeze](https://pytorch.org/docs/stable/generated/torch.Tensor.squeeze.html)) / [torch.unsqueeze](https://pytorch.org/docs/stable/generated/torch.unsqueeze.html) (or [torch.Tensor.unsqueeze](https://pytorch.org/docs/stable/generated/torch.Tensor.unsqueeze.html)) to add / remove a batch dimension.\n",
    "- [torch.argmax](https://pytorch.org/docs/stable/generated/torch.argmax.html) (or [torch.Tensor.argmax](https://pytorch.org/docs/stable/generated/torch.Tensor.argmax.html)) to get the index of the maximum value of a tensor.\n",
    "- [torch.Tensor.item](https://pytorch.org/docs/stable/generated/torch.Tensor.item.html) to get the value of a tensor with a single element as a standard Python number."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Task 2.2:** Your next assignment is to complete the function below, which will be used to evaluate the performance of an agent in a simulated environment over one or multiple episodes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def test_q_network_agent(\n",
    "    env: gym.Env, q_network: torch.nn.Module, num_episode: int = 1\n",
    ") -> List[float]:\n",
    "    \"\"\"\n",
    "    Test a naive agent in the given environment using the provided Q-network.\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    env : gym.Env\n",
    "        The environment in which to test the agent.\n",
    "    q_network : torch.nn.Module\n",
    "        The Q-network to use for decision making.\n",
    "    num_episode : int, optional\n",
    "        The number of episodes to run, by default 1.\n",
    "\n",
    "    Returns\n",
    "    -------\n",
    "    List[float]\n",
    "        A list of rewards per episode.\n",
    "    \"\"\"\n",
    "    episode_reward_list = []\n",
    "\n",
    "    for episode_id in range(num_episode):\n",
    "        state, info = env.reset()\n",
    "        done = False\n",
    "        episode_reward = 0.0\n",
    "\n",
    "        while not done:\n",
    "            # Convert the state to a PyTorch tensor and add a batch dimension (unsqueeze)\n",
    "            state_tensor = torch.tensor(state, dtype=torch.float32, device=device).unsqueeze(0)\n",
    "\n",
    "            # q_values = TODO... # Compute the Q-values for the current state using the Q-network\n",
    "\n",
    "            # action = TODO... # Select the action with the highest Q-value\n",
    "\n",
    "            next_state, reward, terminated, truncated, info = env.step(action)\n",
    "            done = terminated or truncated\n",
    "\n",
    "            episode_reward += float(reward)\n",
    "\n",
    "            state = next_state\n",
    "\n",
    "        episode_reward_list.append(episode_reward)\n",
    "        print(f\"Episode reward: {episode_reward}\")\n",
    "\n",
    "    return episode_reward_list"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Task 2.3:** Test this function on the untrained agent."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "q_network = QNetwork(cartpole_observation_dim, cartpole_num_actions, nn_l1=128, nn_l2=128).to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "lines_to_next_cell": 2
   },
   "outputs": [],
   "source": [
    "VIDEO_PREFIX_EX2_DQN_NAIVE_UNTRAINED = \"lab6_ex2_dqn_naive_untained\"\n",
    "\n",
    "NUM_EPISODES = 3\n",
    "\n",
    "file_path_list = [\n",
    "    FIGS_DIR / f\"{VIDEO_PREFIX_EX2_DQN_NAIVE_UNTRAINED}-episode-{episode_index}.mp4\"\n",
    "    for episode_index in range(NUM_EPISODES)\n",
    "]\n",
    "\n",
    "for file_path in file_path_list:\n",
    "    file_path.unlink(missing_ok=True)\n",
    "\n",
    "env = gym.make(\"CartPole-v1\", render_mode=\"rgb_array\")\n",
    "env = gym.wrappers.RecordVideo(\n",
    "    env,\n",
    "    video_folder=str(FIGS_DIR),\n",
    "    name_prefix=VIDEO_PREFIX_EX2_DQN_NAIVE_UNTRAINED,\n",
    "    episode_trigger=lambda x: True,\n",
    ")\n",
    "env = gym.wrappers.RecordEpisodeStatistics(env, buffer_length=NUM_EPISODES)\n",
    "\n",
    "test_q_network_agent(env, q_network, num_episode=NUM_EPISODES)\n",
    "\n",
    "# print(f'Episode time taken: {env.time_queue}')\n",
    "# print(f'Episode total rewards: {env.return_queue}')\n",
    "# print(f'Episode lengths: {env.length_queue}')\n",
    "\n",
    "env.close()\n",
    "\n",
    "print(\"\\nSelect the episode to play here 👇\\n\")\n",
    "\n",
    "interact(video_selector, file_path=file_path_list)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Question 1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Copy and paste the output of the following cell into the first question of the [Lab 6 - Evaluation](https://moodle.polytechnique.fr/course/section.php?id=66534) in Moodle:\n",
    "*\"What is the total number of parameters in the Q-Network constructed in the second exercise?\"*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(f\"\\n\\n⚠️ Copy and paste this value into Moodle 👉 {len(torch.nn.utils.parameters_to_vector(q_network.parameters()))}\\n\\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "lines_to_next_cell": 2
   },
   "source": [
    "#### Implement the epsilon greedy function\n",
    "\n",
    "In the following cell, you will implement the epsilon-greedy strategy, which is a crucial component in balancing exploration and exploitation during the learning process of our reinforcement learning agent.\n",
    "In this implementation, the epsilon value will decrease over time to encourage the agent to explore more in the early stages of training and exploit more as training progresses. After a certain number of episodes, the epsilon value will remain constant at a minimum value.\n",
    "\n",
    "The figure below illustrates the usage of `epsilon`, `epsilon_min` and `epsilon_start`:\n",
    "\n",
    "<img src=\"https://raw.githubusercontent.com/jeremiedecock/polytechnique-csc-52081-ep-2025-students/main/assets/lab6_epsilon.svg\" width=\"600px\" />\n",
    "\n",
    "**Task 2.4:** Now, let's proceed to implement the epsilon-greedy strategy, which is a crucial component in balancing exploration and exploitation during the learning process of our reinforcement learning agent. To accomplish this, complete the `__call__` function in the following code block.\n",
    "\n",
    "**Tips:** `EpsilonGreedy` is a *[callable](https://docs.python.org/3/glossary.html#term-callable) class* (a.k.a a Python *functor*) that can be used as a function. The `__call__` method is called when the instance is \"called\" as a function. In this case, the `__call__` method should return a random action with probability `epsilon` and the action with the highest Q-value with probability `1 - epsilon`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "lines_to_next_cell": 2
   },
   "outputs": [],
   "source": [
    "class EpsilonGreedy:\n",
    "    \"\"\"\n",
    "    An Epsilon-Greedy policy.\n",
    "\n",
    "    Attributes\n",
    "    ----------\n",
    "    epsilon : float\n",
    "        The initial probability of choosing a random action.\n",
    "    epsilon_min : float\n",
    "        The minimum probability of choosing a random action.\n",
    "    epsilon_decay : float\n",
    "        The decay rate for the epsilon value after each action.\n",
    "    env : gym.Env\n",
    "        The environment in which the agent is acting.\n",
    "    q_network : torch.nn.Module\n",
    "        The Q-Network used to estimate action values.\n",
    "\n",
    "    Methods\n",
    "    -------\n",
    "    __call__(state: np.ndarray) -> np.int64\n",
    "        Select an action for the given state using the epsilon-greedy policy.\n",
    "    decay_epsilon()\n",
    "        Decay the epsilon value after each action.\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(\n",
    "        self,\n",
    "        epsilon_start: float,\n",
    "        epsilon_min: float,\n",
    "        epsilon_decay: float,\n",
    "        env: gym.Env,\n",
    "        q_network: torch.nn.Module,\n",
    "    ):\n",
    "        \"\"\"\n",
    "        Initialize a new instance of EpsilonGreedy.\n",
    "\n",
    "        Parameters\n",
    "        ----------\n",
    "        epsilon_start : float\n",
    "            The initial probability of choosing a random action.\n",
    "        epsilon_min : float\n",
    "            The minimum probability of choosing a random action.\n",
    "        epsilon_decay : float\n",
    "            The decay rate for the epsilon value after each episode.\n",
    "        env : gym.Env\n",
    "            The environment in which the agent is acting.\n",
    "        q_network : torch.nn.Module\n",
    "            The Q-Network used to estimate action values.\n",
    "        \"\"\"\n",
    "        self.epsilon = epsilon_start\n",
    "        self.epsilon_min = epsilon_min\n",
    "        self.epsilon_decay = epsilon_decay\n",
    "        self.env = env\n",
    "        self.q_network = q_network\n",
    "\n",
    "    def __call__(self, state: np.ndarray) -> np.int64:\n",
    "        \"\"\"\n",
    "        Select an action for the given state using the epsilon-greedy policy.\n",
    "\n",
    "        If a randomly chosen number is less than epsilon, a random action is chosen.\n",
    "        Otherwise, the action with the highest estimated action value is chosen.\n",
    "\n",
    "        Parameters\n",
    "        ----------\n",
    "        state : np.ndarray\n",
    "            The current state of the environment.\n",
    "\n",
    "        Returns\n",
    "        -------\n",
    "        np.int64\n",
    "            The chosen action.\n",
    "        \"\"\"\n",
    "\n",
    "        if random.random() < self.epsilon:\n",
    "            # action = TODO... # Select a random action\n",
    "        else:\n",
    "            with torch.no_grad():\n",
    "                # state_tensor = TODO... # Convert the state to a PyTorch tensor and add a batch dimension (unsqueeze)\n",
    "\n",
    "                # q_values = TODO... # Compute the Q-values for the current state using the Q-network\n",
    "\n",
    "                # action = TODO... # Select the action with the highest Q-value\n",
    "\n",
    "        return action\n",
    "\n",
    "    def decay_epsilon(self):\n",
    "        \"\"\"\n",
    "        Decay the epsilon value after each episode.\n",
    "\n",
    "        The new epsilon value is the maximum of `epsilon_min` and the product of the current\n",
    "        epsilon value and `epsilon_decay`.\n",
    "        \"\"\"\n",
    "        self.epsilon = max(self.epsilon_min, self.epsilon * self.epsilon_decay)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "lines_to_next_cell": 2
   },
   "source": [
    "#### Implementing a Learning Rate Scheduler\n",
    "\n",
    "The following cell introduces a PyTorch Learning Rate (LR) scheduler. This scheduler is used for managing and adjusting the learning rate ($\\alpha$ in the \"naive value-based reinforcement learning algorithm\") throughout the training process of our agent. It is designed to adjust the learning rate of an optimizer at each *epoch*, following an exponential decay strategy, but with a lower limit on the learning rate.\n",
    "\n",
    "This class will be used in *Task 2.5*."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "lines_to_next_cell": 2
   },
   "outputs": [],
   "source": [
    "class MinimumExponentialLR(torch.optim.lr_scheduler.ExponentialLR):\n",
    "    def __init__(\n",
    "        self,\n",
    "        optimizer: torch.optim.Optimizer,\n",
    "        lr_decay: float,\n",
    "        last_epoch: int = -1,\n",
    "        min_lr: float = 1e-6,\n",
    "    ):\n",
    "        \"\"\"\n",
    "        Initialize a new instance of MinimumExponentialLR.\n",
    "\n",
    "        Parameters\n",
    "        ----------\n",
    "        optimizer : torch.optim.Optimizer\n",
    "            The optimizer whose learning rate should be scheduled.\n",
    "        lr_decay : float\n",
    "            The multiplicative factor of learning rate decay.\n",
    "        last_epoch : int, optional\n",
    "            The index of the last epoch. Default is -1.\n",
    "        min_lr : float, optional\n",
    "            The minimum learning rate. Default is 1e-6.\n",
    "        \"\"\"\n",
    "        self.min_lr = min_lr\n",
    "        super().__init__(optimizer, lr_decay, last_epoch=-1)\n",
    "\n",
    "    def get_lr(self) -> List[float]:\n",
    "        \"\"\"\n",
    "        Compute learning rate using chainable form of the scheduler.\n",
    "\n",
    "        Returns\n",
    "        -------\n",
    "        List[float]\n",
    "            The learning rates of each parameter group.\n",
    "        \"\"\"\n",
    "        return [\n",
    "            max(base_lr * self.gamma**self.last_epoch, self.min_lr)\n",
    "            for base_lr in self.base_lrs\n",
    "        ]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "lines_to_next_cell": 2
   },
   "source": [
    "#### Implementing the Training Function\n",
    "\n",
    "The following function is the final component of our initial agent. It orchestrates the training process, enabling the agent to learn from its interactions with the environment.\n",
    "\n",
    "During each episode, the agent selects actions based on an epsilon-greedy policy, observes the next state and reward from the environment, and updates the weights of the Q-Network based on the observed reward and the maximum predicted Q-value of the next state."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Parctical tips\n",
    "\n",
    "To compute the target value, you can eliminate the need for an if statement to differentiate between terminal and non-terminal states by using the following formula:  \n",
    "\n",
    "$$\n",
    "y = r + \\gamma \\max_{\\mathbf{a}^\\star \\in \\mathcal{A}} \\hat{Q}_{\\mathbf{\\omega}}(\\mathbf{s'})_{\\mathbf{a}^\\star} \\times (1 - \\text{done})\n",
    "$$\n",
    "\n",
    "where $\\text{done} = 1$ if $s'$ is a terminal state and 0 otherwise.\n",
    "\n",
    "To complete task 2.5, you also need to be familiar with the following PyTorch concepts:\n",
    "\n",
    "- [torch.max](https://pytorch.org/docs/stable/generated/torch.max.html) (or [torch.Tensor.max](https://pytorch.org/docs/stable/generated/torch.Tensor.max.html)) to get the maximum value of a tensor."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Task 2.5:** complete this function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_naive_agent(\n",
    "    env: gym.Env,\n",
    "    q_network: torch.nn.Module,\n",
    "    optimizer: torch.optim.Optimizer,\n",
    "    loss_fn: Callable,\n",
    "    epsilon_greedy: EpsilonGreedy,\n",
    "    device: torch.device,\n",
    "    lr_scheduler: torch.optim.lr_scheduler.LRScheduler,\n",
    "    num_episodes: int,\n",
    "    gamma: float,\n",
    ") -> List[float]:\n",
    "    \"\"\"\n",
    "    Train the Q-network on the given environment.\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    env : gym.Env\n",
    "        The environment to train on.\n",
    "    q_network : torch.nn.Module\n",
    "        The Q-network to train.\n",
    "    optimizer : torch.optim.Optimizer\n",
    "        The optimizer to use for training.\n",
    "    loss_fn : callable\n",
    "        The loss function to use for training.\n",
    "    epsilon_greedy : EpsilonGreedy\n",
    "        The epsilon-greedy policy to use for action selection.\n",
    "    device : torch.device\n",
    "        The device to use for PyTorch computations.\n",
    "    lr_scheduler : torch.optim.lr_scheduler.LRScheduler\n",
    "        The learning rate scheduler to adjust the learning rate during training.\n",
    "    num_episodes : int\n",
    "        The number of episodes to train for.\n",
    "    gamma : float\n",
    "        The discount factor for future rewards.\n",
    "\n",
    "    Returns\n",
    "    -------\n",
    "    List[float]\n",
    "        A list of cumulated rewards per episode.\n",
    "    \"\"\"\n",
    "    episode_reward_list = []\n",
    "\n",
    "    for episode_index in tqdm(range(1, num_episodes)):\n",
    "        state, info = env.reset()\n",
    "        episode_reward = 0.0\n",
    "\n",
    "        for t in itertools.count():\n",
    "\n",
    "            # Get action, next_state and reward\n",
    "\n",
    "            action = epsilon_greedy(state)\n",
    "\n",
    "            next_state, reward, terminated, truncated, info = env.step(action)\n",
    "            done = terminated or truncated\n",
    "\n",
    "            episode_reward += float(reward)\n",
    "\n",
    "            # Update the q_network weights\n",
    "\n",
    "            with torch.no_grad():\n",
    "                # next_state_tensor = TODO... # Convert the next_state to a PyTorch tensor and add a batch dimension (unsqueeze)\n",
    "\n",
    "                # target = TODO... # Compute the **target** Q-value\n",
    "\n",
    "            # state_tensor = TODO... # Convert the state to a PyTorch tensor and add a batch dimension (unsqueeze)\n",
    "\n",
    "            # q_values = TODO... # Compute the Q-values for the current state using the Q-network\n",
    "\n",
    "            # q_value_of_current_action = TODO... # Get the Q-value of the current action\n",
    "\n",
    "            # loss = TODO... # Compute the loss\n",
    "\n",
    "            optimizer.zero_grad()\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "\n",
    "            lr_scheduler.step()\n",
    "\n",
    "            if done:\n",
    "                break\n",
    "\n",
    "            state = next_state\n",
    "\n",
    "\n",
    "        episode_reward_list.append(episode_reward)\n",
    "        epsilon_greedy.decay_epsilon()\n",
    "\n",
    "    return episode_reward_list"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Train the agent"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "env = gym.make(\"CartPole-v1\")\n",
    "\n",
    "NUMBER_OF_TRAININGS = DEFAULT_NUMBER_OF_TRAININGS    # Change the default (global) value here if you want a specific number of trainings for this exercise\n",
    "naive_trains_result_list: List[List[Union[int, float]]] = [[], [], []]    # [int, float, int]\n",
    "\n",
    "for train_index in range(NUMBER_OF_TRAININGS):\n",
    "    # Instantiate required objects\n",
    "\n",
    "    q_network = QNetwork(cartpole_observation_dim, cartpole_num_actions, nn_l1=128, nn_l2=128).to(device)\n",
    "    optimizer = torch.optim.AdamW(q_network.parameters(), lr=0.004, amsgrad=True)\n",
    "    # lr_scheduler = torch.optim.lr_scheduler.ExponentialLR(optimizer, gamma=0.999)\n",
    "    lr_scheduler = MinimumExponentialLR(optimizer, lr_decay=0.97, min_lr=0.0001)\n",
    "    loss_fn = torch.nn.MSELoss()\n",
    "\n",
    "    epsilon_greedy = EpsilonGreedy(\n",
    "        epsilon_start=0.82,\n",
    "        epsilon_min=0.013,\n",
    "        epsilon_decay=0.9675,\n",
    "        env=env,\n",
    "        q_network=q_network,\n",
    "    )\n",
    "\n",
    "    # Train the q-network\n",
    "\n",
    "    episode_reward_list = train_naive_agent(\n",
    "        env,\n",
    "        q_network,\n",
    "        optimizer,\n",
    "        loss_fn,\n",
    "        epsilon_greedy,\n",
    "        device,\n",
    "        lr_scheduler,\n",
    "        num_episodes=150,\n",
    "        gamma=0.9,\n",
    "    )\n",
    "    naive_trains_result_list[0].extend(range(len(episode_reward_list)))\n",
    "    naive_trains_result_list[1].extend(episode_reward_list)\n",
    "    naive_trains_result_list[2].extend([train_index for _ in episode_reward_list])\n",
    "\n",
    "naive_trains_result_df = pd.DataFrame(\n",
    "    np.array(naive_trains_result_list).T,\n",
    "    columns=[\"num_episodes\", \"mean_final_episode_reward\", \"training_index\"],\n",
    ")\n",
    "naive_trains_result_df[\"agent\"] = \"Naive\"\n",
    "\n",
    "# Save the action-value estimation function of the last train\n",
    "\n",
    "torch.save(q_network, MODELS_DIR / \"lab6_naive_q_network.pth\")\n",
    "\n",
    "env.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Plot results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "lines_to_next_cell": 0
   },
   "outputs": [],
   "source": [
    "g = sns.relplot(\n",
    "    x=\"num_episodes\",\n",
    "    y=\"mean_final_episode_reward\",\n",
    "    kind=\"line\",\n",
    "    hue=\"agent\",\n",
    "    estimator=None,\n",
    "    units=\"training_index\",\n",
    "    data=naive_trains_result_df,\n",
    "    height=7,\n",
    "    aspect=2,\n",
    "    alpha=0.5,\n",
    ")\n",
    "plt.savefig(PLOTS_DIR / \"lab6_dqn_naive_trains_result.png\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "lines_to_next_cell": 0
   },
   "outputs": [],
   "source": [
    "g = sns.relplot(\n",
    "    x=\"num_episodes\",\n",
    "    y=\"mean_final_episode_reward\",\n",
    "    hue=\"agent\",\n",
    "    kind=\"line\",\n",
    "    data=naive_trains_result_df,\n",
    "    height=7,\n",
    "    aspect=2,\n",
    ")\n",
    "plt.savefig(PLOTS_DIR / \"lab6_dqn_naive_trains_result_agg.png\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Test it"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "lines_to_next_cell": 2
   },
   "outputs": [],
   "source": [
    "VIDEO_PREFIX_EX2_DQN_NAIVE_TRAINED = \"lab6_ex2_dqn_naive_tained\"\n",
    "\n",
    "NUM_EPISODES = 3\n",
    "\n",
    "file_path_list = [\n",
    "    FIGS_DIR / f\"{VIDEO_PREFIX_EX2_DQN_NAIVE_TRAINED}-episode-{episode_index}.mp4\"\n",
    "    for episode_index in range(NUM_EPISODES)\n",
    "]\n",
    "\n",
    "for file_path in file_path_list:\n",
    "    file_path.unlink(missing_ok=True)\n",
    "\n",
    "env = gym.make(\"CartPole-v1\", render_mode=\"rgb_array\")\n",
    "env = gym.wrappers.RecordVideo(\n",
    "    env,\n",
    "    video_folder=str(FIGS_DIR),\n",
    "    name_prefix=VIDEO_PREFIX_EX2_DQN_NAIVE_TRAINED,\n",
    "    episode_trigger=lambda x: True,\n",
    ")\n",
    "env = gym.wrappers.RecordEpisodeStatistics(env, buffer_length=NUM_EPISODES)\n",
    "\n",
    "test_q_network_agent(env, q_network, num_episode=NUM_EPISODES)\n",
    "\n",
    "# print(f'Episode time taken: {env.time_queue}')\n",
    "# print(f'Episode total rewards: {env.return_queue}')\n",
    "# print(f'Episode lengths: {env.length_queue}')\n",
    "\n",
    "env.close()\n",
    "\n",
    "print(\"\\nSelect the episode to play here 👇\\n\")\n",
    "\n",
    "interact(video_selector, file_path=file_path_list)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Why It Doesn't Work: The Complexity of Deep Reinforcement Learning\n",
    "\n",
    "Our initial deep value-based agent did not converge, primarily due to the three fundamental challenges of value-based deep reinforcement learning:\n",
    "\n",
    "1. **Coverage**: Convergence to the optimal Q-function relies on comprehensive coverage of the state space. However, in the context of deep RL, the state space is often too large to be fully covered. In situations where not all states are sampled due to their vast number, the guarantee of convergence no longer holds.\n",
    "\n",
    "2. **Correlation**: The probability of transitioning to the next state is highly influenced by the current state. This strong correlation can lead to local overfitting and the risk of becoming trapped in a local optimum: the neural network, which approximates the Q-function, may become overly specialized in a small portion of the action-state space and neglect the rest.\n",
    "\n",
    "3. **Convergence**: The \"targets\" used as the truth to be achieved \"move\" during the learning process. For the same prediction (estimation of the value of a state-action pair, i.e., its Q-value), the loss of a given example changes during the learning process (due to *bootstrapping* a main concept of TD-Learning). In other words, DQN tries to minimize a moving target, a target that depends on the model we are learning and optimizing. This can lead to instability and make it difficult for the learning process to converge to an optimal policy.\n",
    "\n",
    "In the following sections, we will explore strategies to address these challenges and improve the performance of our deep reinforcement learning agent."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Exercise 3: Implement Deep Q-Networks v1 (DQN version 2013 with experience replay)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In 2013, DeepMind made a significant contribution to the field of reinforcement learning with the publication of the paper \"Playing Atari with Deep Reinforcement Learning\" by Volodymyr Mnih and al (https://arxiv.org/abs/1312.5602). This paper marked the introduction of the first version of Deep Q-Networks (DQN).\n",
    "\n",
    "The paper's primary innovation was the development of a technique to decorrelate states in reinforcement learning. This technique, known as *experience replay*, leverages a *replay buffer* to store and sample experiences. The introduction of experience replay greatly enhanced the stability and efficiency of the learning process."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Experience replay\n",
    "\n",
    "Experience replay is a key technique used in Deep Q-Networks (DQN) to address the issues of correlation.\n",
    "\n",
    "In a typical reinforcement learning setup, an agent learns by interacting with the environment, receiving feedback in the form of rewards, and updating its policy based on this feedback. This process is inherently sequential and the successive states are highly correlated, which can lead to overfitting and instability in learning.\n",
    "\n",
    "Experience replay addresses these issues by storing the agent's experiences, i.e., the tuples of (state, action, reward, next state), in a data structure known as a replay buffer. During the learning process, instead of learning from the most recent experience, the agent randomly samples a batch of experiences from the replay buffer. This random sampling breaks the correlation between successive experiences, leading to more stable and robust learning.\n",
    "\n",
    "Also, by learning from past experiences, the agent can effectively learn from a fixed target, which mitigates the issue of learning from a moving target. This is because the experiences in the replay buffer remain fixed once they are stored, even though the agent's policy continues to evolve."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### DQN v2013 Algorithm"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Note: main differences with the previous algorithm are highlighted in red."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<b>Input</b>:<br>\n",
    "\t$\\quad\\quad$ none<br>\n",
    "<b>Algorithm parameter</b>:<br>\n",
    "\t$\\quad\\quad$ discount factor $\\gamma$<br>\n",
    "\t$\\quad\\quad$ step size $\\alpha \\in (0,1]$<br>\n",
    "\t$\\quad\\quad$ small $\\epsilon > 0$<br>\n",
    "\t$\\quad\\quad$ capacity of the experience replay memory $\\color{red}{M}$<br>\n",
    "\t$\\quad\\quad$ batch size $\\color{red}{m}$<br><br>\n",
    "\n",
    "<b>Initialize</b> replay memory $\\color{red}{\\mathcal{D}}$ to capacity $M$<br>\n",
    "<b>Initialize</b> action-value function $\\hat{Q}$ with random weights $\\mathbf{\\omega}$<br><br>\n",
    "\n",
    "<b>FOR EACH</b> episode<br>\n",
    "\t$\\quad$ $\\mathbf{s} \\leftarrow \\text{env.reset}()$<br>\n",
    "\t$\\quad$ <b>DO</b> <br>\n",
    "\t\t$\\quad\\quad$ $\\mathbf{a} \\leftarrow \\epsilon\\text{-greedy}(\\mathbf{s}, \\hat{Q}_{\\mathbf{\\omega}})$<br>\n",
    "\t\t$\\quad\\quad$ $r, \\mathbf{s'} \\leftarrow \\text{env.step}(\\mathbf{a})$<br>\n",
    "\t\t$\\quad\\quad$ $\\color{red}{\\text{Store transition } (\\mathbf{s}, \\mathbf{a}, r, \\mathbf{s'}) \\text{ in } \\mathcal{D}}$<br>\n",
    "\t\t$\\quad\\quad$ $\\color{red}{\\text{IF } \\mathcal{D} \\text{ contains \"enough\" transitions}}$<br>\n",
    "\t\t\t$\\quad\\quad\\quad$ $\\color{red}{\\text{Sample random batch of transitions } (\\mathbf{s}_j, \\mathbf{a}_j, r_j, \\mathbf{s'}_j) \\text{ from } \\mathcal{D} \\text{ with } j=1 \\text{ to } m}$<br>\n",
    "\t\t\t$\\quad\\quad\\quad$ $\\color{red}{\\text{For each } j}$, set $y_j \\leftarrow\n",
    "\t\t\t\\begin{cases}\n",
    "\t\t\tr_j & \\text{for terminal } \\mathbf{s'}_j\\\\\n",
    "\t\t\tr_j + \\gamma \\max_{\\mathbf{a}^\\star} \\hat{Q}_{\\mathbf{\\omega}}(\\mathbf{s'}_j)_{\\mathbf{a}^\\star} & \\text{for non-terminal } \\mathbf{s'}_j\n",
    "\t\t\t\\end{cases}$<br>\n",
    "\t\t\t$\\quad\\quad\\quad$ Perform a gradient descent step on $\\left( y_j - \\hat{Q}_{\\mathbf{\\omega}}(\\mathbf{s}_j)_{\\mathbf{a}_j} \\right)^2$ with respect to the weights $\\mathbf{\\omega}$<br>\n",
    "\t\t$\\quad\\quad$ $\\mathbf{s} \\leftarrow \\mathbf{s'}$ <br>\n",
    "\t$\\quad$ <b>UNTIL</b> $\\mathbf{s}$ is final<br><br>\n",
    "<b>RETURN</b> $\\mathbf{\\omega}$ <br>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "lines_to_next_cell": 2
   },
   "source": [
    "#### Implement the Replay Buffer\n",
    "\n",
    "To incorporate experience replay into the provided naive deep value-based reinforcement learning agent definition, we need to introduce a memory buffer where experiences are stored, and then update the algorithm to sample a random batch of experiences from this buffer to update the weights."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "lines_to_next_cell": 2
   },
   "outputs": [],
   "source": [
    "class ReplayBuffer:\n",
    "    \"\"\"\n",
    "    A Replay Buffer.\n",
    "\n",
    "    Attributes\n",
    "    ----------\n",
    "    buffer : collections.deque\n",
    "        A double-ended queue where the transitions are stored.\n",
    "\n",
    "    Methods\n",
    "    -------\n",
    "    add(state: np.ndarray, action: np.int64, reward: float, next_state: np.ndarray, done: bool)\n",
    "        Add a new transition to the buffer.\n",
    "    sample(batch_size: int) -> Tuple[np.ndarray, float, float, np.ndarray, bool]\n",
    "        Sample a batch of transitions from the buffer.\n",
    "    __len__()\n",
    "        Return the current size of the buffer.\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, capacity: int):\n",
    "        \"\"\"\n",
    "        Initializes a ReplayBuffer instance.\n",
    "\n",
    "        Parameters\n",
    "        ----------\n",
    "        capacity : int\n",
    "            The maximum number of transitions that can be stored in the buffer.\n",
    "        \"\"\"\n",
    "        self.buffer: collections.deque = collections.deque(maxlen=capacity)\n",
    "\n",
    "    def add(\n",
    "        self,\n",
    "        state: np.ndarray,\n",
    "        action: np.int64,\n",
    "        reward: float,\n",
    "        next_state: np.ndarray,\n",
    "        done: bool,\n",
    "    ):\n",
    "        \"\"\"\n",
    "        Add a new transition to the buffer.\n",
    "\n",
    "        Parameters\n",
    "        ----------\n",
    "        state : np.ndarray\n",
    "            The state vector of the added transition.\n",
    "        action : np.int64\n",
    "            The action of the added transition.\n",
    "        reward : float\n",
    "            The reward of the added transition.\n",
    "        next_state : np.ndarray\n",
    "            The next state vector of the added transition.\n",
    "        done : bool\n",
    "            The final state of the added transition.\n",
    "        \"\"\"\n",
    "        self.buffer.append((state, action, reward, next_state, done))\n",
    "\n",
    "    def sample(\n",
    "        self, batch_size: int\n",
    "    ) -> Tuple[np.ndarray, Tuple[int], Tuple[float], np.ndarray, Tuple[bool]]:\n",
    "        \"\"\"\n",
    "        Sample a batch of transitions from the buffer.\n",
    "\n",
    "        Parameters\n",
    "        ----------\n",
    "        batch_size : int\n",
    "            The number of transitions to sample.\n",
    "\n",
    "        Returns\n",
    "        -------\n",
    "        Tuple[np.ndarray, float, float, np.ndarray, bool]\n",
    "            A batch of `batch_size` transitions.\n",
    "        \"\"\"\n",
    "        # Here, `random.sample(self.buffer, batch_size)`\n",
    "        # returns a list of tuples `(state, action, reward, next_state, done)`\n",
    "        # where:\n",
    "        # - `state`  and `next_state` are numpy arrays\n",
    "        # - `action` and `reward` are floats\n",
    "        # - `done` is a boolean\n",
    "        #\n",
    "        # `states, actions, rewards, next_states, dones = zip(*random.sample(self.buffer, batch_size))`\n",
    "        # generates 5 tuples `state`, `action`, `reward`, `next_state` and `done`, each having `batch_size` elements.\n",
    "        states, actions, rewards, next_states, dones = zip(*random.sample(self.buffer, batch_size))\n",
    "        return np.array(states), actions, rewards, np.array(next_states), dones\n",
    "\n",
    "    def __len__(self):\n",
    "        \"\"\"\n",
    "        Return the current size of the buffer.\n",
    "\n",
    "        Returns\n",
    "        -------\n",
    "        int\n",
    "            The current size of the buffer.\n",
    "        \"\"\"\n",
    "        return len(self.buffer)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "lines_to_next_cell": 2
   },
   "source": [
    "#### Implement the training function\n",
    "\n",
    "The training function of our initial deep value-based agent needs to be modified to incorporate the use of the replay buffer effectively.\n",
    "\n",
    "1. **Store Experiences**: After the agent takes an action and receives a reward and the next state from the environment, store this experience in the replay buffer.\n",
    "\n",
    "2. **Sample Experiences**: Instead of using the most recent experience to update the agent's policy, randomly sample a batch of experiences from the replay buffer.\n",
    "\n",
    "3. **Compute Loss and Update Weights**: Use the sampled experiences to compute the loss and update the weights of the Q-Network.\n",
    "\n",
    "4. **Handle Terminal States**: If the 'done' flag of an experience is True, indicating a terminal state, make sure to adjust the target Q-value to be just the received reward. This is because there are no future rewards possible after a terminal state."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Parctical tips\n",
    "\n",
    "A key practical difference from the previous exercise is that we now train the Q-network on a (mini)batch of transitions rather than a single transition.  \n",
    "\n",
    "To extract $\\hat{Q}_{\\mathbf{\\omega}}(\\mathbf{s}_j)_{\\mathbf{a}_j}$ from the `q_network` output, it is essential to understand the following PyTorch concept:  \n",
    "\n",
    "- **[torch.gather](https://pytorch.org/docs/stable/generated/torch.gather.html) (or [torch.Tensor.gather](https://pytorch.org/docs/stable/generated/torch.Tensor.gather.html))**: This function retrieves values along a specified axis using an index tensor, making it particularly useful for selecting Q-values corresponding to chosen actions."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Task 3.1:** complete the `train_dqn1_agent` to use the replay buffer."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_dqn1_agent(\n",
    "    env: gym.Env,\n",
    "    q_network: torch.nn.Module,\n",
    "    optimizer: torch.optim.Optimizer,\n",
    "    loss_fn: Callable,\n",
    "    epsilon_greedy: EpsilonGreedy,\n",
    "    device: torch.device,\n",
    "    lr_scheduler: torch.optim.lr_scheduler.LRScheduler,\n",
    "    num_episodes: int,\n",
    "    gamma: float,\n",
    "    batch_size: int,\n",
    "    replay_buffer: ReplayBuffer,\n",
    ") -> List[float]:\n",
    "    \"\"\"\n",
    "    Train the Q-network on the given environment.\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    env : gym.Env\n",
    "        The environment to train on.\n",
    "    q_network : torch.nn.Module\n",
    "        The Q-network to train.\n",
    "    optimizer : torch.optim.Optimizer\n",
    "        The optimizer to use for training.\n",
    "    loss_fn : callable\n",
    "        The loss function to use for training.\n",
    "    epsilon_greedy : EpsilonGreedy\n",
    "        The epsilon-greedy policy to use for action selection.\n",
    "    device : torch.device\n",
    "        The device to use for PyTorch computations.\n",
    "    lr_scheduler : torch.optim.lr_scheduler.LRScheduler\n",
    "        The learning rate scheduler to adjust the learning rate during training.\n",
    "    num_episodes : int\n",
    "        The number of episodes to train for.\n",
    "    gamma : float\n",
    "        The discount factor for future rewards.\n",
    "    batch_size : int\n",
    "        The size of the batch to use for training.\n",
    "    replay_buffer : ReplayBuffer\n",
    "        The replay buffer storing the experiences with their priorities.\n",
    "\n",
    "    Returns\n",
    "    -------\n",
    "    List[float]\n",
    "        A list of cumulated rewards per episode.\n",
    "    \"\"\"\n",
    "    episode_reward_list = []\n",
    "\n",
    "    for episode_index in tqdm(range(1, num_episodes)):\n",
    "        state, info = env.reset()\n",
    "        episode_reward = 0.0\n",
    "\n",
    "        for t in itertools.count():\n",
    "            # Get action, next_state and reward\n",
    "\n",
    "            action = epsilon_greedy(state)\n",
    "\n",
    "            next_state, reward, terminated, truncated, info = env.step(action)\n",
    "            done = terminated or truncated\n",
    "\n",
    "            replay_buffer.add(state, action, float(reward), next_state, done)\n",
    "\n",
    "            episode_reward += float(reward)\n",
    "\n",
    "            # Update the q_network weights with a batch of experiences from the buffer\n",
    "\n",
    "            if len(replay_buffer) > batch_size:\n",
    "                batch_states, batch_actions, batch_rewards, batch_next_states, batch_dones = replay_buffer.sample(batch_size)\n",
    "\n",
    "                # Convert to PyTorch tensors\n",
    "                batch_states_tensor = torch.tensor(batch_states, dtype=torch.float32, device=device)\n",
    "                batch_actions_tensor = torch.tensor(batch_actions, dtype=torch.long, device=device)\n",
    "                batch_rewards_tensor = torch.tensor(batch_rewards, dtype=torch.float32, device=device)\n",
    "                batch_next_states_tensor = torch.tensor(batch_next_states, dtype=torch.float32, device=device)\n",
    "                batch_dones_tensor = torch.tensor(batch_dones, dtype=torch.float32, device=device)\n",
    "\n",
    "                # Compute the target Q values for the batch\n",
    "                with torch.no_grad():\n",
    "                    # next_state_q_values, best_action_index = TODO... # Compute the maximum Q-values for the next states\n",
    "\n",
    "                    # targets = TODO... # Compute the target Q-values for the batch\n",
    "\n",
    "                # current_q_values = TODO... # Compute the Q-values for the current states\n",
    "\n",
    "                # loss = TODO... # Compute the loss\n",
    "\n",
    "                # Optimize the model\n",
    "                optimizer.zero_grad()\n",
    "                loss.backward()\n",
    "                optimizer.step()\n",
    "\n",
    "                lr_scheduler.step()\n",
    "\n",
    "            if done:\n",
    "                break\n",
    "\n",
    "            state = next_state\n",
    "\n",
    "        episode_reward_list.append(episode_reward)\n",
    "        epsilon_greedy.decay_epsilon()\n",
    "\n",
    "    return episode_reward_list"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Train it"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "env = gym.make(\"CartPole-v1\")\n",
    "\n",
    "NUMBER_OF_TRAININGS = DEFAULT_NUMBER_OF_TRAININGS  # Change the default (global) value here if you want a specific number of trainings for this exercise\n",
    "dqn1_trains_result_list: List[List[Union[int, float]]] = [[], [], []]\n",
    "\n",
    "for train_index in range(NUMBER_OF_TRAININGS):\n",
    "    # Instantiate required objects\n",
    "    \n",
    "    q_network = QNetwork(cartpole_observation_dim, cartpole_num_actions, nn_l1=128, nn_l2=128).to(device)\n",
    "    optimizer = torch.optim.AdamW(q_network.parameters(), lr=0.004, amsgrad=True)\n",
    "    # lr_scheduler = torch.optim.lr_scheduler.ExponentialLR(optimizer, gamma=0.999)\n",
    "    lr_scheduler = MinimumExponentialLR(optimizer, lr_decay=0.97, min_lr=0.0001)\n",
    "    loss_fn = torch.nn.MSELoss()\n",
    "\n",
    "    epsilon_greedy = EpsilonGreedy(\n",
    "        epsilon_start=0.82,\n",
    "        epsilon_min=0.013,\n",
    "        epsilon_decay=0.9675,\n",
    "        env=env,\n",
    "        q_network=q_network,\n",
    "    )\n",
    "\n",
    "    replay_buffer = ReplayBuffer(2000)\n",
    "\n",
    "    # Train the q-network\n",
    "\n",
    "    episode_reward_list = train_dqn1_agent(\n",
    "        env,\n",
    "        q_network,\n",
    "        optimizer,\n",
    "        loss_fn,\n",
    "        epsilon_greedy,\n",
    "        device,\n",
    "        lr_scheduler,\n",
    "        num_episodes=150,\n",
    "        gamma=0.9,\n",
    "        batch_size=128,\n",
    "        replay_buffer=replay_buffer,\n",
    "    )\n",
    "    dqn1_trains_result_list[0].extend(range(len(episode_reward_list)))\n",
    "    dqn1_trains_result_list[1].extend(episode_reward_list)\n",
    "    dqn1_trains_result_list[2].extend([train_index for _ in episode_reward_list])\n",
    "\n",
    "dqn1_trains_result_df = pd.DataFrame(\n",
    "    np.array(dqn1_trains_result_list).T,\n",
    "    columns=[\"num_episodes\", \"mean_final_episode_reward\", \"training_index\"],\n",
    ")\n",
    "dqn1_trains_result_df[\"agent\"] = \"DQN 2013\"\n",
    "\n",
    "# Save the action-value estimation function\n",
    "\n",
    "torch.save(q_network, MODELS_DIR / \"lab6_dqn1_q_network.pth\")\n",
    "\n",
    "env.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Plot results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "lines_to_next_cell": 0
   },
   "outputs": [],
   "source": [
    "g = sns.relplot(\n",
    "    x=\"num_episodes\",\n",
    "    y=\"mean_final_episode_reward\",\n",
    "    kind=\"line\",\n",
    "    hue=\"agent\",\n",
    "    estimator=None,\n",
    "    units=\"training_index\",\n",
    "    data=dqn1_trains_result_df,\n",
    "    height=7,\n",
    "    aspect=2,\n",
    "    alpha=0.5,\n",
    ")\n",
    "plt.savefig(PLOTS_DIR / \"lab6_dqn1_trains_result.png\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# g = sns.relplot(x=\"num_episodes\", y=\"mean_final_episode_reward\", kind=\"line\", hue=\"agent\", data=dqn1_trains_result_df, height=7, aspect=2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "lines_to_next_cell": 0
   },
   "outputs": [],
   "source": [
    "all_trains_result_df = pd.concat([naive_trains_result_df, dqn1_trains_result_df])\n",
    "g = sns.relplot(\n",
    "    x=\"num_episodes\",\n",
    "    y=\"mean_final_episode_reward\",\n",
    "    kind=\"line\",\n",
    "    hue=\"agent\",\n",
    "    data=all_trains_result_df,\n",
    "    height=7,\n",
    "    aspect=2,\n",
    ")\n",
    "plt.savefig(PLOTS_DIR / \"lab6_dqn1_trains_result_agg.png\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Test it"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "VIDEO_PREFIX_EX3_DQN1_TRAINED = \"lab6_ex3_dqn1_tained\"\n",
    "\n",
    "NUM_EPISODES = 3\n",
    "\n",
    "file_path_list = [\n",
    "    FIGS_DIR / f\"{VIDEO_PREFIX_EX3_DQN1_TRAINED}-episode-{episode_index}.mp4\"\n",
    "    for episode_index in range(NUM_EPISODES)\n",
    "]\n",
    "\n",
    "for file_path in file_path_list:\n",
    "    file_path.unlink(missing_ok=True)\n",
    "\n",
    "env = gym.make(\"CartPole-v1\", render_mode=\"rgb_array\")\n",
    "env = gym.wrappers.RecordVideo(\n",
    "    env,\n",
    "    video_folder=str(FIGS_DIR),\n",
    "    name_prefix=VIDEO_PREFIX_EX3_DQN1_TRAINED,\n",
    "    episode_trigger=lambda x: True,\n",
    ")\n",
    "env = gym.wrappers.RecordEpisodeStatistics(env, buffer_length=NUM_EPISODES)\n",
    "\n",
    "test_q_network_agent(env, q_network, num_episode=NUM_EPISODES)\n",
    "\n",
    "# print(f'Episode time taken: {env.time_queue}')\n",
    "# print(f'Episode total rewards: {env.return_queue}')\n",
    "# print(f'Episode lengths: {env.length_queue}')\n",
    "\n",
    "env.close()\n",
    "\n",
    "print(\"\\nSelect the episode to play here 👇\\n\")\n",
    "\n",
    "interact(video_selector, file_path=file_path_list)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "lines_to_next_cell": 2
   },
   "outputs": [],
   "source": [
    "score_ex3 = dqn1_trains_result_df[[\"num_episodes\", \"mean_final_episode_reward\"]].groupby(\"num_episodes\").mean().max()\n",
    "score_ex3"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Exercise 4: Implement Deep Q-Networks v2 (DQN version 2015) with *infrequent weight updates*\n",
    "\n",
    "In 2015, DeepMind further advanced the field of reinforcement learning with the publication of the paper \"Human-level control through deep reinforcement learning\" by Volodymyr Mnih and colleagues (https://www.nature.com/articles/nature14236). This work introduced the second version of Deep Q-Networks (DQN).\n",
    "\n",
    "<img src=\"https://raw.githubusercontent.com/jeremiedecock/polytechnique-csc-52081-ep-2025-students/main/assets/lab6_dqn_nature_journal.jpg\" width=\"200px\" />\n",
    "\n",
    "The key contribution of this paper was the introduction of a method to stabilize the learning process by infrequently updating the target weights. This technique, known as *infrequent updates of target weights*, significantly improved the stability of the learning process."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Infrequent weight updates\n",
    "\n",
    "Infrequent weight updates, also known as the use of a target network, is a technique used in Deep Q-Networks (DQN) to address the issue of learning from a moving target.\n",
    "\n",
    "In a typical DQN setup, there are two neural networks: the Q-network and the target network. The Q-network is used to predict the Q-values and is updated at every time step. The target network is used to compute the target Q-values for the update, and its weights are updated less frequently, typically every few thousand steps, by copying the weights from the Q-network.\n",
    "\n",
    "The idea behind infrequent weight updates is to stabilize the learning process by keeping the target Q-values fixed for a number of steps. This mitigates the issue of learning from a moving target, as the target Q-values remain fixed between updates.\n",
    "\n",
    "Without infrequent weight updates, both the predicted and target Q-values would change at every step, which could lead to oscillations and divergence in the learning process. By introducing a delay between updates of the target Q-values, the risk of such oscillations is reduced."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### DQN v2015 Algorithm"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Note: main differences with the previous algorithm are highlighted in red."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<b>Input</b>:<br>\n",
    "\t$\\quad\\quad$ none<br>\n",
    "<b>Algorithm parameter</b>:<br>\n",
    "\t$\\quad\\quad$ discount factor $\\gamma$<br>\n",
    "\t$\\quad\\quad$ step size $\\alpha \\in (0,1]$<br>\n",
    "\t$\\quad\\quad$ small $\\epsilon > 0$<br>\n",
    "\t$\\quad\\quad$ capacity of the experience replay memory $M$<br>\n",
    "\t$\\quad\\quad$ batch size $m$<br>\n",
    "\t$\\quad\\quad$ target network update frequency $\\color{red}{\\tau}$<br><br>\n",
    "\n",
    "<b>Initialize</b> replay memory $\\mathcal{D}$ to capacity $M$<br>\n",
    "<b>Initialize</b> action-value function $\\hat{Q}_{\\mathbf{\\omega_1}}$ with random weights $\\mathbf{\\omega_1}$<br>\n",
    "<b>Initialize</b> target action-value function $\\hat{Q}_{\\mathbf{\\omega_2}}$ with weights $\\color{red}{\\mathbf{\\omega_2} = \\mathbf{\\omega_1}}$<br><br>\n",
    "\n",
    "<b>FOR EACH</b> episode<br>\n",
    "\t$\\quad$ $\\mathbf{s} \\leftarrow \\text{env.reset}()$<br>\n",
    "\t$\\quad$ <b>DO</b> <br>\n",
    "\t\t$\\quad\\quad$ $\\mathbf{a} \\leftarrow \\epsilon\\text{-greedy}(\\mathbf{s}, \\hat{Q}_{\\mathbf{\\omega_1}})$<br>\n",
    "\t\t$\\quad\\quad$ $r, \\mathbf{s'} \\leftarrow \\text{env.step}(\\mathbf{a})$<br>\n",
    "\t\t$\\quad\\quad$ Store transition $(\\mathbf{s}, \\mathbf{a}, r, \\mathbf{s'})$ in $\\mathcal{D}$<br>\n",
    "\t\t$\\quad\\quad$ If $\\mathcal{D}$ contains \"enough\" transitions<br>\n",
    "\t\t\t$\\quad\\quad\\quad$ Sample random batch of transitions $(\\mathbf{s}_j, \\mathbf{a}_j, r_j, \\mathbf{s'}_j)$ from $\\mathcal{D}$ with $j=1$ to $m$<br>\n",
    "\t\t\t$\\quad\\quad\\quad$ For each $j$, set $y_j =\n",
    "\t\t\t\\begin{cases}\n",
    "\t\t\tr_j & \\text{for terminal } \\mathbf{s'}_j\\\\\n",
    "\t\t\tr_j + \\gamma \\max_{\\mathbf{a}^\\star} \\hat{Q}_{\\mathbf{\\omega_{\\color{red}{2}}}} (\\mathbf{s'}_j)_{\\mathbf{a}^\\star} & \\text{for non-terminal } \\mathbf{s'}_j\n",
    "\t\t\t\\end{cases}$<br>\n",
    "\t\t\t$\\quad\\quad\\quad$ Perform a gradient descent step on $\\left( y_j - \\hat{Q}_{\\mathbf{\\omega_1}}(\\mathbf{s}_j)_{\\mathbf{a}_j} \\right)^2$ with respect to the weights $\\mathbf{\\omega_1}$<br>\n",
    "\t\t\t$\\quad\\quad\\quad$ Every $\\color{red}{\\tau}$ steps reset $\\hat{Q}_{\\mathbf{\\omega_2}}$ to $\\hat{Q}_{\\mathbf{\\omega_1}}$, i.e., set $\\color{red}{\\mathbf{\\omega_2} \\leftarrow \\mathbf{\\omega_1}}$<br>\n",
    "\t\t$\\quad\\quad$ $\\mathbf{s} \\leftarrow \\mathbf{s'}$ <br>\n",
    "\t$\\quad$ <b>UNTIL</b> $\\mathbf{s}$ is final<br><br>\n",
    "<b>RETURN</b> $\\mathbf{\\omega_1}$ <br>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "lines_to_next_cell": 2
   },
   "source": [
    "#### Implement the training function\n",
    "\n",
    "To incorporate the use of infrequent weight updates in the training function, you would need to make the following modifications:\n",
    "\n",
    "1. **Update the Target Network Infrequently**: Instead of updating the weights of the target network at every time step, update them less frequently, for example, every few thousand steps. The weights of the target network are updated by copying the weights from the Q-network.\n",
    "\n",
    "2. **Compute Target Q-values with the Target Network**: When computing the target Q-values for the update, use the target network instead of the Q-network. This ensures that the target Q-values remain fixed between updates, which stabilizes the learning process."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Parctical tips\n",
    "\n",
    "To complete this exercise, you need to be familiar with the following PyTorch concepts.\n",
    "\n",
    "**Copying the weights and biases from one neural network to another**\n",
    "\n",
    "To transfer the parameters from `model1` to `model2`, use the following command:\n",
    "\n",
    "```python\n",
    "model2.load_state_dict(model1.state_dict())\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Task 4.1:** complete the `train_dqn2_agent` to apply infrequent weight updates."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_dqn2_agent(\n",
    "    env: gym.Env,\n",
    "    q_network: torch.nn.Module,\n",
    "    target_q_network: torch.nn.Module,\n",
    "    optimizer: torch.optim.Optimizer,\n",
    "    loss_fn: Callable,\n",
    "    epsilon_greedy: EpsilonGreedy,\n",
    "    device: torch.device,\n",
    "    lr_scheduler: torch.optim.lr_scheduler.LRScheduler,\n",
    "    num_episodes: int,\n",
    "    gamma: float,\n",
    "    batch_size: int,\n",
    "    replay_buffer: ReplayBuffer,\n",
    "    target_q_network_sync_period: int,\n",
    ") -> List[float]:\n",
    "    \"\"\"\n",
    "    Train the Q-network on the given environment.\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    env : gym.Env\n",
    "        The environment to train on.\n",
    "    q_network : torch.nn.Module\n",
    "        The Q-network to train.\n",
    "    target_q_network : torch.nn.Module\n",
    "        The target Q-network to use for estimating the target Q-values.\n",
    "    optimizer : torch.optim.Optimizer\n",
    "        The optimizer to use for training.\n",
    "    loss_fn : callable\n",
    "        The loss function to use for training.\n",
    "    epsilon_greedy : EpsilonGreedy\n",
    "        The epsilon-greedy policy to use for action selection.\n",
    "    device : torch.device\n",
    "        The device to use for PyTorch computations.\n",
    "    lr_scheduler : torch.optim.lr_scheduler.LRScheduler\n",
    "        The learning rate scheduler to adjust the learning rate during training.\n",
    "    num_episodes : int\n",
    "        The number of episodes to train for.\n",
    "    gamma : float\n",
    "        The discount factor for future rewards.\n",
    "    batch_size : int\n",
    "        The size of the batch to use for training.\n",
    "    replay_buffer : ReplayBuffer\n",
    "        The replay buffer storing the experiences with their priorities.\n",
    "    target_q_network_sync_period : int\n",
    "        The number of episodes after which the target Q-network should be updated with the weights of the Q-network.\n",
    "\n",
    "    Returns\n",
    "    -------\n",
    "    List[float]\n",
    "        A list of cumulated rewards per episode.\n",
    "    \"\"\"\n",
    "    iteration = 0\n",
    "    episode_reward_list = []\n",
    "\n",
    "    for episode_index in tqdm(range(1, num_episodes)):\n",
    "        state, info = env.reset()\n",
    "        episode_reward = 0.0\n",
    "\n",
    "        for t in itertools.count():\n",
    "            # Get action, next_state and reward\n",
    "\n",
    "            action = epsilon_greedy(state)\n",
    "\n",
    "            next_state, reward, terminated, truncated, info = env.step(action)\n",
    "            done = terminated or truncated\n",
    "\n",
    "            replay_buffer.add(state, action, float(reward), next_state, done)\n",
    "\n",
    "            episode_reward += float(reward)\n",
    "\n",
    "            # Update the q_network weights with a batch of experiences from the buffer\n",
    "\n",
    "            if len(replay_buffer) > batch_size:\n",
    "                batch_states, batch_actions, batch_rewards, batch_next_states, batch_dones = replay_buffer.sample(batch_size)\n",
    "\n",
    "                # Convert to PyTorch tensors\n",
    "                batch_states_tensor = torch.tensor(batch_states, dtype=torch.float32, device=device)\n",
    "                batch_actions_tensor = torch.tensor(batch_actions, dtype=torch.long, device=device)\n",
    "                batch_rewards_tensor = torch.tensor(batch_rewards, dtype=torch.float32, device=device)\n",
    "                batch_next_states_tensor = torch.tensor(batch_next_states, dtype=torch.float32, device=device)\n",
    "                batch_dones_tensor = torch.tensor(batch_dones, dtype=torch.float32, device=device)\n",
    "\n",
    "                # Compute the target Q values for the batch\n",
    "                with torch.no_grad():\n",
    "                    # next_state_q_values, best_action_index = TODO...\n",
    "                    \n",
    "                    # targets = TODO...\n",
    "\n",
    "                # current_q_values = TODO...\n",
    "\n",
    "                # Compute loss\n",
    "                loss = loss_fn(current_q_values, targets)\n",
    "\n",
    "                # Optimize the model\n",
    "                optimizer.zero_grad()\n",
    "                loss.backward()\n",
    "                optimizer.step()\n",
    "\n",
    "                lr_scheduler.step()\n",
    "\n",
    "            # Update the target q-network weights\n",
    "\n",
    "            # TODO:...  # Every episodes (e.g., every `target_q_network_sync_period` episodes), the weights of the target network are updated with the weights of the Q-network\n",
    "\n",
    "            iteration += 1\n",
    "\n",
    "            if done:\n",
    "                break\n",
    "\n",
    "            state = next_state\n",
    "\n",
    "        episode_reward_list.append(episode_reward)\n",
    "        epsilon_greedy.decay_epsilon()\n",
    "\n",
    "    return episode_reward_list"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Train it\n",
    "\n",
    "In order to test this new implementation, we needs de adapt the following cell to instantiate and initialize the two neural networks.\n",
    "\n",
    "**Task 4.2:** complete the following cell to make the two Q-Networks. Initialize a target network that has the same architecture as the Q-network. The weights of the target network are initially copied from the Q-network."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "env = gym.make(\"CartPole-v1\")\n",
    "\n",
    "NUMBER_OF_TRAININGS = DEFAULT_NUMBER_OF_TRAININGS  # Change the default (global) value here if you want a specific number of trainings for this exercise\n",
    "dqn2_trains_result_list: List[List[Union[int, float]]] = [[], [], []]\n",
    "\n",
    "for train_index in range(NUMBER_OF_TRAININGS):\n",
    "    # Instantiate required objects\n",
    "\n",
    "    # q_network = TODO...\n",
    "\n",
    "    # target_q_network = TODO... # The target Q-network is used to compute the target Q-values for the loss function\n",
    "\n",
    "    # TODO... # Initialize the target Q-network with the same weights as the Q-network (c.f. the \"Practical tips\" section of the exercise)\n",
    "\n",
    "    optimizer = torch.optim.AdamW(q_network.parameters(), lr=0.004, amsgrad=True)\n",
    "    # lr_scheduler = torch.optim.lr_scheduler.ExponentialLR(optimizer, gamma=0.999)\n",
    "    lr_scheduler = MinimumExponentialLR(optimizer, lr_decay=0.97, min_lr=0.0001)\n",
    "    loss_fn = torch.nn.MSELoss()\n",
    "\n",
    "    epsilon_greedy = EpsilonGreedy(\n",
    "        epsilon_start=0.82,\n",
    "        epsilon_min=0.013,\n",
    "        epsilon_decay=0.9675,\n",
    "        env=env,\n",
    "        q_network=q_network,\n",
    "    )\n",
    "\n",
    "    replay_buffer = ReplayBuffer(2000)\n",
    "\n",
    "    # Train the q-network\n",
    "\n",
    "    episode_reward_list = train_dqn2_agent(\n",
    "        env,\n",
    "        q_network,\n",
    "        target_q_network,\n",
    "        optimizer,\n",
    "        loss_fn,\n",
    "        epsilon_greedy,\n",
    "        device,\n",
    "        lr_scheduler,\n",
    "        num_episodes=150,\n",
    "        gamma=0.9,\n",
    "        batch_size=128,\n",
    "        replay_buffer=replay_buffer,\n",
    "        target_q_network_sync_period=30,\n",
    "    )\n",
    "    dqn2_trains_result_list[0].extend(range(len(episode_reward_list)))\n",
    "    dqn2_trains_result_list[1].extend(episode_reward_list)\n",
    "    dqn2_trains_result_list[2].extend([train_index for _ in episode_reward_list])\n",
    "\n",
    "dqn2_trains_result_df = pd.DataFrame(\n",
    "    np.array(dqn2_trains_result_list).T,\n",
    "    columns=[\"num_episodes\", \"mean_final_episode_reward\", \"training_index\"],\n",
    ")\n",
    "dqn2_trains_result_df[\"agent\"] = \"DQN 2015\"\n",
    "\n",
    "# Save the action-value estimation function\n",
    "\n",
    "torch.save(q_network, MODELS_DIR / \"lab6_dqn2_q_network.pth\")\n",
    "\n",
    "env.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Plot results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "lines_to_next_cell": 0
   },
   "outputs": [],
   "source": [
    "g = sns.relplot(\n",
    "    x=\"num_episodes\",\n",
    "    y=\"mean_final_episode_reward\",\n",
    "    kind=\"line\",\n",
    "    hue=\"agent\",\n",
    "    estimator=None,\n",
    "    units=\"training_index\",\n",
    "    data=dqn2_trains_result_df,\n",
    "    height=7,\n",
    "    aspect=2,\n",
    "    alpha=0.5,\n",
    ")\n",
    "plt.savefig(PLOTS_DIR / \"lab6_dqn2_trains_result.png\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# g = sns.relplot(x=\"num_episodes\", y=\"mean_final_episode_reward\", kind=\"line\", hue=\"agent\", data=dqn2_trains_result_df, height=7, aspect=2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "lines_to_next_cell": 0
   },
   "outputs": [],
   "source": [
    "all_trains_result_df = pd.concat(\n",
    "    [naive_trains_result_df, dqn1_trains_result_df, dqn2_trains_result_df]\n",
    ")\n",
    "g = sns.relplot(\n",
    "    x=\"num_episodes\",\n",
    "    y=\"mean_final_episode_reward\",\n",
    "    kind=\"line\",\n",
    "    hue=\"agent\",\n",
    "    data=all_trains_result_df,\n",
    "    height=7,\n",
    "    aspect=2,\n",
    ")\n",
    "plt.savefig(PLOTS_DIR / \"lab6_dqn2_trains_result_agg.png\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Test it"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "VIDEO_PREFIX_EX4_DQN2_TRAINED = \"lab6_ex4_dqn2_tained\"\n",
    "\n",
    "NUM_EPISODES = 3\n",
    "\n",
    "file_path_list = [\n",
    "    FIGS_DIR / f\"{VIDEO_PREFIX_EX4_DQN2_TRAINED}-episode-{episode_index}.mp4\"\n",
    "    for episode_index in range(NUM_EPISODES)\n",
    "]\n",
    "\n",
    "for file_path in file_path_list:\n",
    "    file_path.unlink(missing_ok=True)\n",
    "\n",
    "env = gym.make(\"CartPole-v1\", render_mode=\"rgb_array\")\n",
    "env = gym.wrappers.RecordVideo(\n",
    "    env,\n",
    "    video_folder=str(FIGS_DIR),\n",
    "    name_prefix=VIDEO_PREFIX_EX4_DQN2_TRAINED,\n",
    "    episode_trigger=lambda x: True,\n",
    ")\n",
    "env = gym.wrappers.RecordEpisodeStatistics(env, buffer_length=NUM_EPISODES)\n",
    "\n",
    "test_q_network_agent(env, q_network, num_episode=NUM_EPISODES)\n",
    "\n",
    "# print(f'Episode time taken: {env.time_queue}')\n",
    "# print(f'Episode total rewards: {env.return_queue}')\n",
    "# print(f'Episode lengths: {env.length_queue}')\n",
    "\n",
    "env.close()\n",
    "\n",
    "print(\"\\nSelect the episode to play here 👇\\n\")\n",
    "\n",
    "interact(video_selector, file_path=file_path_list)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "lines_to_next_cell": 2
   },
   "outputs": [],
   "source": [
    "score_ex4 = dqn2_trains_result_df[[\"num_episodes\", \"mean_final_episode_reward\"]].groupby(\"num_episodes\").mean().max()\n",
    "score_ex4"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Bonus 1: Deep policy-based reinforcement learning with Monte Carlo Policy Gradient (REINFORCE)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In the previous part, we explored DQN, a value-based method that can be used to solve Reinforcement Learning problems having large state spaces.\n",
    "Now, we will solve the CartPole environment using a policy gradient method which directly searchs in a family of parameterized policies $\\pi_\\theta$ for the optimal policy."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### The Policy Gradient theorem"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": []
   },
   "source": [
    "We will solve the CartPole environment using a policy gradient method which directly searchs in a family of parameterized policies $\\pi_\\theta$ for the optimal policy.\n",
    "\n",
    "This method performs gradient ascent in the policy space so that the total return is maximized.\n",
    "We will restrict our work to episodic tasks, *i.e.* tasks that have a starting states and last for a finite and fixed number of steps $T$, called horizon.\n",
    "\n",
    "More formally, we define an optimization criterion that we want to maximize:\n",
    "\n",
    "$$J(\\theta) = \\mathbb{E}_{\\pi_\\theta}\\left[\\sum_{t=1}^T r(s_t,a_t)\\right],$$\n",
    "\n",
    "where $\\mathbb{E}_{\\pi_\\theta}$ means $a \\sim \\pi_\\theta(\\cdot|s)$ and $T$ is the horizon of the episode.\n",
    "In other words, we want to maximize the value of the starting state: $V^{\\pi_\\theta}(s)$.\n",
    "The policy gradient theorem tells us that:\n",
    "\n",
    "$$\n",
    "\\nabla_\\theta J(\\theta) = \\nabla_\\theta V^{\\pi_\\theta}(s) = \\mathbb{E}_{\\pi_\\theta} \\left[\\nabla_\\theta \\log \\pi_\\theta (a|s) ~ Q^{\\pi_\\theta}(s,a) \\right],\n",
    "$$\n",
    "\n",
    "where the $Q$-function is defined as:\n",
    "\n",
    "$$Q^{\\pi_\\theta}(a|s) = \\mathbb{E}^{\\pi_\\theta} \\left[\\sum_{t=1}^T r(s_t,a_t)|s=s_1, a=a_1\\right].$$\n",
    "\n",
    "The policy gradient theorem is particularly effective because it allows gradient computation without needing to understand the system's dynamics, as long as the $Q$-function for the current policy is computable. By simply applying the policy and observing the one-step transitions, sufficient information is gathered. Implementing a stochastic gradient ascent and substituting $Q^{\\pi_\\theta}(s_t,a_t)$ with a Monte Carlo estimate $R_t = \\sum_{t'=t}^T r(s_{t'},a_{t'})$ for a single trajectory, we derive the REINFORCE algorithm."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The REINFORCE algorithm, introduced by Williams in 1992, is a Monte Carlo policy gradient method. It updates the policy in the direction that maximizes rewards, using full-episode returns as an unbiased estimate of the gradient. Each step involves generating an episode using the current policy, computing the gradient estimate, and updating the policy parameters. This algorithm is simple yet powerful, and it's particularly effective in environments where the policy gradient is noisy or the dynamics are complex.\n",
    "\n",
    "For further reading and a deeper understanding, refer to Williams' seminal paper (https://link.springer.com/article/10.1007/BF00992696) and the comprehensive text on reinforcement learning by Richard S. Sutton and Andrew G. Barto: \"Reinforcement Learning: An Introduction\", chap.13 (http://incompleteideas.net/book/RLbook2020.pdf)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here is the REINFORCE algorithm."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Monte Carlo policy gradient (REINFORCE)\n",
    "\n",
    "<b>REQUIRE</b> <br>\n",
    "$\\quad$ A differentiable policy $\\pi_{\\boldsymbol{\\theta}}$ <br>\n",
    "$\\quad$ A learning rate $\\alpha \\in \\mathbb{R}^+$ <br>\n",
    "<b>INITIALIZATION</b> <br>\n",
    "$\\quad$ Initialize parameters $\\boldsymbol{\\theta} \\in \\mathbb{R}^d$ <br>\n",
    "<br>\n",
    "<b>FOR EACH</b> episode <br>\n",
    "$\\quad$ Generate full trace $\\tau = \\{ \\boldsymbol{s}_0, \\boldsymbol{a}_0, r_1, \\boldsymbol{s}_1, \\boldsymbol{a}_1, \\dots, r_T, \\boldsymbol{s}_T \\}$ following $\\pi_{\\boldsymbol{\\theta}}$ <br>\n",
    "$\\quad$ <b>FOR</b> $~ t=0,\\dots,T-1$ <br>\n",
    "$\\quad\\quad$ $G \\leftarrow \\sum_{k=t}^{T-1} r_k$ <br>\n",
    "$\\quad\\quad$ $\\boldsymbol{\\theta} \\leftarrow \\boldsymbol{\\theta} + \\alpha ~ \\underbrace{G ~ \\nabla_{\\boldsymbol{\\theta}} \\ln \\pi_{\\boldsymbol{\\theta}}(\\boldsymbol{a}_t|\\boldsymbol{s}_t)}_{\\nabla_{\\boldsymbol{\\theta}} J(\\boldsymbol{\\theta})}$ <br>\n",
    "<br>\n",
    "<b>RETURN</b> $\\boldsymbol{\\theta}$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Exercise 5: REINFORCE for discrete action spaces (Cartpole)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Policy Implementation\n",
    "\n",
    "We will implement a stochastic policy to control the cart using a simple one-layer neural network. Given the simplicity of the problem, a single layer will suffice. We will not incorporate a bias term in this layer.\n",
    "\n",
    "This neural network will output the probabilities of each possible action (in this case, there are only two actions: \"push left\" or \"push right\") given the input vector $s$ (the 4-dimensional state vector)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Task 5.1**: Implement the `PolicyNetwork`  defined as follow."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "lines_to_next_cell": 2
   },
   "source": [
    "The network takes an input tensor representing the state of the environment and outputs a tensor of action probabilities.\n",
    "The network has the following components:\n",
    "\n",
    "- `layer1`: This is a linear (fully connected) layer that takes `n_observations` as input and outputs `n_actions`. It does not include a bias term.\n",
    "\n",
    "- `forward` method: This method defines the forward pass of the network. It takes a state tensor as input and returns a tensor of action probabilities. It first applies the linear layer to the input state tensor to get the logits (the raw, unnormalized scores for each action), and then applies the softmax function to the logits to get the action probabilities. The softmax function ensures that the action probabilities are positive and sum to 1, so they can be interpreted as probabilities.\n",
    "\n",
    "This network is quite simple and may not perform well on complex tasks with large state or action spaces. However, it can be a good starting point for simple reinforcement learning tasks, and can be easily extended with more layers or different types of layers (such as convolutional layers for image inputs) to handle more complex tasks."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "lines_to_next_cell": 2
   },
   "outputs": [],
   "source": [
    "class PolicyNetwork(torch.nn.Module):\n",
    "    \"\"\"\n",
    "    A neural network used as a policy for the REINFORCE algorithm.\n",
    "\n",
    "    Attributes\n",
    "    ----------\n",
    "    layer1 : torch.nn.Linear\n",
    "        A fully connected layer.\n",
    "\n",
    "    Methods\n",
    "    -------\n",
    "    forward(state: torch.Tensor) -> torch.Tensor\n",
    "        Define the forward pass of the PolicyNetwork.\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, n_observations: int, n_actions: int):\n",
    "        \"\"\"\n",
    "        Initialize a new instance of PolicyNetwork.\n",
    "\n",
    "        Parameters\n",
    "        ----------\n",
    "        n_observations : int\n",
    "            The size of the observation space.\n",
    "        n_actions : int\n",
    "            The size of the action space.\n",
    "        \"\"\"\n",
    "        super(PolicyNetwork, self).__init__()\n",
    "\n",
    "        # TODO...\n",
    "\n",
    "    def forward(self, state_tensor: torch.Tensor) -> torch.Tensor:\n",
    "        \"\"\"\n",
    "        Calculate the probability of each action for the given state.\n",
    "\n",
    "        Parameters\n",
    "        ----------\n",
    "        state_tensor : torch.Tensor\n",
    "            The input tensor (state).\n",
    "            The shape of the tensor should be (N, dim),\n",
    "            where N is the number of states vectors in the batch\n",
    "            and dim is the dimension of state vectors.\n",
    "\n",
    "        Returns\n",
    "        -------\n",
    "        torch.Tensor\n",
    "            The output tensor (the probability of each action for the given state).\n",
    "        \"\"\"\n",
    "\n",
    "        # TODO...\n",
    "\n",
    "        return out"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "lines_to_next_cell": 2
   },
   "source": [
    "**Task 5.2**: Complete the `sample_discrete_action` function. This function is used to sample a discrete action based on a given state and a policy network. It first converts the state into a tensor and passes it through the policy network to get the parameters of the action probability distribution. Then, it creates a categorical distribution from these parameters and samples an action from this distribution. It also calculates the log probability of the sampled action according to the distribution. The function returns the sampled action and its log probability."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def sample_discrete_action(\n",
    "    policy_nn: PolicyNetwork, state: np.ndarray\n",
    ") -> Tuple[int, torch.Tensor]:\n",
    "    \"\"\"\n",
    "    Sample a discrete action based on the given state and policy network.\n",
    "\n",
    "    This function takes a state and a policy network, and returns a sampled action and its log probability.\n",
    "    The action is sampled from a categorical distribution defined by the output of the policy network.\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    policy_nn : PolicyNetwork\n",
    "        The policy network that defines the probability distribution of the actions.\n",
    "    state : np.ndarray\n",
    "        The state based on which an action needs to be sampled.\n",
    "\n",
    "    Returns\n",
    "    -------\n",
    "    Tuple[int, torch.Tensor]\n",
    "        The sampled action and its log probability.\n",
    "\n",
    "    \"\"\"\n",
    "\n",
    "    # Convert the state into a tensor, specify its data type as float32, and send it to the device (CPU or GPU).\n",
    "    # The unsqueeze(0) function is used to add an extra dimension to the tensor to match the input shape required by the policy network.\n",
    "    # state_tensor = TODO...\n",
    "\n",
    "    # Pass the state tensor through the policy network to get the parameters of the action probability distribution.\n",
    "    # actions_probability_distribution_params = TODO...\n",
    "\n",
    "    # Create the categorical distribution used to sample an action from the parameters obtained from the policy network.\n",
    "    # See https://pytorch.org/docs/stable/distributions.html#categorical\n",
    "    # actions_probability_distribution = TODO...\n",
    "\n",
    "    # Sample an action from the categorical distribution.\n",
    "    # sampled_action_tensor = TODO...\n",
    "\n",
    "    # Convert the tensor containing the sampled action into a Python integer.\n",
    "    # sampled_action = TODO...\n",
    "\n",
    "    # Calculate the log probability of the sampled action according to the categorical distribution.\n",
    "    # sampled_action_log_probability = TODO...\n",
    "\n",
    "    # Return the sampled action and its log probability.\n",
    "    return sampled_action, sampled_action_log_probability"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Task 5.3**: Test the `sample_discrete_action` function on a random state using an untrained policy network."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "lines_to_next_cell": 2
   },
   "outputs": [],
   "source": [
    "env = gym.make(\"CartPole-v1\")\n",
    "\n",
    "# policy_nn = TODO...\n",
    "\n",
    "# state = TODO...\n",
    "# theta = TODO...\n",
    "# action, action_log_probability = TODO...\n",
    "\n",
    "print(\"state:\", state)\n",
    "print(\"theta:\", theta)\n",
    "print(\"sampled action:\", action)\n",
    "print(\"log probability of the sampled action:\", action_log_probability)\n",
    "\n",
    "env.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Implement the sample_one_episode function"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "lines_to_next_cell": 2
   },
   "source": [
    "Remember that in the REINFORCE algorithm, we need to generate a complete trajectory, denoted as $\\tau = \\{ \\boldsymbol{s}_0, \\boldsymbol{a}_0, r_1, \\boldsymbol{s}_1, \\boldsymbol{a}_1, \\dots, r_T, \\boldsymbol{s}_T \\}$. This trajectory includes the states, actions, and rewards at each time step, as outlined in the algorithm at the beginning of Part 1.\n",
    "\n",
    "**Task 5.4**: Your task is to implement the `sample_one_episode` function. This function should play one episode using the given policy $\\pi_\\theta$ and return its rollouts. The function should adhere to a fixed horizon $T$, which represents the maximum number of steps in the episode."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def sample_one_episode(\n",
    "    env: gym.Env, policy_nn: PolicyNetwork, max_episode_duration: int\n",
    ") -> Tuple[List[np.ndarray], List[int], List[float], List[torch.Tensor]]:\n",
    "    \"\"\"\n",
    "    Execute one episode within the `env` environment utilizing the policy defined by the `policy_nn` parameter.\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    env : gym.Env\n",
    "        The environment to play in.\n",
    "    policy_nn : PolicyNetwork\n",
    "        The policy neural network.\n",
    "    max_episode_duration : int\n",
    "        The maximum duration of the episode.\n",
    "\n",
    "    Returns\n",
    "    -------\n",
    "    Tuple[List[np.ndarray], List[int], List[float], List[torch.Tensor]]\n",
    "        The states, actions, rewards, and log probability of action for each time step in the episode.\n",
    "    \"\"\"\n",
    "    state_t, info = env.reset()\n",
    "\n",
    "    episode_states = []\n",
    "    episode_actions = []\n",
    "    episode_log_prob_actions = []\n",
    "    episode_rewards = []\n",
    "    episode_states.append(state_t)\n",
    "\n",
    "    for t in range(max_episode_duration):\n",
    "\n",
    "        # Sample a discrete action and its log probability from the policy network based on the current state\n",
    "        # action_t, log_prob_action_t = TODO...\n",
    "\n",
    "        # Execute the sampled action in the environment, which returns the new state, reward, and whether the episode has terminated or been truncated\n",
    "        # state_t, reward_t, terminated, truncated, info = TODO...\n",
    "\n",
    "        # Check if the episode is done, either due to termination (reaching a terminal state) or truncation (reaching a maximum number of steps)\n",
    "        done = terminated or truncated\n",
    "\n",
    "        # Append the new state, action, action log probability and reward to their respective lists\n",
    "\n",
    "        episode_states.append(state_t)\n",
    "        episode_actions.append(action_t)\n",
    "        episode_log_prob_actions.append(log_prob_action_t)\n",
    "        episode_rewards.append(float(reward_t))\n",
    "\n",
    "        if done:\n",
    "            break\n",
    "\n",
    "    return episode_states, episode_actions, episode_rewards, episode_log_prob_actions"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Task 5.5:** Test this function on the untrained agent."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "VIDEO_PREFIX_EX5_REINFORCE_UNTRAINED = \"lab6_ex5_reinforce_untained\"\n",
    "\n",
    "NUM_EPISODES = 3\n",
    "\n",
    "file_path_list = [\n",
    "    FIGS_DIR / f\"{VIDEO_PREFIX_EX5_REINFORCE_UNTRAINED}-episode-{episode_index}.mp4\"\n",
    "    for episode_index in range(NUM_EPISODES)\n",
    "]\n",
    "\n",
    "for file_path in file_path_list:\n",
    "    file_path.unlink(missing_ok=True)\n",
    "\n",
    "env = gym.make(\"CartPole-v1\", render_mode=\"rgb_array\")\n",
    "env = gym.wrappers.RecordVideo(\n",
    "    env,\n",
    "    video_folder=str(FIGS_DIR),\n",
    "    name_prefix=VIDEO_PREFIX_EX5_REINFORCE_UNTRAINED,\n",
    "    episode_trigger=lambda x: True,\n",
    ")\n",
    "env = gym.wrappers.RecordEpisodeStatistics(env, buffer_length=NUM_EPISODES)\n",
    "\n",
    "for episode_index in range(NUM_EPISODES):\n",
    "    # policy_nn = TODO...\n",
    "    # episode_states, episode_actions, episode_rewards, episode_log_prob_actions = TODO...\n",
    "\n",
    "print(f\"Episode time taken: {env.time_queue}\")\n",
    "print(f\"Episode total rewards: {env.return_queue}\")\n",
    "print(f\"Episode lengths: {env.length_queue}\")\n",
    "\n",
    "env.close()\n",
    "\n",
    "print(\"\\nSelect the episode to play here 👇\\n\")\n",
    "\n",
    "interact(video_selector, file_path=file_path_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "episode_states"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "episode_actions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "lines_to_next_cell": 2
   },
   "outputs": [],
   "source": [
    "episode_rewards"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Implement a test function"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "lines_to_next_cell": 2
   },
   "source": [
    "**Task 5.6**: Implement the `avg_return_on_multiple_episodes` function that test the given policy $\\pi_\\theta$ on `num_episodes` episodes (for fixed horizon $T$) and returns the average reward on the `num_episodes` episodes.\n",
    "\n",
    "The function `avg_return_on_multiple_episodes` is designed to play multiple episodes of a given environment using a specified policy neural network and calculate the average return. It takes as input the environment to play in, the policy neural network to use, the number of episodes to play, the maximum duration of an episode, and an optional parameter to decide whether to render the environment or not.\n",
    "In each episode, it uses the `sample_one_episode` function to play the episode and collect the rewards. The function then returns the average of these cumulated rewards.\n",
    "\n",
    "`avg_return_on_multiple_episodes` will be used for evaluating the performance of a policy over multiple episodes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def avg_return_on_multiple_episodes(\n",
    "    env: gym.Env,\n",
    "    policy_nn: PolicyNetwork,\n",
    "    num_test_episode: int,\n",
    "    max_episode_duration: int,\n",
    ") -> float:\n",
    "    \"\"\"\n",
    "    Play multiple episodes of the environment and calculate the average return.\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    env : gym.Env\n",
    "        The environment to play in.\n",
    "    policy_nn : PolicyNetwork\n",
    "        The policy neural network.\n",
    "    num_test_episode : int\n",
    "        The number of episodes to play.\n",
    "    max_episode_duration : int\n",
    "        The maximum duration of an episode.\n",
    "\n",
    "    Returns\n",
    "    -------\n",
    "    float\n",
    "        The average return.\n",
    "    \"\"\"\n",
    "\n",
    "    # TODO...\n",
    "\n",
    "    return average_return"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Task 5.7:** Test this function on the untrained agent."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "lines_to_next_cell": 2
   },
   "outputs": [],
   "source": [
    "env = gym.make(\"CartPole-v1\")\n",
    "\n",
    "# TODO...\n",
    "\n",
    "print(average_return)\n",
    "\n",
    "env.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Implement the train function"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "lines_to_next_cell": 2
   },
   "source": [
    "**Task 5.8**: Implement the `train_reinforce_discrete` function, used to train a policy network using the REINFORCE algorithm in the given environment. This function takes as input the environment, the number of training episodes, the number of tests to perform per episode, the maximum duration of an episode, and the learning rate for the optimizer.\n",
    "\n",
    "The function first initializes a policy network and an Adam optimizer. Then, for each training episode, it generates an episode using the current policy and calculates the return at each time step. It uses this return and the log probability of the action taken at that time step to compute the loss, which is the negative of the product of the return and the log probability. This loss is used to update the policy network parameters using gradient ascent.\n",
    "\n",
    "After each training episode, the function tests the current policy by playing a number of test episodes and calculating the average return. This average return is added to a list for monitoring purposes.\n",
    "\n",
    "The function returns the trained policy network and the list of average returns for each episode. This function encapsulates the main loop of the REINFORCE algorithm, including the policy update step. Please refer back to the algorithm outlined at the start of Part 3 for additional context, if necessary."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_reinforce_discrete(\n",
    "    env: gym.Env,\n",
    "    num_train_episodes: int,\n",
    "    num_test_per_episode: int,\n",
    "    max_episode_duration: int,\n",
    "    learning_rate: float,\n",
    ") -> Tuple[PolicyNetwork, List[float]]:\n",
    "    \"\"\"\n",
    "    Train a policy using the REINFORCE algorithm.\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    env : gym.Env\n",
    "        The environment to train in.\n",
    "    num_train_episodes : int\n",
    "        The number of training episodes.\n",
    "    num_test_per_episode : int\n",
    "        The number of tests to perform per episode.\n",
    "    max_episode_duration : int\n",
    "        The maximum length of an episode, by default EPISODE_DURATION.\n",
    "    learning_rate : float\n",
    "        The initial step size.\n",
    "\n",
    "    Returns\n",
    "    -------\n",
    "    Tuple[PolicyNetwork, List[float]]\n",
    "        The final trained policy and the average returns for each episode.\n",
    "    \"\"\"\n",
    "    episode_avg_return_list = []\n",
    "\n",
    "    policy_nn = PolicyNetwork(cartpole_observation_dim, cartpole_num_actions).to(device)\n",
    "    optimizer = torch.optim.Adam(policy_nn.parameters(), lr=learning_rate)\n",
    "\n",
    "    for episode_index in tqdm(range(num_train_episodes)):\n",
    "        # TODO...\n",
    "\n",
    "        # Test the current policy\n",
    "        test_avg_return = avg_return_on_multiple_episodes(\n",
    "            env=env,\n",
    "            policy_nn=policy_nn,\n",
    "            num_test_episode=num_test_per_episode,\n",
    "            max_episode_duration=max_episode_duration,\n",
    "        )\n",
    "\n",
    "        # Monitoring\n",
    "        episode_avg_return_list.append(test_avg_return)\n",
    "\n",
    "    return policy_nn, episode_avg_return_list"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Train the agent"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "env = gym.make(\"CartPole-v1\")\n",
    "\n",
    "NUMBER_OF_TRAININGS = DEFAULT_NUMBER_OF_TRAININGS  # Change the default (global) value here if you want a specific number of trainings for this exercise\n",
    "reinforce_trains_result_list: List[List[Union[int, float]]] = [[], [], []]\n",
    "\n",
    "for train_index in range(NUMBER_OF_TRAININGS):\n",
    "    # Train the agent\n",
    "    reinforce_policy_nn, episode_reward_list = train_reinforce_discrete(\n",
    "        env=env,\n",
    "        num_train_episodes=150,\n",
    "        num_test_per_episode=5,\n",
    "        max_episode_duration=500,\n",
    "        learning_rate=0.01,\n",
    "    )\n",
    "\n",
    "    reinforce_trains_result_list[0].extend(range(len(episode_reward_list)))\n",
    "    reinforce_trains_result_list[1].extend(episode_reward_list)\n",
    "    reinforce_trains_result_list[2].extend([train_index for _ in episode_reward_list])\n",
    "\n",
    "reinforce_trains_result_df = pd.DataFrame(\n",
    "    np.array(reinforce_trains_result_list).T,\n",
    "    columns=[\"num_episodes\", \"mean_final_episode_reward\", \"training_index\"],\n",
    ")\n",
    "reinforce_trains_result_df[\"agent\"] = \"REINFORCE\"\n",
    "\n",
    "# Save the action-value estimation function of the last train\n",
    "\n",
    "torch.save(reinforce_policy_nn, MODELS_DIR / \"lab6_reinforce_policy_network.pth\")\n",
    "\n",
    "env.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Plot results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "g = sns.relplot(\n",
    "    x=\"num_episodes\",\n",
    "    y=\"mean_final_episode_reward\",\n",
    "    kind=\"line\",\n",
    "    hue=\"agent\",\n",
    "    estimator=None,\n",
    "    units=\"training_index\",\n",
    "    data=reinforce_trains_result_df,\n",
    "    height=7,\n",
    "    aspect=2,\n",
    "    alpha=0.5,\n",
    ")\n",
    "plt.savefig(PLOTS_DIR / \"lab6_reinforce_cartpole_trains_result.png\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "lines_to_next_cell": 0
   },
   "outputs": [],
   "source": [
    "all_trains_result_df = pd.concat(\n",
    "    [\n",
    "        naive_trains_result_df,\n",
    "        dqn1_trains_result_df,\n",
    "        dqn2_trains_result_df,\n",
    "        reinforce_trains_result_df,\n",
    "    ]\n",
    ")\n",
    "g = sns.relplot(\n",
    "    x=\"num_episodes\",\n",
    "    y=\"mean_final_episode_reward\",\n",
    "    kind=\"line\",\n",
    "    hue=\"agent\",\n",
    "    data=all_trains_result_df,\n",
    "    height=7,\n",
    "    aspect=2,\n",
    ")\n",
    "plt.savefig(PLOTS_DIR / \"lab6_reinforce_cartpole_trains_result_agg.png\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Test final policy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "VIDEO_PREFIX_EX5_REINFORCE_TRAINED = \"lab6_ex5_reinforce_tained\"\n",
    "\n",
    "NUM_EPISODES = 3\n",
    "\n",
    "file_path_list = [\n",
    "    FIGS_DIR / f\"{VIDEO_PREFIX_EX5_REINFORCE_TRAINED}-episode-{episode_index}.mp4\"\n",
    "    for episode_index in range(NUM_EPISODES)\n",
    "]\n",
    "\n",
    "for file_path in file_path_list:\n",
    "    file_path.unlink(missing_ok=True)\n",
    "\n",
    "env = gym.make(\"CartPole-v1\", render_mode=\"rgb_array\")\n",
    "env = gym.wrappers.RecordVideo(\n",
    "    env,\n",
    "    video_folder=str(FIGS_DIR),\n",
    "    name_prefix=VIDEO_PREFIX_EX5_REINFORCE_TRAINED,\n",
    "    episode_trigger=lambda x: True,\n",
    ")\n",
    "env = gym.wrappers.RecordEpisodeStatistics(env, buffer_length=NUM_EPISODES)\n",
    "\n",
    "# TODO...\n",
    "\n",
    "print(f\"Episode time taken: {env.time_queue}\")\n",
    "print(f\"Episode total rewards: {env.return_queue}\")\n",
    "print(f\"Episode lengths: {env.length_queue}\")\n",
    "\n",
    "env.close()\n",
    "\n",
    "print(\"\\nSelect the episode to play here 👇\\n\")\n",
    "\n",
    "interact(video_selector, file_path=file_path_list)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Task 5.9**: decrease the learning rate value (e.g. 0.001), increase the number of episodes per training and retrain the agent. What do you observe ?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "reinforce_trains_result_df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "score_ex5 = reinforce_trains_result_df[[\"num_episodes\", \"mean_final_episode_reward\"]].groupby(\"num_episodes\").mean().max()\n",
    "score_ex5"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Bonus 2: Hyperparameters optimization with Optuna\n",
    "\n",
    "Optuna is an open-source hyperparameter optimization framework designed to automate the process of searching for the best hyperparameters in machine learning models. It is highly efficient and flexible, supporting various optimization algorithms. Optuna works with Python-based machine learning libraries like PyTorch, TensorFlow, and Scikit-learn. Optuna’s core feature is its ability to perform dynamic search spaces and pruning, allowing faster convergence by terminating poorly performing trials early.\n",
    "Optuna supports distributed optimization for large-scale tuning.\n",
    "\n",
    "### Installation\n",
    "\n",
    "```\n",
    "pip install optuna\n",
    "```\n",
    "\n",
    "### Official documentation\n",
    "\n",
    "- Optuna GitHub: [https://github.com/optuna/optuna](https://github.com/optuna/optuna)\n",
    "- Optuna Documentation: [https://optuna.org](https://optuna.org)\n",
    "\n",
    "### Example of usage with PyTorch\n",
    "\n",
    "Here's an example of how to use Optuna to optimize the hyperparameters of a simple neural network with PyTorch."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import optuna\n",
    "from torch.utils.data import DataLoader, TensorDataset\n",
    "\n",
    "# Define the PyTorch model\n",
    "class Net(nn.Module):\n",
    "    def __init__(self, input_size, hidden_size, output_size):\n",
    "        super(Net, self).__init__()\n",
    "        self.fc1 = nn.Linear(input_size, hidden_size)\n",
    "        self.fc2 = nn.Linear(hidden_size, output_size)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = torch.relu(self.fc1(x))\n",
    "        x = self.fc2(x)\n",
    "        return x\n",
    "\n",
    "# Objective function for Optuna\n",
    "def objective(trial):\n",
    "    # Hyperparameters to be tuned\n",
    "    hidden_size = trial.suggest_int('hidden_size', 32, 128)\n",
    "    lr = trial.suggest_loguniform('lr', 1e-4, 1e-2)\n",
    "\n",
    "    model = Net(input_size=28*28, hidden_size=hidden_size, output_size=10)\n",
    "    optimizer = optim.Adam(model.parameters(), lr=lr)\n",
    "    loss_fn = nn.CrossEntropyLoss()\n",
    "\n",
    "    # Dummy dataset\n",
    "    X = torch.randn(100, 28*28)\n",
    "    y = torch.randint(0, 10, (100,))\n",
    "    train_loader = DataLoader(TensorDataset(X, y), batch_size=32)\n",
    "\n",
    "    # Training loop\n",
    "    for epoch in range(10):\n",
    "        for batch_x, batch_y in train_loader:\n",
    "            optimizer.zero_grad()\n",
    "            output = model(batch_x)\n",
    "            loss = loss_fn(output, batch_y)\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "\n",
    "    return loss.item()\n",
    "\n",
    "# Optimize hyperparameters\n",
    "study = optuna.create_study(direction='minimize')\n",
    "study.optimize(objective, n_trials=10)\n",
    "\n",
    "# Show best hyperparameters\n",
    "print(study.best_trial)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This example creates a basic neural network and tunes the `hidden_size` and learning rate (`lr`) using Optuna."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
