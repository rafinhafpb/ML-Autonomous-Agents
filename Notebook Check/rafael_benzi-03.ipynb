{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "3c1b9f6e",
   "metadata": {
    "cell_marker": "\"\"\"",
    "lines_to_next_cell": 2
   },
   "source": [
    "# CSC_52081_EP - Lab 03\n",
    "\n",
    "### Main Objectives \n",
    "\n",
    "Today we will study bandits in the context of sequential decision making, and the exploration-exploitation tradeoff. You may find the required background in the lecture slides, lecture notes, and the references provided within. \n",
    " \n",
    "### Instructions\n",
    "\n",
    "Work your way through the notebook, and provide code where indicated by `# TODO` to complete the tasks. Check Moodle for details on how to submit your work."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "291048ed",
   "metadata": {
    "cell_marker": "\"\"\""
   },
   "source": [
    "### Imports\n",
    "\n",
    "First, we're going to import `numpy` and some utility functions/classes that we will use. Make sure the `bandit_machines.py` is in your working directory. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fe773891",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import scipy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "773cc6bb",
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7a7aa1c3",
   "metadata": {
    "lines_to_next_cell": 1
   },
   "outputs": [],
   "source": [
    "from bandit_machines import BernoulliMAB, GaussianMAB"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bd54d4bf",
   "metadata": {
    "lines_to_next_cell": 1
   },
   "outputs": [],
   "source": [
    "def randmax(a):\n",
    "    \"\"\" return a random maximum \"\"\"\n",
    "    a = np.array(a)  \n",
    "    max_indices = np.flatnonzero(a == a.max())\n",
    "    return np.random.choice(max_indices)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b6301d28",
   "metadata": {
    "cell_marker": "\"\"\""
   },
   "source": [
    "### Creating a bandit environment. \n",
    "\n",
    "What do we mean by a bandit environment or bandit machine? It's a kind of state-less enviornment. When generates from a particular generation, depending on which arm (or, action) is pulled (or, taken). "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8e5c6233",
   "metadata": {},
   "outputs": [],
   "source": [
    "n_arms = 2\n",
    "env_B = BernoulliMAB(n_arms=2,labels=[\"Arm 1\",\"Arm 2\"],means=[0.5,0.7])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "26d48144",
   "metadata": {
    "cell_marker": "\"\"\"",
    "lines_to_next_cell": 1
   },
   "source": [
    "### Evaluating a bandit learner\n",
    "\n",
    "The function given below executes one bandit algorithm on one multi-armed-bandit (MAB) instance. We will use this to compare bandit algorithms. Note that we compare them on a basis of **cumulative regret**. In this lab we are mainly interested in an empirical analysis of bandits. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c6e8d906",
   "metadata": {
    "lines_to_next_cell": 1
   },
   "outputs": [],
   "source": [
    "def evaluate_run(env, pi, T=1000):\n",
    "    '''\n",
    "    Run a bandit agent on a bandit instance (environment) for T steps.\n",
    "    '''\n",
    "    r_log = []\n",
    "    a_log = []\n",
    "    pi.clear()\n",
    "    for t in range(T):\n",
    "        a = pi.act()\n",
    "        r = env.rwd(a)\n",
    "        pi.update(a,r)\n",
    "        r_log.append(r)\n",
    "        a_log.append(a)\n",
    "    return a_log, r_log"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "29c313e5",
   "metadata": {},
   "outputs": [],
   "source": [
    "def cumulative_regret(env, selections):\n",
    "    \"\"\"Compute the pseudo-regret associated with a sequence of arm selections\"\"\"\n",
    "    best = np.max(env.means)*np.ones(len(selections))\n",
    "    real = np.array(env.means)[selections]\n",
    "    return np.cumsum(best - real)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8704bc99",
   "metadata": {
    "cell_marker": "\"\"\"",
    "lines_to_next_cell": 1
   },
   "source": [
    "### Baselines\n",
    "\n",
    "In the following, we implemented two naive bandit strategies: the greedy strategy (or Follow-the-Leader, `FTL`) and a strategy that explores arms uniformly at random (`UniformExploration`). \n",
    "\n",
    "Take note of the `class` structure (methods implemented), you will need to use the same structure in your implementations shortly. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a6e50afc",
   "metadata": {},
   "outputs": [],
   "source": [
    "class FTL:\n",
    "\n",
    "    \"\"\"Follow the Leader (a.k.a. greedy strategy)\"\"\"\n",
    "\n",
    "    def __init__(self,n_arms):\n",
    "        self.n_arms = n_arms\n",
    "        self.clear()\n",
    "\n",
    "    def clear(self):\n",
    "        self.nbDraws = np.zeros(self.n_arms)\n",
    "        self.cumRewards = np.zeros(self.n_arms)\n",
    "    \n",
    "    def act(self):\n",
    "        ''' start by pulling every arm at least once, then always select the one with the highest average reward '''\n",
    "        if (min(self.nbDraws)==0):\n",
    "            return randmax(-self.nbDraws)\n",
    "        else:\n",
    "            return randmax(self.cumRewards/self.nbDraws)\n",
    "\n",
    "    def update(self,a,r):\n",
    "        self.cumRewards[a] = self.cumRewards[a] + r\n",
    "        self.nbDraws[a] = self.nbDraws[a] + 1\n",
    "\n",
    "    def name(self):\n",
    "        return \"FTL\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2ba03ca7",
   "metadata": {
    "lines_to_next_cell": 1
   },
   "outputs": [],
   "source": [
    "class UniformExploration:\n",
    "\n",
    "    \"\"\"a strategy that uniformly explores arms\"\"\"\n",
    "\n",
    "    def __init__(self,n_arms):\n",
    "        self.n_arms = n_arms\n",
    "        self.clear()\n",
    "\n",
    "    def clear(self):\n",
    "        self.nbDraws = np.zeros(self.n_arms)\n",
    "        self.cumRewards = np.zeros(self.n_arms)\n",
    "    \n",
    "    def act(self):\n",
    "        ''' always selects a random arm '''\n",
    "        return np.random.randint(0,self.n_arms)\n",
    "\n",
    "    def update(self,arm,reward):\n",
    "        self.cumRewards[arm] = self.cumRewards[arm]+reward\n",
    "        self.nbDraws[arm] = self.nbDraws[arm] +1\n",
    "\n",
    "    def name(self):\n",
    "        return \"Uniform\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e915cfa2",
   "metadata": {
    "cell_marker": "\"\"\""
   },
   "source": [
    "Below we run `FTL` on the simple Bernoulli bandit instance defined above, and we visualize its behavior."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4ed6c24f",
   "metadata": {},
   "outputs": [],
   "source": [
    "ftl = FTL(n_arms)\n",
    "T = 200\n",
    "actions, rewards = evaluate_run(env_B, ftl, T)\n",
    "regret1 = cumulative_regret(env_B, actions)\n",
    "# Histogram of the number of arms selections\n",
    "plt.clf()\n",
    "plt.xlabel(\"Arms\", fontsize=14)\n",
    "plt.xticks(range(ftl.n_arms))\n",
    "plt.ylabel(\"Number of arms selections\", fontsize=14)\n",
    "plt.hist(actions, np.max(actions) + 1)\n",
    "plt.title(\"Number of selections of each arm\", fontsize=14)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f388ba1b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cumulative regret as a function of time\n",
    "plt.figure(2)\n",
    "plt.clf()\n",
    "plt.xlabel(\"$t$\", fontsize=14)\n",
    "plt.ylabel(\"Cumulative regret\", fontsize=14)\n",
    "plt.title(\"Regret as a function of time\")\n",
    "plt.plot(range(T), regret1, 'black', linewidth=1)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d8865fec",
   "metadata": {
    "cell_marker": "\"\"\""
   },
   "source": [
    "Let's compare regret between FTL and the Uniform algorithm.\n",
    "\n",
    "You can run the code more than once, and you should get a different result each time."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "616c4a39",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compare their regret\n",
    "ufm = UniformExploration(n_arms)\n",
    "T = 200\n",
    "actions, rewards = evaluate_run(env_B, ufm, T)\n",
    "regret2 = cumulative_regret(env_B, actions)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "76e8b6b1",
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.xlabel(\"Time steps\", fontsize=14)\n",
    "plt.ylabel(\"Cumulative regret\", fontsize=14)\n",
    "plt.title(\"Regret as a function of time\")\n",
    "plt.plot(range(0, T), regret1,label=ftl.name())\n",
    "plt.plot(range(0, T), regret2,label=ufm.name())\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e7bbacd0",
   "metadata": {
    "cell_marker": "\"\"\"",
    "lines_to_next_cell": 1
   },
   "source": [
    "### Comparison averaged over multiple runs\n",
    "\n",
    "Since the regret is defined as an **expectation**, we need several runs to estimate its value. We can also take a look at the distribution of the pseudo-regret. The function below gathers results accross multiple runs."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0bd3777e",
   "metadata": {
    "lines_to_next_cell": 1
   },
   "outputs": [],
   "source": [
    "def evaluate_runs(env, pi, T=1000, N=10):\n",
    "    '''\n",
    "        Parameters\n",
    "        ----------\n",
    "\n",
    "        env : \n",
    "            bandit machine\n",
    "        pi : \n",
    "            bandit algorithm\n",
    "        N : int\n",
    "            number of experiments\n",
    "        T : int\n",
    "            number of trails per experiment\n",
    "    '''\n",
    "    R_log = np.zeros((N,T))\n",
    "    A_log = np.zeros((N,T),dtype=int)\n",
    "    for n in range(N):\n",
    "        np.random.seed()\n",
    "        A_log[n], R_log[n] = evaluate_run(env, pi, T)\n",
    "\n",
    "    return A_log, R_log"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d0132e28",
   "metadata": {
    "cell_marker": "\"\"\"",
    "lines_to_next_cell": 1
   },
   "source": [
    "The following function will be useful for comparing arbitrary combinations of bandit algorithms empirically under a number (`N`) of runs. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c63abf4d",
   "metadata": {
    "lines_to_next_cell": 1
   },
   "outputs": [],
   "source": [
    "def plot_results(env, algorithms, T=1000, N=10, fname=None): \n",
    "\n",
    "    plt.clf()\n",
    "    plt.title(f\"Cumulative Regret over {N} Runs - {env.name()} Environment\")\n",
    "\n",
    "    colours = ['b', 'r', 'g', 'm']\n",
    "\n",
    "    for j, pi in enumerate(algorithms):\n",
    "\n",
    "        A_log, R_log = evaluate_runs(env, pi, T, N)\n",
    "        Regret = np.array([cumulative_regret(env,A_log[n,:]) for n in range(N)])\n",
    "        meanRegret = np.mean(Regret, 0)\n",
    "        upperQuantile = np.quantile(Regret, 0.95, 0) \n",
    "        lowerQuantile = np.quantile(Regret, 0.05, 0)\n",
    "\n",
    "        plt.plot(range(T), meanRegret, linewidth=3.0, color=colours[j], label=\"\"+pi.name())\n",
    "        plt.plot(range(T), upperQuantile, linestyle=\"dashed\", color=colours[j])\n",
    "        plt.plot(range(T), lowerQuantile, linestyle=\"dashed\", color=colours[j])\n",
    "\n",
    "    plt.xlabel(\"$t$\", fontsize=10)\n",
    "    plt.ylabel(\"Cumulative regret\", fontsize=10)\n",
    "    plt.legend()\n",
    "    plt.tight_layout()\n",
    "    if fname is not None:\n",
    "        plt.savefig(fname,bbox_inches='tight')\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "65716bed",
   "metadata": {
    "cell_marker": "\"\"\""
   },
   "source": [
    "So now let's compare FTL and Uniform in terms of expectation. What do you observe? "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "76c93d5a",
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_results(env_B, [ftl, ufm])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8c908dc0",
   "metadata": {
    "cell_marker": "r\"\"\"",
    "lines_to_next_cell": 1
   },
   "source": [
    "### Task \n",
    "\n",
    "Implement Epsilon Greedy ($\\epsilon$) in the `EpsilonGreedy` class below, with the same structure (method definitions) as `FTL` and `Uniform` above."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a19f5f29",
   "metadata": {},
   "outputs": [],
   "source": [
    "class EpsilonGreedy:\n",
    "\n",
    "    \"\"\" Epsilon Greedy \"\"\"\n",
    "\n",
    "    def __init__(self, n_arms, epsilon=0.05):\n",
    "        self.n_arms = n_arms\n",
    "        self.epsilon = epsilon\n",
    "        self.clear()\n",
    "\n",
    "    def clear(self):\n",
    "        self.nbDraws = np.zeros(self.n_arms)\n",
    "        self.cumRewards = np.zeros(self.n_arms)\n",
    "\n",
    "    def act(self):\n",
    "        ''' With probability ϵ, the agent explores. With probability 1-ϵ, the agent exploits '''\n",
    "        if (min(self.nbDraws)==0):\n",
    "            return randmax(-self.nbDraws)\n",
    "        else:\n",
    "            if np.random.rand() <= self.epsilon:\n",
    "                return np.random.randint(0,self.n_arms)\n",
    "            else:\n",
    "                return randmax(self.cumRewards/self.nbDraws)\n",
    "\n",
    "    def update(self,a,r):\n",
    "        self.cumRewards[a] = self.cumRewards[a] + r\n",
    "        self.nbDraws[a] = self.nbDraws[a] + 1\n",
    "     \n",
    "    def name(self):\n",
    "        return \"e-Greedy(%3.2f)\" % self.epsilon"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "de45667b",
   "metadata": {
    "cell_marker": "r\"\"\""
   },
   "source": [
    "Have a look at the performance of this method for different values of $\\epsilon$. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "41fc0e57",
   "metadata": {
    "lines_to_next_cell": 2
   },
   "outputs": [],
   "source": [
    "plot_results(env_B, [EpsilonGreedy(env_B.n_arms,0),EpsilonGreedy(env_B.n_arms,0.1),EpsilonGreedy(env_B.n_arms,0.01)], T=100, N=100)\n",
    "# \n",
    "\n",
    "# And with another bandit instance (Gaussian arms, this time)\n",
    "env_G = GaussianMAB(2)\n",
    "plot_results(env_G, [EpsilonGreedy(env_G.n_arms,0),EpsilonGreedy(env_G.n_arms,0.1),EpsilonGreedy(env_G.n_arms,0.01)], T=100, N=100)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d70e8e3d",
   "metadata": {
    "cell_marker": "r\"\"\"",
    "lines_to_next_cell": 1
   },
   "source": [
    "### Task \n",
    "\n",
    "Implement UCB($\\alpha$) in the `UCB` class below, with the same structure (method definitions) as `FTL` and `Uniform` above."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "60186b13",
   "metadata": {},
   "outputs": [],
   "source": [
    "class UCB():\n",
    "\n",
    "    \"\"\"UCB1 with parameter alpha\"\"\"\n",
    "\n",
    "    def __init__(self,n_arms,alpha=1/2):\n",
    "        self.n_arms = n_arms\n",
    "        self.alpha = alpha\n",
    "        self.clear()\n",
    "\n",
    "    def clear(self):\n",
    "        self.nbDraws = np.zeros(self.n_arms)\n",
    "        self.cumRewards = np.zeros(self.n_arms)\n",
    "        self.t = 0\n",
    "\n",
    "    def act(self):\n",
    "        ''' balance exploration and exploitation by encourages the agent to pick arms with high average rewards, but ensures that arms with fewer pulls are favored '''\n",
    "        self.t += 1\n",
    "        if (min(self.nbDraws)==0):\n",
    "            return randmax(-self.nbDraws)\n",
    "        else:\n",
    "            return randmax(self.cumRewards/self.nbDraws + np.sqrt(self.alpha*np.log(self.t)/self.nbDraws))\n",
    "    \n",
    "    def update(self,a,r):\n",
    "        self.cumRewards[a] = self.cumRewards[a] + r\n",
    "        self.nbDraws[a] = self.nbDraws[a] + 1\n",
    "\n",
    "    def name(self):\n",
    "        return \"UCB(%3.2f)\" % self.alpha"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c97523f5",
   "metadata": {
    "cell_marker": "r\"\"\""
   },
   "source": [
    "We'll just check that your UCB implementation is working."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5764fd4b",
   "metadata": {},
   "outputs": [],
   "source": [
    "ucb = UCB(env_G.n_arms, alpha=1/2)\n",
    "plot_results(env_G, [ucb], T=100, N=10)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ad38b686",
   "metadata": {
    "cell_marker": "\"\"\""
   },
   "source": [
    "Let's try a different bandit machine. Namely, we'll try a mixed multi-arm bandit, with arms of different reward distributions, but you could experiment with others also. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "57e1ef83",
   "metadata": {},
   "outputs": [],
   "source": [
    "from bandit_machines import MixedMAB, TruncatedExponential, Bernoulli, Gaussian\n",
    "# You should play around with different types of bandit environments here \n",
    "\n",
    "env_M = MixedMAB([TruncatedExponential(2, 1), Bernoulli(0.3), TruncatedExponential(3.5, 1)])\n",
    "\n",
    "plot_results(env_M, [UCB(env_M.n_arms, alpha=0.2), UCB(env_M.n_arms, alpha=0.4), UCB(env_M.n_arms, alpha=0.6)], T=100, N=10)\n",
    "\n",
    "env_M1 = MixedMAB([Gaussian(1), Bernoulli(0.5), TruncatedExponential(5, 2)])\n",
    "\n",
    "plot_results(env_M1, [UCB(env_M1.n_arms, alpha=0.2), UCB(env_M1.n_arms, alpha=0.4), UCB(env_M1.n_arms, alpha=0.6)], T=100, N=10)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8c96266c",
   "metadata": {
    "cell_marker": "r\"\"\""
   },
   "source": [
    "### Task \n",
    "\n",
    "Implement Thompson Sampling in the class provided below, with the same structure as the methods you implemented above."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aa81d9cd",
   "metadata": {
    "lines_to_next_cell": 1
   },
   "outputs": [],
   "source": [
    "from scipy.stats import beta"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f8e96fb8",
   "metadata": {
    "lines_to_next_cell": 1
   },
   "outputs": [],
   "source": [
    "class ThompsonSampling():\n",
    "\n",
    "    def __init__(self,n_arms):\n",
    "        ''' Bernoulli bandit machine (n_arms arms, each giving 0 or 1 with some probability) '''\n",
    "        self.n_arms = n_arms\n",
    "        self.clear()\n",
    "    \n",
    "    def clear(self):\n",
    "        self.alpha = np.ones(self.n_arms)\n",
    "        self.beta = np.ones(self.n_arms)\n",
    "\n",
    "    def act(self):\n",
    "        ''' generates a random sample from the Beta(alpha, beta) distribution. '''\n",
    "        return randmax(beta.rvs(self.alpha, self.beta))\n",
    "    \n",
    "    def update(self,a,r):\n",
    "        ''' Updates the posterior: if reward was received, increase Alpha. If not, increase Beta '''\n",
    "        self.alpha[a] = self.alpha[a] + r\n",
    "        self.beta[a] = self.beta[a] + 1 - r\n",
    "         \n",
    "    def name(self):\n",
    "        return \"Thompson\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3e59fc79",
   "metadata": {
    "cell_marker": "\"\"\""
   },
   "source": [
    "Finally, let's compare the three algorithms that you have implemented."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2fe95035",
   "metadata": {
    "lines_to_next_cell": 2
   },
   "outputs": [],
   "source": [
    "# Here, we carry out the comparison\n",
    "plot_results(env_B, [UCB(env_B.n_arms), EpsilonGreedy(env_B.n_arms), ThompsonSampling(env_B.n_arms)],N=100)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "105f7252",
   "metadata": {
    "cell_marker": "r\"\"\""
   },
   "source": [
    "### Something to Think About \n",
    "\n",
    "Regarding the Environment we used for Lab1 and Lab2, we could also use bandits in that scenario, namely, to if we had to decide on the best action to take *without* any observation. When considered as a 'rollout', this takes us closer and closer to Monte Carlo Tree Search and other algorithms in the domain of reinforcement learning. "
   ]
  }
 ],
 "metadata": {
  "jupytext": {
   "cell_metadata_filter": "-all",
   "main_language": "python",
   "notebook_metadata_filter": "-all"
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
